# Test Binary Generator Context Fix Plan

## Problem Statement

The test binary generator (`compiler/src/testgen.rs`) produces invalid instruction bytes, resulting in only 67.9% verification against Binary Ninja (while IDA/BN agree on 97.1%). The root cause is improper handling of context fields:

1. **VEX/EVEX instructions get invalid bytes** - The generator doesn't properly emit VEX/EVEX prefix bytes when `vexMode=1` or `vexMode=2` is required
2. **BFS prefix chain fails silently** - When `find_prefix_chain()` can't find a path, it returns empty and the instruction gets invalid bytes
3. **No validation** - Generated bytes aren't validated against the decoder
4. **Hardcoded alignment** - Not derived from architecture specs

## Current Architecture

```
Sleigh spec → Compiler → Scanners → TestBinaryGenerator
                ↓
            context_fields: Vec<ContextFieldDef>
                - name, start_bit, end_bit, default_value

            ScannerPattern:
                - context_requirements: HashMap<String, u64>
                - context_actions: Vec<ContextAction>
                - is_recursive: bool (prefix patterns)
```

### Key Files

| File | Role |
|------|------|
| `compiler/src/testgen.rs` | Test binary generator |
| `compiler/src/scanners.rs` | Pattern definitions, ContextFieldDef |
| `compiler/src/pspec.rs` | XML parsing for defaults |
| `vm/src/scanner_decode.rs` | Decoder (for validation) |

## Solution Design

### Phase 1: Extract Valid Context Constraints from Sleigh

**Goal**: Build a map of valid context field values from the sleigh spec.

```rust
// New structure to capture valid values per field
pub struct ContextConstraints {
    pub field_values: HashMap<String, HashSet<u64>>,  // field -> valid values
    pub field_metadata: HashMap<String, FieldConstraint>,
}

pub struct FieldConstraint {
    pub name: String,
    pub start_bit: u32,
    pub end_bit: u32,
    pub default_value: u64,
    pub max_value: u64,  // (1 << width) - 1
    pub valid_values: HashSet<u64>,  // Values actually used in patterns
}
```

**Implementation**:
1. Scan all patterns' `context_requirements` to collect values actually used
2. For boolean fields (1-bit), valid values are {0, 1}
3. For multi-bit fields, collect all values from pattern requirements
4. Store in `ContextConstraints` attached to `Scanners`

### Phase 2: Rewrite Prefix Byte Generation

**Goal**: Generate valid prefix bytes for any context requirement.

The current BFS approach through recursive patterns is fragile. Instead:

1. **Direct VEX/EVEX generation** for x86:
```rust
fn generate_prefix_for_context(&self, requirements: &HashMap<String, u64>) -> Vec<u8> {
    let vex_mode = requirements.get("vexMode").copied().unwrap_or(0);

    match vex_mode {
        0 => self.generate_legacy_prefix(requirements),
        1 => self.generate_vex_prefix(requirements),
        2 => self.generate_evex_prefix(requirements),
        _ => Vec::new(),
    }
}

fn generate_vex_prefix(&self, requirements: &HashMap<String, u64>) -> Vec<u8> {
    // Extract fields from requirements
    let w = requirements.get("rexWprefix").copied().unwrap_or(0) as u8;
    let l = requirements.get("vexL").copied().unwrap_or(0) as u8;
    let pp = self.encode_vex_pp(requirements);  // From mandover/prefix_66/f2/f3
    let mmmmm = requirements.get("vexMMMMM").copied().unwrap_or(1) as u8;
    let vvvv = requirements.get("vexVVVV").copied().unwrap_or(0xF) as u8;

    // Use existing build_vex3_prefix helper
    build_vex3_prefix(mmmmm, w, pp, l)
}
```

2. **Architecture-agnostic prefix handling** for other archs:
   - ARM Thumb: Generate Thumb prefix when `TMode=1`
   - Keep BFS as fallback for non-x86 architectures

### Phase 3: Derive Alignment from Sleigh Specs

**Goal**: Use architecture-specific alignment from spec files.

```rust
// In ArchConfig, derive alignment from architecture
pub fn alignment_for_arch(arch: &str, scanners: &Scanners) -> usize {
    // Check for alignment hints in context fields
    // x86: Use 32 for cache line optimization
    // ARM: 4 (word-aligned)
    // Thumb: 2 (halfword-aligned)
    // MIPS: 4 (word-aligned)

    match arch.to_lowercase().as_str() {
        "x86" | "x86_64" | "x86-64" => 32,
        "arm" | "aarch64" => 4,
        "thumb" | "thumb2" => 2,
        "mips" | "mipsel" | "mips64" => 4,
        "riscv" | "riscv32" | "riscv64" => 4,
        "sparc" => 4,
        "ppc" | "powerpc" => 4,
        "z80" | "8051" | "6502" => 1,  // 8-bit archs
        _ => 4,  // Default to word-aligned
    }
}
```

### Phase 4: Validate Generated Bytes

**Goal**: Ensure every generated instruction can be decoded.

```rust
fn validate_instruction(&self, bytes: &[u8], expected_mnemonic: &str) -> bool {
    // Use ScannerDecoder to verify bytes decode correctly
    let decoder = ScannerDecoder::from_plugin(&self.plugin_bytes);
    let mut ctx = decoder.create_context();

    match decoder.decode(bytes, &mut ctx) {
        Some(result) if result.len > 0 => {
            // Optionally verify mnemonic matches
            result.mnemonic == expected_mnemonic
        }
        _ => false,
    }
}
```

Add validation pass after generation:
```rust
fn collect_unique_instructions(&self) -> Vec<GeneratedInstruction> {
    // ... existing generation ...

    // Validation pass
    let mut valid_instructions = Vec::new();
    for inst in instructions {
        if self.validate_instruction(&inst.bytes, &inst.mnemonic) {
            valid_instructions.push(inst);
        } else {
            eprintln!("WARN: Invalid bytes for {}: {:02x?}", inst.mnemonic, inst.bytes);
        }
    }
    valid_instructions
}
```

### Phase 5: Ensure Reachability

**Goal**: All instructions reachable from entry point.

Current implementation already chains with jumps. Add verification:

```rust
fn verify_reachability(&self, binary: &TestBinary) -> bool {
    let mut reachable: HashSet<usize> = HashSet::new();
    let mut worklist = vec![0usize];  // Start at entry point

    while let Some(offset) = worklist.pop() {
        if reachable.contains(&offset) { continue; }
        reachable.insert(offset);

        // Find instruction at this offset
        if let Some(inst) = binary.manifest.instructions.iter()
            .find(|i| i.offset == offset)
        {
            // Add fall-through target
            let next = offset + inst.length as usize;
            worklist.push(next);

            // Add jump targets (if we track them)
            if let Some(target) = self.get_jump_target(binary, offset) {
                worklist.push(target);
            }
        }
    }

    // Verify all instructions are reachable
    binary.manifest.instructions.iter()
        .all(|i| reachable.contains(&i.offset))
}
```

## Implementation Order

1. **Phase 2 first** - Fix VEX/EVEX generation (highest impact)
2. **Phase 4 second** - Add validation to catch remaining issues
3. **Phase 1 third** - Extract constraints for smarter generation
4. **Phase 3 fourth** - Proper alignment
5. **Phase 5 last** - Verify reachability

## Test Strategy

1. Run `verify_platform x86_64` after each phase
2. Target: 95%+ match rate (up from 67.9%)
3. Run benchmark to ensure no performance regression

## Documentation

Create `docs/testgen.md` covering:
- Architecture-specific prefix generation
- Context constraint extraction
- Validation pipeline
- Reachability guarantees
- Adding support for new architectures

## Files to Modify

| File | Changes |
|------|---------|
| `compiler/src/testgen.rs` | Main fixes (Phases 1-5) |
| `compiler/src/scanners.rs` | Add ContextConstraints |
| `compiler/src/lib.rs` | Wire up validation |
| `docs/testgen.md` | New documentation |

## Success Criteria

- [ ] x86-64 verification: 67.9% → 95%+
- [ ] All VEX/EVEX instructions have valid prefix bytes
- [ ] No invalid instruction bytes in generated binary
- [ ] All instructions reachable from entry point
- [ ] Alignment matches architecture requirements
- [ ] Documentation complete

;; aarch64 ISLE lowering rules
;; Minimal bootstrap set for ARM64 instruction selection

;; Primitive types (required by ISLE compiler)
(type Unit (primitive Unit))
(type Type (primitive Type))
(type Value (primitive Value))
(type Inst (primitive Inst))

;; Type extractors (foundation for type-parameterized patterns)
(decl multi_lane (Type) (u32 u32))
(extern extractor multi_lane multi_lane)

(decl fits_in_64 (Type) Type)
(extern extractor fits_in_64 fits_in_64)

(decl lane_fits_in_32 (Type) Type)
(extern extractor lane_fits_in_32 lane_fits_in_32)

(decl pure ty_bits (Type) u8)
(extern constructor ty_bits ty_bits)

;; Integer arithmetic lowering
;; Integer arithmetic lowering

;; iadd: Add two integers (default, lowest priority)
(rule 0 (lower (iadd ty x y))
      (aarch64_add_rr ty x y))

;; iadd with sign-extended byte operand (higher priority than generic)
;; Pattern: iadd(x, sextb(y)) => ADD Xd, Xn, Wm, SXTB
(rule 2 (lower (iadd ty x (aarch64_sxtb ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtb))

;; iadd with sign-extended halfword operand (higher priority than generic)
;; Pattern: iadd(x, sexth(y)) => ADD Xd, Xn, Wm, SXTH
(rule 2 (lower (iadd ty x (aarch64_sxth ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxth))

;; iadd with sign-extended word operand (higher priority than generic)
;; Pattern: iadd(x, sextw(y)) => ADD Xd, Xn, Wm, SXTW
(rule 2 (lower (iadd ty x (aarch64_sxtw y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtw))

;; iadd with zero-extended byte operand (higher priority than generic)
;; Pattern: iadd(x, uextb(y)) => ADD Xd, Xn, Wm, UXTB
(rule 2 (lower (iadd ty x (aarch64_uxtb ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxtb))

;; iadd with zero-extended halfword operand (higher priority than generic)
;; Pattern: iadd(x, uexth(y)) => ADD Xd, Xn, Wm, UXTH
(rule 2 (lower (iadd ty x (aarch64_uxth ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxth))

;; iadd with commuted operands (sign-extended byte, higher priority)
(rule 2 (lower (iadd ty (aarch64_sxtb ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtb))

;; iadd with commuted operands (sign-extended halfword, higher priority)
(rule 2 (lower (iadd ty (aarch64_sxth ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxth))

;; iadd with commuted operands (sign-extended word, higher priority)
(rule 2 (lower (iadd ty (aarch64_sxtw y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtw))

;; iadd with commuted operands (zero-extended byte, higher priority)
(rule 2 (lower (iadd ty (aarch64_uxtb ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxtb))

;; iadd with commuted operands (zero-extended halfword, higher priority)
(rule 2 (lower (iadd ty (aarch64_uxth ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxth))

;; isub: Subtract integers
(rule (lower (isub ty x y))
      (aarch64_sub_rr ty x y))

;; imul: Multiply integers
(rule (lower (imul ty x y))
      (aarch64_mul_rr ty x y))

;; Multiply-add fusion patterns (highest priority - fuse before generic ops)
;; iadd(a, imul(x, y)) => madd(x, y, a)
(rule 3 (lower (iadd ty (imul ty x y) a))
      (aarch64_madd ty x y a))

(rule 3 (lower (iadd ty a (imul ty x y)))
      (aarch64_madd ty x y a))

;; Multiply-subtract fusion patterns (highest priority - fuse before generic ops)
;; isub(a, imul(x, y)) => msub(x, y, a)
(rule 3 (lower (isub ty a (imul ty x y)))
      (aarch64_msub ty x y a))

;; smul_hi: Signed multiply returning upper bits (64×64→128, return high 64)
(rule (lower (smul_hi ty x y))
      (if-let $I64 ty)
      (aarch64_smulh x y))

;; umul_hi: Unsigned multiply returning upper bits (64×64→128, return high 64)
(rule (lower (umul_hi ty x y))
      (if-let $I64 ty)
      (aarch64_umulh x y))

;; smulhi: Signed multiply returning upper bits (64×64→128, return high 64)
(rule (lower (has_type $I64 (smulhi x y)))
      (aarch64_smulh x y))

;; umulhi: Unsigned multiply returning upper bits (64×64→128, return high 64)
(rule (lower (has_type $I64 (umulhi x y)))
      (aarch64_umulh x y))

;; sdiv: Signed divide
(rule (lower (sdiv ty x y))
      (aarch64_sdiv ty x y))

;; udiv: Unsigned divide
(rule (lower (udiv ty x y))
      (aarch64_udiv ty x y))

;; srem: Signed remainder (implemented as sdiv + msub)
;; srem(x, y) = x - (x / y) * y
(rule (lower (srem ty x y))
      (let ((quot Aarch64Inst (aarch64_sdiv ty x y)))
            (aarch64_msub ty quot y x)))

;; urem: Unsigned remainder (implemented as udiv + msub)
;; urem(x, y) = x - (x / y) * y
(rule (lower (urem ty x y))
      (let ((quot Aarch64Inst (aarch64_udiv ty x y)))
            (aarch64_msub ty quot y x)))

;; Type conversion operations lowering

;; sextend: Sign-extend to target type
;; I8 -> I16/I32/I64 => SXTB
(rule (lower (sextend dst_ty src_ty x))
      (if-let 8 (ty_bits src_ty))
      (aarch64_sxtb dst_ty x))

;; I16 -> I32/I64 => SXTH
(rule (lower (sextend dst_ty src_ty x))
      (if-let 16 (ty_bits src_ty))
      (aarch64_sxth dst_ty x))

;; I32 -> I64 => SXTW
(rule (lower (sextend dst_ty src_ty x))
      (if-let 32 (ty_bits src_ty))
      (if-let 64 (ty_bits dst_ty))
      (aarch64_sxtw x))

;; uextend: Zero-extend to target type
;; I8 -> I16/I32/I64 => UXTB
(rule (lower (uextend dst_ty src_ty x))
      (if-let 8 (ty_bits src_ty))
      (aarch64_uxtb dst_ty x))

;; I16 -> I32/I64 => UXTH
(rule (lower (uextend dst_ty src_ty x))
      (if-let 16 (ty_bits src_ty))
      (aarch64_uxth dst_ty x))

;; I32 -> I64 => UXTW (note: 32-bit ops auto zero-extend on ARM64)
(rule (lower (uextend dst_ty src_ty x))
      (if-let 32 (ty_bits src_ty))
      (if-let 64 (ty_bits dst_ty))
      (aarch64_uxtw x))

;; ireduce: Truncate integer to narrower type
;; On ARM64, this is implicit when using smaller register size
;; I64 -> I32/I16/I8: move to W register (32-bit) truncates
(rule (lower (ireduce dst_ty src_ty x))
      (aarch64_ireduce dst_ty x))

;; fadd: Floating-point addition (FADD)
(rule (lower (fadd ty x y))
      (aarch64_fadd ty x y))

;; fsub: Floating-point subtraction (FSUB)
(rule (lower (fsub ty x y))
      (aarch64_fsub ty x y))

;; fmul: Floating-point multiplication (FMUL)
(rule (lower (fmul ty x y))
      (aarch64_fmul ty x y))

;; fdiv: Floating-point division (FDIV)
(rule (lower (fdiv ty x y))
      (aarch64_fdiv ty x y))

;; fmin: Floating-point minimum (FMIN)
(rule (lower (fmin ty x y))
      (aarch64_fmin ty x y))

;; fmax: Floating-point maximum (FMAX)
(rule (lower (fmax ty x y))
      (aarch64_fmax ty x y))

;; sqrt: Floating-point square root (FSQRT)
(rule (lower (sqrt ty x))
      (aarch64_fsqrt ty x))

;; splat: Duplicate scalar to all vector lanes (DUP)
(rule (lower (splat ty x))
      (aarch64_splat ty x))

;; extractlane: Extract vector lane to scalar (UMOV)
(rule (lower (extractlane ty vec lane))
      (aarch64_extractlane ty vec lane))

;; insertlane: Insert scalar into vector lane (INS)
(rule (lower (insertlane ty vec x lane))
      (aarch64_insertlane ty vec x lane))

;; fcvt_from_sint: Convert signed integer to float (SCVTF)
(rule (lower (fcvt_from_sint dst_ty src_ty x))
      (aarch64_scvtf dst_ty src_ty x))

;; fcvt_from_uint: Convert unsigned integer to float (UCVTF)
(rule (lower (fcvt_from_uint dst_ty src_ty x))
      (aarch64_ucvtf dst_ty src_ty x))

;; fpromote: Promote f32 to f64 (FCVT)
(rule (lower (fpromote dst_ty src_ty x))
      (if-let 32 (ty_bits src_ty))
      (if-let 64 (ty_bits dst_ty))
      (aarch64_fpromote x))

;; fdemote: Demote f64 to f32 (FCVT)
(rule (lower (fdemote dst_ty src_ty x))
      (if-let 64 (ty_bits src_ty))
      (if-let 32 (ty_bits dst_ty))
      (aarch64_fdemote x))

;; nearest: Round to nearest (FRINTN)
(rule (lower (nearest ty x))
      (aarch64_nearest ty x))

;; trunc: Round toward zero (FRINTZ)
(rule (lower (trunc ty x))
      (aarch64_trunc ty x))

;; ceil: Round toward +infinity (FRINTP)
(rule (lower (ceil ty x))
      (aarch64_ceil ty x))

;; floor: Round toward -infinity (FRINTM)
(rule (lower (floor ty x))
      (aarch64_floor ty x))

;; iconcat: Concatenate two integers into wider type (I64 + I64 → I128)
;; On ARM64, this creates a register pair from two separate registers
(rule (lower (has_type $I128 (iconcat lo hi)))
      (value_regs_from_values lo hi))

;; isplit: Split wider integer into two parts (I128 → I64 + I64)
;; Returns ValueRegs with low and high parts
(rule (lower (isplit x))
      (aarch64_isplit x))

;; Atomic operations lowering

;; atomic_load with acquire ordering: LDAR
(rule (lower (atomic_load ty addr AtomicOrdering.Acquire _flags))
      (aarch64_atomic_load_acquire ty addr))

;; atomic_load with sequential consistency: DMB + LDAR
;; Note: Simplified - full seq_cst may need DMB before as well
(rule (lower (atomic_load ty addr AtomicOrdering.SeqCst _flags))
      (aarch64_atomic_load_acquire ty addr))

;; atomic_store with release ordering: STLR
(rule (lower (atomic_store val addr AtomicOrdering.Release _flags))
      (let ((ty Type (value_type val)))
           (aarch64_atomic_store_release ty addr val)))

;; atomic_store with sequential consistency: STLR + DMB
;; Note: Simplified - full seq_cst may need DMB after as well
(rule (lower (atomic_store val addr AtomicOrdering.SeqCst _flags))
      (let ((ty Type (value_type val)))
           (aarch64_atomic_store_release ty addr val)))

;; fence: Memory barrier
(rule (lower (fence ordering))
      (aarch64_fence ordering))

;; Shift operations lowering

;; ishl: Integer shift left (register form)
(rule (lower (ishl ty x y))
      (aarch64_lsl_rr ty x y))

;; ishl: Integer shift left (immediate form)
(rule (lower (ishl ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_lsl_imm ty x amt))

;; ushr: Unsigned shift right (register form)
(rule (lower (ushr ty x y))
      (aarch64_lsr_rr ty x y))

;; ushr: Unsigned shift right (immediate form)
(rule (lower (ushr ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_lsr_imm ty x amt))

;; sshr: Signed shift right (register form)
(rule (lower (sshr ty x y))
      (aarch64_asr_rr ty x y))

;; sshr: Signed shift right (immediate form)
(rule (lower (sshr ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_asr_imm ty x amt))

;; rotr: Rotate right (register form)
(rule (lower (rotr ty x y))
      (aarch64_ror_rr ty x y))

;; rotr: Rotate right (immediate form)
(rule (lower (rotr ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_ror_imm ty x amt))

;; rotl: Rotate left (register form) - implemented as rotr(x, bits - y)
(rule (lower (rotl ty x y))
      (aarch64_rotl_rr ty x y))

;; rotl: Rotate left (immediate form) - implemented as rotr(x, bits - amount)
(rule (lower (rotl ty x (iconst k)))
      (if-let amt (valid_rotl_imm ty k))
      (aarch64_ror_imm ty x amt))
;; rotl(x, k) = rotr(x, width - k)

;; smin: Signed minimum (CMP + CSEL)
(rule (lower (smin ty x y))
      (aarch64_smin ty x y))

;; umin: Unsigned minimum (CMP + CSEL)
(rule (lower (umin ty x y))
      (aarch64_umin ty x y))

;; smax: Signed maximum (CMP + CSEL)
(rule (lower (smax ty x y))
      (aarch64_smax ty x y))

;; umax: Unsigned maximum (CMP + CSEL)
(rule (lower (umax ty x y))
      (aarch64_umax ty x y))

;; sadd_sat: Signed saturating add (SQADD)
(rule (lower (has_type $I8 (sadd_sat x y)))
      (aarch64_sqadd_8 x y))

(rule (lower (has_type $I16 (sadd_sat x y)))
      (aarch64_sqadd_16 x y))

(rule (lower (has_type $I32 (sadd_sat x y)))
      (aarch64_sqadd_32 x y))

(rule (lower (has_type $I64 (sadd_sat x y)))
      (aarch64_sqadd_64 x y))

;; ssub_sat: Signed saturating subtract (SQSUB)
(rule (lower (has_type $I8 (ssub_sat x y)))
      (aarch64_sqsub_8 x y))

(rule (lower (has_type $I16 (ssub_sat x y)))
      (aarch64_sqsub_16 x y))

(rule (lower (has_type $I32 (ssub_sat x y)))
      (aarch64_sqsub_32 x y))

(rule (lower (has_type $I64 (ssub_sat x y)))
      (aarch64_sqsub_64 x y))

;; uadd_sat: Unsigned saturating add (UQADD)
(rule (lower (has_type $I8 (uadd_sat x y)))
      (aarch64_uqadd_8 x y))

(rule (lower (has_type $I16 (uadd_sat x y)))
      (aarch64_uqadd_16 x y))

(rule (lower (has_type $I32 (uadd_sat x y)))
      (aarch64_uqadd_32 x y))

(rule (lower (has_type $I64 (uadd_sat x y)))
      (aarch64_uqadd_64 x y))

;; usub_sat: Unsigned saturating subtract (UQSUB)
(rule (lower (has_type $I8 (usub_sat x y)))
      (aarch64_uqsub_8 x y))

(rule (lower (has_type $I16 (usub_sat x y)))
      (aarch64_uqsub_16 x y))

(rule (lower (has_type $I32 (usub_sat x y)))
      (aarch64_uqsub_32 x y))

(rule (lower (has_type $I64 (usub_sat x y)))
      (aarch64_uqsub_64 x y))

;; bitselect: Bitwise select ((x & c) | (y & ~c))
(rule (lower (bitselect c x y))
      (aarch64_bitselect c x y))

;; iabs: Integer absolute value (CMP + NEG + CSEL)
(rule (lower (iabs ty x))
      (aarch64_iabs ty x))

;; Comparison operations lowering

;; icmp: Integer comparison with Equal condition
(rule (lower (icmp IntCC.Equal ty x y))
      (aarch64_cmp_rr ty x y IntCC.Equal))

;; icmp: Integer comparison with NotEqual condition
(rule (lower (icmp IntCC.NotEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.NotEqual))

;; icmp: Integer comparison with SignedLessThan condition
(rule (lower (icmp IntCC.SignedLessThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedLessThan))

;; icmp: Integer comparison with SignedGreaterThanOrEqual condition
(rule (lower (icmp IntCC.SignedGreaterThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedGreaterThanOrEqual))

;; icmp: Integer comparison with SignedGreaterThan condition
(rule (lower (icmp IntCC.SignedGreaterThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedGreaterThan))

;; icmp: Integer comparison with SignedLessThanOrEqual condition
(rule (lower (icmp IntCC.SignedLessThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedLessThanOrEqual))

;; icmp: Integer comparison with UnsignedLessThan condition
(rule (lower (icmp IntCC.UnsignedLessThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedLessThan))

;; icmp: Integer comparison with UnsignedGreaterThanOrEqual condition
(rule (lower (icmp IntCC.UnsignedGreaterThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedGreaterThanOrEqual))

;; icmp: Integer comparison with UnsignedGreaterThan condition
(rule (lower (icmp IntCC.UnsignedGreaterThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedGreaterThan))

;; icmp: Integer comparison with UnsignedLessThanOrEqual condition
(rule (lower (icmp IntCC.UnsignedLessThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedLessThanOrEqual))

;; icmp with immediate form (12-bit unsigned immediate)
(rule (lower (icmp cc ty x (iconst k)))
      (if-let $true (uimm12 k))
      (aarch64_cmp_imm ty x k cc))

;; Bitwise operations lowering

;; band: Bitwise AND (register form)
(rule (lower (band ty x y))
      (aarch64_and_rr ty x y))

;; bor: Bitwise OR (register form)
(rule (lower (bor ty x y))
      (aarch64_orr_rr ty x y))

;; bor: Bitwise OR (immediate form with logical immediate)
(rule 1 (lower (bor ty x (iconst k)))
      (if-let imm (imm_logic_from_u64 ty k))
      (aarch64_orr_imm ty x imm))

;; bxor: Bitwise XOR (register form)
(rule (lower (bxor ty x y))
      (aarch64_eor_rr ty x y))

;; bxor: Bitwise XOR (immediate form with logical immediate)
(rule 1 (lower (bxor ty x (iconst k)))
      (if-let imm (imm_logic_from_u64 ty k))
      (aarch64_eor_imm ty x imm))

;; bnot: Bitwise NOT (implemented as MVN)
(rule (lower (bnot ty x))
      (aarch64_mvn_rr ty x))

;; band_not: Bitwise AND NOT (BIC)
(rule (lower (band_not ty x y))
      (aarch64_bic_rr ty x y))

;; bor_not: Bitwise OR NOT (ORN)
(rule (lower (bor_not ty x y))
      (aarch64_orn_rr ty x y))

;; bxor_not: Bitwise XOR NOT (EON)
(rule (lower (bxor_not ty x y))
      (aarch64_eon_rr ty x y))

;; Bit manipulation operations

;; clz: Count leading zeros (CLZ)
(rule (lower (has_type $I32 (clz x)))
      (aarch64_clz_32 x))

(rule (lower (has_type $I64 (clz x)))
      (aarch64_clz_64 x))

;; cls: Count leading sign bits (CLS)
(rule (lower (has_type $I32 (cls x)))
      (aarch64_cls_32 x))

(rule (lower (has_type $I64 (cls x)))
      (aarch64_cls_64 x))

;; ctz: Count trailing zeros (RBIT + CLZ)
;; ARM64 doesn't have CTZ, so we reverse bits then count leading zeros
(rule (lower (has_type $I32 (ctz x)))
      (aarch64_ctz_32 x))

(rule (lower (has_type $I64 (ctz x)))
      (aarch64_ctz_64 x))

;; bitrev: Reverse bits (RBIT)
(rule (lower (has_type $I32 (bitrev x)))
      (aarch64_rbit_32 x))

(rule (lower (has_type $I64 (bitrev x)))
      (aarch64_rbit_64 x))

;; bswap: Byte swap (REV16, REV32, REV64)
(rule (lower (has_type $I16 (bswap x)))
      (aarch64_bswap_16 x))

(rule (lower (has_type $I32 (bswap x)))
      (aarch64_bswap_32 x))

(rule (lower (has_type $I64 (bswap x)))
      (aarch64_bswap_64 x))

;; popcnt: Population count (count set bits)
(rule (lower (has_type $I8 (popcnt x)))
      (aarch64_popcnt_8 x))

(rule (lower (has_type $I16 (popcnt x)))
      (aarch64_popcnt_16 x))

(rule (lower (has_type $I32 (popcnt x)))
      (aarch64_popcnt_32 x))

(rule (lower (has_type $I64 (popcnt x)))
      (aarch64_popcnt_64 x))

;; ineg: Integer negation (implemented as NEG)
(rule (lower (ineg ty x))
      (aarch64_neg ty x))

;; iabs: Integer absolute value
;; Implemented as (x XOR sign) - sign, where sign = x >> (bits-1)
(rule (lower (has_type $I8 (iabs x)))
      (aarch64_iabs_8 x))

(rule (lower (has_type $I16 (iabs x)))
      (aarch64_iabs_16 x))

(rule (lower (has_type $I32 (iabs x)))
      (aarch64_iabs_32 x))

(rule (lower (has_type $I64 (iabs x)))
      (aarch64_iabs_64 x))

;; Conditional select lowering

;; select: Conditional select based on comparison result
;; select(cond, true_val, false_val) -> true_val if cond != 0, else false_val
(rule (lower (select ty (icmp cc cmp_ty x y) true_val false_val))
      (let ((cmp_inst Aarch64Inst (aarch64_cmp_rr cmp_ty x y cc)))
            (aarch64_csel ty true_val false_val cmp_inst cc)))

;; Immediate forms

;; iadd with 12-bit immediate (priority 1 - higher than generic, lower than extensions/fusions)
(rule 1 (lower (iadd ty x (iconst k)))
      (if-let $true (uimm12 k))
      (aarch64_add_imm ty x k))

;; isub with negated 12-bit immediate optimization
;; Lower (isub x iconst) as (iadd x -iconst) when -imm fits in 12-bit
;; This provides more flexibility in immediate selection
(rule (lower (isub ty x (iconst k)))
      (if-let $true (in_neg_uimm12_range k))
      (aarch64_add_imm ty x (negate_i64 k)))

;; isub with 12-bit immediate
(rule (lower (isub ty x (iconst k)))
      (if-let $true (uimm12 k))
      (aarch64_sub_imm ty x k))

;; Load/store lowering

;; load: Load from memory with register+immediate offset
;; Pattern: load(iadd(base, iconst(offset))) => LDR Xt, [Xn, #offset]
(rule (lower (load ty (iadd ptr_ty base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_ldr_imm_offset ty offset))
      (aarch64_ldr_imm ty base scaled_offset))

;; load: Load from memory with commuted register+immediate offset
(rule (lower (load ty (iadd ptr_ty (iconst offset) base) _flags _mem_offset))
      (if-let scaled_offset (valid_ldr_imm_offset ty offset))
      (aarch64_ldr_imm ty base scaled_offset))

;; load: Load from memory with register+register offset
;; Pattern: load(iadd(base, index)) => LDR Xt, [Xn, Xm]
(rule (lower (load ty (iadd ptr_ty base index) _flags _mem_offset))
      (aarch64_ldr_reg ty base index))

;; load: Load from memory with register+extended register offset
;; Pattern: load(iadd(base, sxtw(index))) => LDR Xt, [Xn, Wm, SXTW]
(rule (lower (load ty (iadd ptr_ty base (aarch64_sxtw index)) _flags _mem_offset))
      (aarch64_ldr_ext ty base index ExtendOp.sxtw))

;; load: Load from memory with commuted register+extended register offset
(rule (lower (load ty (iadd ptr_ty (aarch64_sxtw index) base) _flags _mem_offset))
      (aarch64_ldr_ext ty base index ExtendOp.sxtw))

;; load: Load from memory with register+shifted register offset
;; Pattern: load(iadd(base, ishl(index, iconst(shift)))) => LDR Xt, [Xn, Xm, LSL #shift]
(rule (lower (load ty (iadd ptr_ty base (ishl idx_ty index (iconst shift))) _flags _mem_offset))
      (if-let shift_amt (valid_ldr_shift ty shift))
      (aarch64_ldr_shifted ty base index shift_amt))

;; load: Load from memory with commuted register+shifted register offset
(rule (lower (load ty (iadd ptr_ty (ishl idx_ty index (iconst shift)) base) _flags _mem_offset))
      (if-let shift_amt (valid_ldr_shift ty shift))
      (aarch64_ldr_shifted ty base index shift_amt))

;; load: Load from memory (base register only)
(rule (lower (load ty addr _flags _offset))
      (aarch64_ldr ty addr))

;; store: Store to memory with register+immediate offset
;; Pattern: store(val, iadd(base, iconst(offset))) => STR Xt, [Xn, #offset]
(rule (lower (store val (iadd ptr_ty base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_str_imm_offset val offset))
      (aarch64_str_imm val base scaled_offset))

;; store: Store to memory with commuted register+immediate offset
(rule (lower (store val (iadd ptr_ty (iconst offset) base) _flags _mem_offset))
      (if-let scaled_offset (valid_str_imm_offset val offset))
      (aarch64_str_imm val base scaled_offset))

;; store: Store to memory with register+register offset
;; Pattern: store(val, iadd(base, index)) => STR Xt, [Xn, Xm]
(rule (lower (store val (iadd ptr_ty base index) _flags _mem_offset))
      (aarch64_str_reg val base index))

;; store: Store to memory with register+extended register offset
;; Pattern: store(val, iadd(base, sxtw(index))) => STR Xt, [Xn, Wm, SXTW]
(rule (lower (store val (iadd ptr_ty base (aarch64_sxtw index)) _flags _mem_offset))
      (aarch64_str_ext val base index ExtendOp.sxtw))

;; store: Store to memory with commuted register+extended register offset
(rule (lower (store val (iadd ptr_ty (aarch64_sxtw index) base) _flags _mem_offset))
      (aarch64_str_ext val base index ExtendOp.sxtw))

;; store: Store to memory with register+shifted register offset
;; Pattern: store(val, iadd(base, ishl(index, iconst(shift)))) => STR Xt, [Xn, Xm, LSL #shift]
(rule (lower (store val (iadd ptr_ty base (ishl idx_ty index (iconst shift))) _flags _mem_offset))
      (if-let shift_amt (valid_str_shift val shift))
      (aarch64_str_shifted val base index shift_amt))

;; store: Store to memory with commuted register+shifted register offset
(rule (lower (store val (iadd ptr_ty (ishl idx_ty index (iconst shift)) base) _flags _mem_offset))
      (if-let shift_amt (valid_str_shift val shift))
      (aarch64_str_shifted val base index shift_amt))

;; store: Store to memory (base register only)
(rule (lower (store val addr _flags _offset))
      (aarch64_str val addr))

;; Pre-index addressing modes

;; load with pre-index: base+=offset, then load
;; Pattern: load(pre_inc(base, offset)) => LDR Xt, [Xn, #offset]!
(rule (lower (load ty (pre_inc base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_ldr_imm_offset ty offset))
      (aarch64_ldr_pre ty base scaled_offset))

;; store with pre-index: base+=offset, then store
;; Pattern: store(val, pre_inc(base, offset)) => STR Xt, [Xn, #offset]!
(rule (lower (store val (pre_inc base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_str_imm_offset val offset))
      (aarch64_str_pre val base scaled_offset))

;; Post-index addressing modes

;; load with post-index: load, then base+=offset
;; Pattern: load(post_inc(base, offset)) => LDR Xt, [Xn], #offset
(rule (lower (load ty (post_inc base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_ldr_imm_offset ty offset))
      (aarch64_ldr_post ty base scaled_offset))

;; store with post-index: store, then base+=offset
;; Pattern: store(val, post_inc(base, offset)) => STR Xt, [Xn], #offset
(rule (lower (store val (post_inc base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_str_imm_offset val offset))
      (aarch64_str_post val base scaled_offset))

;; Load pair optimization (LDP)
;; Pattern: two adjacent loads with 8-byte offset
;; load(base+0) and load(base+8) => LDP X0, X1, [base]
(rule (lower (load_pair ty base offset1 offset2))
      (if-let $true (is_ldp_valid_offset offset1 offset2))
      (aarch64_ldp ty base offset1))

;; Store pair optimization (STP)
;; Pattern: two adjacent stores with 8-byte offset
;; store(val1, base+0) and store(val2, base+8) => STP X0, X1, [base]
(rule (lower (store_pair val1 val2 base offset1 offset2))
      (if-let $true (is_stp_valid_offset offset1 offset2))
      (aarch64_stp val1 val2 base offset1))

;; Control flow lowering

;; jump: Unconditional branch
(rule (lower (jump target))
      (aarch64_b target))

;; brif: Conditional branch
(rule (lower (brif cond target))
      (aarch64_b_cond cond target))

;; cbz: Compare-and-branch-zero optimization
;; Pattern: if (x == 0) goto target
(rule (lower (brif (icmp eq x (iconst 0)) target))
      (aarch64_cbz x target))

;; cbnz: Compare-and-branch-nonzero optimization
;; Pattern: if (x != 0) goto target
(rule (lower (brif (icmp ne x (iconst 0)) target))
      (aarch64_cbnz x target))

;; tbz: Test-bit-and-branch-zero optimization
;; Pattern: if ((x & (1 << bit)) == 0) goto target
(rule (lower (brif (icmp eq (band x (ishl (iconst 1) bit)) (iconst 0)) target))
      (aarch64_tbz x bit target))

;; tbnz: Test-bit-and-branch-nonzero optimization
;; Pattern: if ((x & (1 << bit)) != 0) goto target
(rule (lower (brif (icmp ne (band x (ishl (iconst 1) bit)) (iconst 0)) target))
      (aarch64_tbnz x bit target))

;; return: Return from function
(rule (lower (return))
      (aarch64_ret))

;; Floating-point conversion lowering

;; fcvt_to_uint: Convert float to unsigned integer (FCVTZU)
(rule (lower (fcvt_to_uint (value_type $F32) x (value_type $I32)))
      (aarch64_fcvtzu_32 x))

(rule (lower (fcvt_to_uint (value_type $F32) x (value_type $I64)))
      (aarch64_fcvtzu_64 x))

(rule (lower (fcvt_to_uint (value_type $F64) x (value_type $I32)))
      (aarch64_fcvtzu_32 x))

(rule (lower (fcvt_to_uint (value_type $F64) x (value_type $I64)))
      (aarch64_fcvtzu_64 x))

;; fcvt_to_sint: Convert float to signed integer (FCVTZS)
(rule (lower (fcvt_to_sint (value_type $F32) x (value_type $I32)))
      (aarch64_fcvtzs_32 x))

(rule (lower (fcvt_to_sint (value_type $F32) x (value_type $I64)))
      (aarch64_fcvtzs_64 x))

(rule (lower (fcvt_to_sint (value_type $F64) x (value_type $I32)))
      (aarch64_fcvtzs_32 x))

(rule (lower (fcvt_to_sint (value_type $F64) x (value_type $I64)))
      (aarch64_fcvtzs_64 x))

;; fcvt_from_uint: Convert unsigned integer to float (UCVTF)
(rule (lower (fcvt_from_uint (value_type $I32) x (value_type $F32)))
      (aarch64_ucvtf_32_to_f32 x))

(rule (lower (fcvt_from_uint (value_type $I32) x (value_type $F64)))
      (aarch64_ucvtf_32_to_f64 x))

(rule (lower (fcvt_from_uint (value_type $I64) x (value_type $F32)))
      (aarch64_ucvtf_64_to_f32 x))

(rule (lower (fcvt_from_uint (value_type $I64) x (value_type $F64)))
      (aarch64_ucvtf_64_to_f64 x))

;; fcvt_from_sint: Convert signed integer to float (SCVTF)
(rule (lower (fcvt_from_sint (value_type $I32) x (value_type $F32)))
      (aarch64_scvtf_32_to_f32 x))

(rule (lower (fcvt_from_sint (value_type $I32) x (value_type $F64)))
      (aarch64_scvtf_32_to_f64 x))

(rule (lower (fcvt_from_sint (value_type $I64) x (value_type $F32)))
      (aarch64_scvtf_64_to_f32 x))

(rule (lower (fcvt_from_sint (value_type $I64) x (value_type $F64)))
      (aarch64_scvtf_64_to_f64 x))

;; fcvt_to_sint_sat: Convert float to signed integer with saturation
;; ARM64 FCVTZS natively saturates: NaN→0, overflow→INT_MAX/MIN
(rule (lower (has_type $I32 (fcvt_to_sint_sat x @ (value_type $F32))))
      (aarch64_fcvtzs_32_to_32 x))

(rule (lower (has_type $I32 (fcvt_to_sint_sat x @ (value_type $F64))))
      (aarch64_fcvtzs_64_to_32 x))

(rule (lower (has_type $I64 (fcvt_to_sint_sat x @ (value_type $F32))))
      (aarch64_fcvtzs_32_to_64 x))

(rule (lower (has_type $I64 (fcvt_to_sint_sat x @ (value_type $F64))))
      (aarch64_fcvtzs_64_to_64 x))

;; fcvt_to_uint_sat: Convert float to unsigned integer with saturation
;; ARM64 FCVTZU natively saturates: NaN→0, negative→0, overflow→UINT_MAX
(rule (lower (has_type $I32 (fcvt_to_uint_sat x @ (value_type $F32))))
      (aarch64_fcvtzu_32_to_32 x))

(rule (lower (has_type $I32 (fcvt_to_uint_sat x @ (value_type $F64))))
      (aarch64_fcvtzu_64_to_32 x))

(rule (lower (has_type $I64 (fcvt_to_uint_sat x @ (value_type $F32))))
      (aarch64_fcvtzu_32_to_64 x))

(rule (lower (has_type $I64 (fcvt_to_uint_sat x @ (value_type $F64))))
      (aarch64_fcvtzu_64_to_64 x))

;; fdemote: Convert f64 to f32 (FCVT)
(rule (lower (fdemote x))
      (aarch64_fcvt_f64_to_f32 x))

;; fpromote: Convert f32 to f64 (FCVT)
(rule (lower (fpromote x))
      (aarch64_fcvt_f32_to_f64 x))

;; Floating-point arithmetic

;; fadd: FP addition (FADD)
(rule (lower (fadd ty x y))
      (aarch64_fadd ty x y))

;; fsub: FP subtraction (FSUB)
(rule (lower (fsub ty x y))
      (aarch64_fsub ty x y))

;; fmul: FP multiplication (FMUL)
(rule (lower (fmul ty x y))
      (aarch64_fmul ty x y))

;; fdiv: FP division (FDIV)
(rule (lower (fdiv ty x y))
      (aarch64_fdiv ty x y))

;; fsqrt: FP square root (FSQRT)
(rule (lower (fsqrt ty x))
      (aarch64_fsqrt ty x))

;; fma: FP fused multiply-add (FMADD)
;; fma(x, y, z) = x * y + z
(rule (lower (fma ty x y z))
      (aarch64_fmadd ty x y z))

;; fcmp: FP comparison (FCMP)
(rule (lower (fcmp cond x y))
      (aarch64_fcmp x y))

;; fabs: FP absolute value (FABS)
(rule (lower (fabs ty x))
      (aarch64_fabs ty x))

;; fneg: FP negation (FNEG)
(rule (lower (fneg ty x))
      (aarch64_fneg ty x))

;; fcopysign: Copy sign from y to magnitude of x
;; Implemented as: abs_x = fabs(x), result = y < 0 ? -abs_x : abs_x
(rule (lower (has_type $F32 (fcopysign x y)))
      (aarch64_fcopysign_32 x y))

(rule (lower (has_type $F64 (fcopysign x y)))
      (aarch64_fcopysign_64 x y))

;; Constant materialization

;; iconst: Integer constant (16-bit immediate via MOVZ)
(rule (lower (iconst ty k))
      (if-let $true (uimm16 k))
      (aarch64_movz ty k))

;; iconst: Larger constants (needs multiple instructions)
(rule (lower (iconst ty k))
      (aarch64_iconst ty k))

;; Helper extractors

;; Immediate value extractors (extern - implemented in isle_helpers.zig)

;; Check if value fits in unsigned 12-bit (0-4095)
(decl uimm12 (u64) u64)
(extern extractor uimm12 uimm12)

;; Check if value fits in unsigned 16-bit (0-65535)
(decl uimm16 (u64) u64)
(extern extractor uimm16 uimm16)

;; Check if value is a valid shift amount (0-63)
(decl valid_shift_imm (u64) u64)
(extern extractor valid_shift_imm valid_shift_imm)

;; Extract rotl immediate and convert to rotr immediate
;; rotl(x, k) = rotr(x, width - k)
(decl valid_rotl_imm (u32 u64) u32)
(extern extractor valid_rotl_imm valid_rotl_imm)

;; Check if value is in range where negation fits in unsigned 12-bit
;; Returns true if -4095 <= val <= -1
(decl in_neg_uimm12_range (i64) Unit)
(extern extractor in_neg_uimm12_range in_neg_uimm12_range)

;; Addressing mode extractors

;; Check if offset is valid for load immediate addressing
;; Accepts offsets 0-32760 (max for I64 8-byte aligned access)
(decl valid_ldr_imm_offset (Type u64) u64)
(extern extractor valid_ldr_imm_offset valid_ldr_imm_offset)

;; Check if offset is valid for store immediate addressing
(decl valid_str_imm_offset (Value u64) u64)
(extern extractor valid_str_imm_offset valid_str_imm_offset)

;; Check if shift is valid for load (must be 0-3)
(decl valid_ldr_shift (Type u64) u64)
(extern extractor valid_ldr_shift valid_ldr_shift)

;; Check if shift is valid for store (must be 0-3)
(decl valid_str_shift (Value u64) u64)
(extern extractor valid_str_shift valid_str_shift)

;; Type declarations

;; Integer comparison condition codes
(type IntCC (enum
  Equal
  NotEqual
  SignedLessThan
  SignedLessThanOrEqual
  SignedGreaterThan
  SignedGreaterThanOrEqual
  UnsignedLessThan
  UnsignedLessThanOrEqual
  UnsignedGreaterThan
  UnsignedGreaterThanOrEqual
))

;; Extend operations for extended register operands
(type ExtendOp (enum
  uxtb  ;; Zero-extend byte (8-bit)
  uxth  ;; Zero-extend halfword (16-bit)
  uxtw  ;; Zero-extend word (32-bit to 64-bit)
  uxtx  ;; Zero-extend doubleword (no-op for 64-bit)
  sxtb  ;; Sign-extend byte (8-bit)
  sxth  ;; Sign-extend halfword (16-bit)
  sxtw  ;; Sign-extend word (32-bit to 64-bit)
  sxtx  ;; Sign-extend doubleword (no-op for 64-bit)
))

;; Multi-register value type for wide operations (I128)
;; Maps to Zig's ValueRegs(VReg) type
(type ValueRegs extern (enum))

;; Constructor terms for aarch64 instructions

(type Aarch64Inst (enum))

;; Arithmetic
(decl aarch64_add_rr (Type Value Value) Aarch64Inst)
(decl aarch64_add_imm (Type Value i64) Aarch64Inst)
(decl aarch64_add_extended (Type Value Value ExtendOp) Aarch64Inst)
(decl aarch64_sub_rr (Type Value Value) Aarch64Inst)
(decl aarch64_sub_imm (Type Value i64) Aarch64Inst)
(decl aarch64_mul_rr (Type Value Value) Aarch64Inst)
(decl aarch64_madd (Type Value Value Value) Aarch64Inst)
(decl aarch64_msub (Type Value Value Value) Aarch64Inst)
(decl aarch64_smulh (Value Value) Aarch64Inst)
(decl aarch64_umulh (Value Value) Aarch64Inst)
(decl aarch64_sdiv (Type Value Value) Aarch64Inst)
(decl aarch64_udiv (Type Value Value) Aarch64Inst)
(decl aarch64_smin (Type Value Value) Aarch64Inst)
(decl aarch64_umin (Type Value Value) Aarch64Inst)
(decl aarch64_smax (Type Value Value) Aarch64Inst)
(decl aarch64_umax (Type Value Value) Aarch64Inst)
(decl aarch64_bitselect (Value Value Value) Aarch64Inst)
(decl aarch64_iabs (Type Value) Aarch64Inst)
(decl aarch64_isplit (Value) ValueRegs)

;; Saturating arithmetic operations
(decl aarch64_sqadd_8 (Value Value) Aarch64Inst)
(decl aarch64_sqadd_16 (Value Value) Aarch64Inst)
(decl aarch64_sqadd_32 (Value Value) Aarch64Inst)
(decl aarch64_sqadd_64 (Value Value) Aarch64Inst)
(decl aarch64_sqsub_8 (Value Value) Aarch64Inst)
(decl aarch64_sqsub_16 (Value Value) Aarch64Inst)
(decl aarch64_sqsub_32 (Value Value) Aarch64Inst)
(decl aarch64_sqsub_64 (Value Value) Aarch64Inst)
(decl aarch64_uqadd_8 (Value Value) Aarch64Inst)
(decl aarch64_uqadd_16 (Value Value) Aarch64Inst)
(decl aarch64_uqadd_32 (Value Value) Aarch64Inst)
(decl aarch64_uqadd_64 (Value Value) Aarch64Inst)
(decl aarch64_uqsub_8 (Value Value) Aarch64Inst)
(decl aarch64_uqsub_16 (Value Value) Aarch64Inst)
(decl aarch64_uqsub_32 (Value Value) Aarch64Inst)
(decl aarch64_uqsub_64 (Value Value) Aarch64Inst)

;; Shift operations
(decl aarch64_lsl_rr (Type Value Value) Aarch64Inst)
(decl aarch64_lsl_imm (Type Value i64) Aarch64Inst)
(decl aarch64_lsr_rr (Type Value Value) Aarch64Inst)
(decl aarch64_lsr_imm (Type Value i64) Aarch64Inst)
(decl aarch64_asr_rr (Type Value Value) Aarch64Inst)
(decl aarch64_asr_imm (Type Value i64) Aarch64Inst)
(decl aarch64_ror_rr (Type Value Value) Aarch64Inst)
(decl aarch64_ror_imm (Type Value i64) Aarch64Inst)
(decl aarch64_rotl_rr (Type Value Value) Aarch64Inst)

;; Bitwise operations
(decl aarch64_and_rr (Type Value Value) Aarch64Inst)
(decl aarch64_orr_rr (Type Value Value) Aarch64Inst)
(decl aarch64_orr_imm (Type Value ImmLogic) Aarch64Inst)
(decl aarch64_eor_rr (Type Value Value) Aarch64Inst)
(decl aarch64_eor_imm (Type Value ImmLogic) Aarch64Inst)
(decl aarch64_mvn_rr (Type Value) Aarch64Inst)
(decl aarch64_bic_rr (Type Value Value) Aarch64Inst)
(decl aarch64_orn_rr (Type Value Value) Aarch64Inst)
(decl aarch64_eon_rr (Type Value Value) Aarch64Inst)

;; Bit manipulation operations
(decl aarch64_clz_32 (Value) Aarch64Inst)
(decl aarch64_clz_64 (Value) Aarch64Inst)
(decl aarch64_cls_32 (Value) Aarch64Inst)
(decl aarch64_cls_64 (Value) Aarch64Inst)
(decl aarch64_ctz_32 (Value) Aarch64Inst)
(decl aarch64_ctz_64 (Value) Aarch64Inst)
(decl aarch64_rbit_32 (Value) Aarch64Inst)
(decl aarch64_rbit_64 (Value) Aarch64Inst)
(decl aarch64_bswap_16 (Value) Aarch64Inst)
(decl aarch64_bswap_32 (Value) Aarch64Inst)
(decl aarch64_bswap_64 (Value) Aarch64Inst)

;; Population count operations
(decl aarch64_popcnt_8 (Value) Aarch64Inst)
(decl aarch64_popcnt_16 (Value) Aarch64Inst)
(decl aarch64_popcnt_32 (Value) Aarch64Inst)
(decl aarch64_popcnt_64 (Value) Aarch64Inst)

;; Negation operations
(decl aarch64_neg (Type Value) Aarch64Inst)
(decl aarch64_ngc (Type Value) Aarch64Inst)
(decl aarch64_iabs_8 (Value) Aarch64Inst)
(decl aarch64_iabs_16 (Value) Aarch64Inst)
(decl aarch64_iabs_32 (Value) Aarch64Inst)
(decl aarch64_iabs_64 (Value) Aarch64Inst)

;; Sign/zero extend operations
(decl aarch64_sxtb (Type Value) Aarch64Inst)
(decl aarch64_sxth (Type Value) Aarch64Inst)
(decl aarch64_sxtw (Value) Aarch64Inst)
(decl aarch64_uxtb (Type Value) Aarch64Inst)
(decl aarch64_uxth (Type Value) Aarch64Inst)

;; Comparison operations
(decl aarch64_cmp_rr (Type Value Value IntCC) Aarch64Inst)
(decl aarch64_cmp_imm (Type Value i64 IntCC) Aarch64Inst)

;; Conditional select operations
(decl aarch64_csel (Type Value Value Value IntCC) Aarch64Inst)
(decl aarch64_csinc (Type Value Value Value IntCC) Aarch64Inst)
(decl aarch64_csinv (Type Value Value Value IntCC) Aarch64Inst)
(decl aarch64_csneg (Type Value Value Value IntCC) Aarch64Inst)

;; Memory
(decl aarch64_ldr (Type Value) Aarch64Inst)
(decl aarch64_ldr_imm (Type Value i64) Aarch64Inst)
(decl aarch64_ldr_reg (Type Value Value) Aarch64Inst)
(decl aarch64_ldr_ext (Type Value Value ExtendOp) Aarch64Inst)
(decl aarch64_ldr_shifted (Type Value Value i64) Aarch64Inst)
(decl aarch64_ldr_pre (Type Value i64) Aarch64Inst)   ;; Pre-index: base+=offset, then load
(decl aarch64_ldr_post (Type Value i64) Aarch64Inst)  ;; Post-index: load, then base+=offset
(decl aarch64_str (Value Value) Aarch64Inst)
(decl aarch64_str_imm (Value Value i64) Aarch64Inst)
(decl aarch64_str_reg (Value Value Value) Aarch64Inst)
(decl aarch64_str_ext (Value Value Value ExtendOp) Aarch64Inst)
(decl aarch64_str_shifted (Value Value Value i64) Aarch64Inst)
(decl aarch64_str_pre (Value Value i64) Aarch64Inst)  ;; Pre-index: base+=offset, then store
(decl aarch64_str_post (Value Value i64) Aarch64Inst) ;; Post-index: store, then base+=offset

;; Control flow
(decl aarch64_b (Block) Aarch64Inst)
(decl aarch64_b_cond (Value Block) Aarch64Inst)
(decl aarch64_ret () Aarch64Inst)

;; Constants
(decl aarch64_movz (Type i64) Aarch64Inst)
(decl aarch64_iconst (Type i64) Aarch64Inst)

;; Floating-point arithmetic
(decl aarch64_fadd (Type Value Value) Aarch64Inst)
(decl aarch64_fsub (Type Value Value) Aarch64Inst)
(decl aarch64_fmul (Type Value Value) Aarch64Inst)
(decl aarch64_fdiv (Type Value Value) Aarch64Inst)
(decl aarch64_fmin (Type Value Value) Aarch64Inst)
(decl aarch64_fmax (Type Value Value) Aarch64Inst)
(decl aarch64_fsqrt (Type Value) Aarch64Inst)
(decl aarch64_splat (Type Value) Aarch64Inst)
(decl aarch64_extractlane (Type Value Value) Aarch64Inst)
(decl aarch64_insertlane (Type Value Value Value) Aarch64Inst)

;; Floating-point conversions
(decl aarch64_fcvtzu_32 (Value) Aarch64Inst)  ;; FCVTZU: float to unsigned i32
(decl aarch64_fcvtzu_64 (Value) Aarch64Inst)  ;; FCVTZU: float to unsigned i64
(decl aarch64_fcvtzs_32 (Value) Aarch64Inst)  ;; FCVTZS: float to signed i32
(decl aarch64_fcvtzs_64 (Value) Aarch64Inst)  ;; FCVTZS: float to signed i64
(decl aarch64_ucvtf_32_to_f32 (Value) Aarch64Inst)  ;; UCVTF: u32 to f32
(decl aarch64_ucvtf_32_to_f64 (Value) Aarch64Inst)  ;; UCVTF: u32 to f64
(decl aarch64_ucvtf_64_to_f32 (Value) Aarch64Inst)  ;; UCVTF: u64 to f32
(decl aarch64_ucvtf_64_to_f64 (Value) Aarch64Inst)  ;; UCVTF: u64 to f64
(decl aarch64_scvtf_32_to_f32 (Value) Aarch64Inst)  ;; SCVTF: i32 to f32
(decl aarch64_scvtf_32_to_f64 (Value) Aarch64Inst)  ;; SCVTF: i32 to f64
(decl aarch64_scvtf_64_to_f32 (Value) Aarch64Inst)  ;; SCVTF: i64 to f32
(decl aarch64_scvtf_64_to_f64 (Value) Aarch64Inst)  ;; SCVTF: i64 to f64
(decl aarch64_fcvt_f64_to_f32 (Value) Aarch64Inst)  ;; FCVT: f64 to f32
(decl aarch64_fcvt_f32_to_f64 (Value) Aarch64Inst)  ;; FCVT: f32 to f64

;; Float to integer conversions with saturation
(decl aarch64_fcvtzs_32_to_32 (Value) Aarch64Inst)  ;; FCVTZS: f32 to i32 (saturating)
(decl aarch64_fcvtzs_64_to_32 (Value) Aarch64Inst)  ;; FCVTZS: f64 to i32 (saturating)
(decl aarch64_fcvtzs_32_to_64 (Value) Aarch64Inst)  ;; FCVTZS: f32 to i64 (saturating)
(decl aarch64_fcvtzs_64_to_64 (Value) Aarch64Inst)  ;; FCVTZS: f64 to i64 (saturating)
(decl aarch64_fcvtzu_32_to_32 (Value) Aarch64Inst)  ;; FCVTZU: f32 to u32 (saturating)
(decl aarch64_fcvtzu_64_to_32 (Value) Aarch64Inst)  ;; FCVTZU: f64 to u32 (saturating)
(decl aarch64_fcvtzu_32_to_64 (Value) Aarch64Inst)  ;; FCVTZU: f32 to u64 (saturating)
(decl aarch64_fcvtzu_64_to_64 (Value) Aarch64Inst)  ;; FCVTZU: f64 to u64 (saturating)

;; General floating-point conversion (type-generic)
(decl aarch64_scvtf (Type Type Value) Aarch64Inst)  ;; SCVTF: signed int to float
(decl aarch64_ucvtf (Type Type Value) Aarch64Inst)  ;; UCVTF: unsigned int to float
(decl aarch64_fpromote (Value) Aarch64Inst)         ;; FCVT: f32 to f64
(decl aarch64_fdemote (Value) Aarch64Inst)          ;; FCVT: f64 to f32

;; Floating-point rounding operations
(decl aarch64_nearest (Type Value) Aarch64Inst)     ;; FRINTN: round to nearest
(decl aarch64_trunc (Type Value) Aarch64Inst)       ;; FRINTZ: round toward zero
(decl aarch64_ceil (Type Value) Aarch64Inst)        ;; FRINTP: round toward +infinity
(decl aarch64_floor (Type Value) Aarch64Inst)       ;; FRINTM: round toward -infinity

;; Multi-register value operations
(decl value_regs_from_values (Value Value) ValueRegs)  ;; Construct ValueRegs from two I64 values
(extern constructor value_regs_from_values value_regs_from_values)

;; Atomic operations
(decl aarch64_atomic_load_acquire (Type Value) Aarch64Inst)   ;; LDAR: atomic load with acquire
(decl aarch64_atomic_store_release (Type Value Value) Aarch64Inst)  ;; STLR: atomic store with release
(decl aarch64_fence (AtomicOrdering) Aarch64Inst)  ;; DMB: memory fence

;; Floating-point arithmetic
(decl aarch64_fadd (Type Value Value) Aarch64Inst)
(decl aarch64_fsub (Type Value Value) Aarch64Inst)
(decl aarch64_fmul (Type Value Value) Aarch64Inst)
(decl aarch64_fdiv (Type Value Value) Aarch64Inst)
(decl aarch64_fmadd (Type Value Value Value) Aarch64Inst)
(decl aarch64_fcmp (Value Value) Aarch64Inst)
(decl aarch64_fabs (Type Value) Aarch64Inst)
(decl aarch64_fneg (Type Value) Aarch64Inst)
(decl aarch64_fcopysign_32 (Value Value) Aarch64Inst)
(decl aarch64_fcopysign_64 (Value Value) Aarch64Inst)

;; SIMD/Vector operations
(decl aarch64_vec_add (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_sub (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_mul (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_smin (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_smax (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_umin (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_umax (VectorSize Value Value) Aarch64Inst)

;; SIMD/Vector reductions (horizontal operations)
(decl aarch64_addv (VectorSize Value) Aarch64Inst)     ;; Horizontal add across vector
(decl aarch64_sminv (VectorSize Value) Aarch64Inst)    ;; Horizontal signed min
(decl aarch64_smaxv (VectorSize Value) Aarch64Inst)    ;; Horizontal signed max
(decl aarch64_uminv (VectorSize Value) Aarch64Inst)    ;; Horizontal unsigned min
(decl aarch64_umaxv (VectorSize Value) Aarch64Inst)    ;; Horizontal unsigned max

;; SIMD lane manipulation
(decl aarch64_dup (VectorSize Value) Aarch64Inst)             ;; Duplicate scalar to all lanes
(decl aarch64_extract_lane (Value Value) Aarch64Inst)         ;; Extract lane to scalar
(decl aarch64_addp (VectorSize Value Value) Aarch64Inst)      ;; Add adjacent pairs

;; Atomic operations (LSE - Large System Extensions)
(decl aarch64_ldar (Value) Aarch64Inst)                ;; Load-acquire register
(decl aarch64_stlr (Value Value) Aarch64Inst)          ;; Store-release register
(decl aarch64_ldadd (Value Value) Aarch64Inst)         ;; Atomic add with acquire/release
(decl aarch64_ldclr (Value Value) Aarch64Inst)         ;; Atomic clear bits with acquire/release
(decl aarch64_ldeor (Value Value) Aarch64Inst)         ;; Atomic XOR with acquire/release
(decl aarch64_ldset (Value Value) Aarch64Inst)         ;; Atomic OR (set bits) with acquire/release
(decl aarch64_ldsmax (Value Value) Aarch64Inst)        ;; Atomic signed max with acquire/release
(decl aarch64_ldsmin (Value Value) Aarch64Inst)        ;; Atomic signed min with acquire/release
(decl aarch64_ldumax (Value Value) Aarch64Inst)        ;; Atomic unsigned max with acquire/release
(decl aarch64_ldumin (Value Value) Aarch64Inst)        ;; Atomic unsigned min with acquire/release
(decl aarch64_swpal (Value Value) Aarch64Inst)         ;; Atomic swap with acquire/release
(decl aarch64_casal (Value Value Value) Aarch64Inst)   ;; Compare-and-swap with acquire/release

;; LSE Atomic helper implementations
;; These generate atomic instructions with sequentially-consistent ordering

;; aarch64_ldadd: Atomic add - LDADDAL (acquire-release for seq_cst)
(rule (aarch64_ldadd addr val)
      ;; TODO: Select ordering variant based on context
      ;; For now, use sequentially-consistent (acquire-release)
      (Aarch64Inst.ldaddal val addr))

;; aarch64_ldclr: Atomic clear bits - LDCLRAL
(rule (aarch64_ldclr addr val)
      (Aarch64Inst.ldclral val addr))

;; aarch64_ldeor: Atomic XOR - LDEORAL
(rule (aarch64_ldeor addr val)
      (Aarch64Inst.ldeoral val addr))

;; aarch64_ldset: Atomic OR - LDSETAL
(rule (aarch64_ldset addr val)
      (Aarch64Inst.ldsetal val addr))

;; aarch64_ldsmax: Atomic signed maximum - LDSMAXAL
(rule (aarch64_ldsmax addr val)
      (Aarch64Inst.ldsmaxal val addr))

;; aarch64_ldsmin: Atomic signed minimum - LDSMINAL
(rule (aarch64_ldsmin addr val)
      (Aarch64Inst.ldsminal val addr))

;; aarch64_ldumax: Atomic unsigned maximum - LDUMAXAL
(rule (aarch64_ldumax addr val)
      (Aarch64Inst.ldumaxal val addr))

;; aarch64_ldumin: Atomic unsigned minimum - LDUMINAL
(rule (aarch64_ldumin addr val)
      (Aarch64Inst.lduminal val addr))

;; aarch64_swpal: Atomic swap - SWPAL
(rule (aarch64_swpal addr val)
      (Aarch64Inst.swpal val addr))

;; aarch64_casal: Compare-and-swap - CASAL
(rule (aarch64_casal addr expected new_val)
      (Aarch64Inst.casal expected new_val addr))

;; Memory barrier operations
(decl aarch64_dmb (ShareabilityDomain) Aarch64Inst)    ;; Data Memory Barrier
(decl aarch64_dsb (ShareabilityDomain) Aarch64Inst)    ;; Data Synchronization Barrier
(decl aarch64_isb () Aarch64Inst)                      ;; Instruction Synchronization Barrier

;; Shareability domain for memory barrier instructions
(type ShareabilityDomain (enum
  SY      ;; Full system - all agents
  ISH     ;; Inner shareable - processors in same inner shareable domain
  ISHLD   ;; Inner shareable load - acquire barrier for inner shareable
  ISHST   ;; Inner shareable store - release barrier for inner shareable
  NSH     ;; Non-shareable - only this processor
  OSH     ;; Outer shareable - processors in same outer shareable domain
))

;; Vector type enum
(type VectorSize (enum
  V8B   ;; 8 bytes (8x i8)
  V16B  ;; 16 bytes (16x i8)
  V4H   ;; 4 halfwords (4x i16)
  V8H   ;; 8 halfwords (8x i16)
  V2S   ;; 2 singles (2x i32/f32)
  V4S   ;; 4 singles (4x i32/f32)
  V2D   ;; 2 doubles (2x i64/f64)
))

;; SIMD/Vector lowering rules

;; viadd: Vector integer addition
(rule (lower (viadd (vector_type V8B) x y))
      (aarch64_vec_add V8B x y))

(rule (lower (viadd (vector_type V16B) x y))
      (aarch64_vec_add V16B x y))

(rule (lower (viadd (vector_type V4H) x y))
      (aarch64_vec_add V4H x y))

(rule (lower (viadd (vector_type V8H) x y))
      (aarch64_vec_add V8H x y))

(rule (lower (viadd (vector_type V2S) x y))
      (aarch64_vec_add V2S x y))

(rule (lower (viadd (vector_type V4S) x y))
      (aarch64_vec_add V4S x y))

(rule (lower (viadd (vector_type V2D) x y))
      (aarch64_vec_add V2D x y))

;; visub: Vector integer subtraction
(rule (lower (visub (vector_type V8B) x y))
      (aarch64_vec_sub V8B x y))

(rule (lower (visub (vector_type V16B) x y))
      (aarch64_vec_sub V16B x y))

(rule (lower (visub (vector_type V4H) x y))
      (aarch64_vec_sub V4H x y))

(rule (lower (visub (vector_type V8H) x y))
      (aarch64_vec_sub V8H x y))

(rule (lower (visub (vector_type V2S) x y))
      (aarch64_vec_sub V2S x y))

(rule (lower (visub (vector_type V4S) x y))
      (aarch64_vec_sub V4S x y))

(rule (lower (visub (vector_type V2D) x y))
      (aarch64_vec_sub V2D x y))

;; vimul: Vector integer multiplication
(rule (lower (vimul (vector_type V8B) x y))
      (aarch64_vec_mul V8B x y))

(rule (lower (vimul (vector_type V16B) x y))
      (aarch64_vec_mul V16B x y))

(rule (lower (vimul (vector_type V4H) x y))
      (aarch64_vec_mul V4H x y))

(rule (lower (vimul (vector_type V8H) x y))
      (aarch64_vec_mul V8H x y))

(rule (lower (vimul (vector_type V2S) x y))
      (aarch64_vec_mul V2S x y))

(rule (lower (vimul (vector_type V4S) x y))
      (aarch64_vec_mul V4S x y))

;; vimin: Vector signed minimum
(rule (lower (vimin (vector_type V8B) x y))
      (aarch64_vec_smin V8B x y))

(rule (lower (vimin (vector_type V16B) x y))
      (aarch64_vec_smin V16B x y))

(rule (lower (vimin (vector_type V4H) x y))
      (aarch64_vec_smin V4H x y))

(rule (lower (vimin (vector_type V8H) x y))
      (aarch64_vec_smin V8H x y))

(rule (lower (vimin (vector_type V2S) x y))
      (aarch64_vec_smin V2S x y))

(rule (lower (vimin (vector_type V4S) x y))
      (aarch64_vec_smin V4S x y))

;; vimax: Vector signed maximum
(rule (lower (vimax (vector_type V8B) x y))
      (aarch64_vec_smax V8B x y))

(rule (lower (vimax (vector_type V16B) x y))
      (aarch64_vec_smax V16B x y))

(rule (lower (vimax (vector_type V4H) x y))
      (aarch64_vec_smax V4H x y))

(rule (lower (vimax (vector_type V8H) x y))
      (aarch64_vec_smax V8H x y))

(rule (lower (vimax (vector_type V2S) x y))
      (aarch64_vec_smax V2S x y))

(rule (lower (vimax (vector_type V4S) x y))
      (aarch64_vec_smax V4S x y))

;; Vector reduction lowering rules

;; vreduce_add: Horizontal vector addition (sum all elements)
(rule (lower (vreduce_add (vector_type V8B) x))
      (aarch64_addv V8B x))

(rule (lower (vreduce_add (vector_type V16B) x))
      (aarch64_addv V16B x))

(rule (lower (vreduce_add (vector_type V4H) x))
      (aarch64_addv V4H x))

(rule (lower (vreduce_add (vector_type V8H) x))
      (aarch64_addv V8H x))

(rule (lower (vreduce_add (vector_type V2S) x))
      (aarch64_addv V2S x))

(rule (lower (vreduce_add (vector_type V4S) x))
      (aarch64_addv V4S x))

;; vreduce_smin: Horizontal signed minimum
(rule (lower (vreduce_smin (vector_type V8B) x))
      (aarch64_sminv V8B x))

(rule (lower (vreduce_smin (vector_type V16B) x))
      (aarch64_sminv V16B x))

(rule (lower (vreduce_smin (vector_type V4H) x))
      (aarch64_sminv V4H x))

(rule (lower (vreduce_smin (vector_type V8H) x))
      (aarch64_sminv V8H x))

(rule (lower (vreduce_smin (vector_type V4S) x))
      (aarch64_sminv V4S x))

;; vreduce_smax: Horizontal signed maximum
(rule (lower (vreduce_smax (vector_type V8B) x))
      (aarch64_smaxv V8B x))

(rule (lower (vreduce_smax (vector_type V16B) x))
      (aarch64_smaxv V16B x))

(rule (lower (vreduce_smax (vector_type V4H) x))
      (aarch64_smaxv V4H x))

(rule (lower (vreduce_smax (vector_type V8H) x))
      (aarch64_smaxv V8H x))

(rule (lower (vreduce_smax (vector_type V4S) x))
      (aarch64_smaxv V4S x))

;; vreduce_umin: Horizontal unsigned minimum
(rule (lower (vreduce_umin (vector_type V8B) x))
      (aarch64_uminv V8B x))

(rule (lower (vreduce_umin (vector_type V16B) x))
      (aarch64_uminv V16B x))

(rule (lower (vreduce_umin (vector_type V4H) x))
      (aarch64_uminv V4H x))

(rule (lower (vreduce_umin (vector_type V8H) x))
      (aarch64_uminv V8H x))

(rule (lower (vreduce_umin (vector_type V4S) x))
      (aarch64_uminv V4S x))

;; vreduce_umax: Horizontal unsigned maximum
(rule (lower (vreduce_umax (vector_type V8B) x))
      (aarch64_umaxv V8B x))

(rule (lower (vreduce_umax (vector_type V16B) x))
      (aarch64_umaxv V16B x))

(rule (lower (vreduce_umax (vector_type V4H) x))
      (aarch64_umaxv V4H x))

(rule (lower (vreduce_umax (vector_type V8H) x))
      (aarch64_umaxv V8H x))

(rule (lower (vreduce_umax (vector_type V4S) x))
      (aarch64_umaxv V4S x))

;; SIMD lane manipulation lowering

;; scalar_to_vector: Duplicate scalar to all vector lanes
;; Pattern: scalar_to_vector(s) => DUP
(rule (lower (scalar_to_vector (value_type $I8X16) x))
      (aarch64_dup VecElemSize_8x16 x))

(rule (lower (scalar_to_vector (value_type $I16X8) x))
      (aarch64_dup VecElemSize_16x8 x))

(rule (lower (scalar_to_vector (value_type $I32X4) x))
      (aarch64_dup VecElemSize_32x4 x))

(rule (lower (scalar_to_vector (value_type $I64X2) x))
      (aarch64_dup VecElemSize_64x2 x))

;; extract_vector: Extract lane from vector to scalar
;; Pattern: extract_vector(vec, lane) => UMOV
(rule (lower (extract_vector vec lane))
      (aarch64_extract_lane vec lane))

;; iadd_pairwise: Add adjacent pairs of elements
;; Pattern: iadd_pairwise(vec) => ADDP
(rule (lower (iadd_pairwise (value_type $I8X16) x))
      (aarch64_addp VecElemSize_8x16 x x))

(rule (lower (iadd_pairwise (value_type $I16X8) x))
      (aarch64_addp VecElemSize_16x8 x x))

(rule (lower (iadd_pairwise (value_type $I32X4) x))
      (aarch64_addp VecElemSize_32x4 x x))

(rule (lower (iadd_pairwise (value_type $I64X2) x))
      (aarch64_addp VecElemSize_64x2 x x))

;; Atomic operations lowering

;; atomic_load: Load with acquire semantics
;; Pattern: atomic_load(addr) => LDAR
(rule (lower (atomic_load addr _flags))
      (aarch64_ldar addr))

;; atomic_store: Store with release semantics
;; Pattern: atomic_store(val, addr) => STLR
(rule (lower (atomic_store val addr _flags))
      (aarch64_stlr val addr))

;; atomic_rmw: Atomic read-modify-write operations
;; These use LSE (Large System Extensions) instructions when available

;; atomic_rmw.add: Atomic add
;; Pattern: atomic_rmw.add(addr, val) => LDADD / LDADDAL
(rule (lower (atomic_rmw_add addr val))
      (aarch64_ldadd addr val))

;; atomic_rmw.sub: Atomic subtract (implemented as add with negated value)
;; Pattern: atomic_rmw.sub(addr, val) => LDADD with -val
(rule (lower (atomic_rmw_sub addr val))
      (let ((neg_val Aarch64Inst (aarch64_neg $I64 val)))
            (aarch64_ldadd addr neg_val)))

;; atomic_rmw.and: Atomic AND (implemented as clear with complement)
;; Pattern: atomic_rmw.and(addr, val) => LDCLR with ~val
(rule (lower (atomic_rmw_and addr val))
      (let ((not_val Aarch64Inst (aarch64_mvn_rr $I64 val)))
            (aarch64_ldclr addr not_val)))

;; atomic_rmw.or: Atomic OR
;; Pattern: atomic_rmw.or(addr, val) => LDSET
(rule (lower (atomic_rmw_or addr val))
      (aarch64_ldset addr val))

;; atomic_rmw.xor: Atomic XOR
;; Pattern: atomic_rmw.xor(addr, val) => LDEOR
(rule (lower (atomic_rmw_xor addr val))
      (aarch64_ldeor addr val))

;; atomic_rmw.xchg: Atomic exchange
;; Pattern: atomic_rmw.xchg(addr, val) => SWPAL
(rule (lower (atomic_rmw_xchg addr val))
      (aarch64_swpal addr val))

;; atomic_rmw.smax: Atomic signed maximum
;; Pattern: atomic_rmw.smax(addr, val) => LDSMAX
(rule (lower (atomic_rmw_smax addr val))
      (aarch64_ldsmax addr val))

;; atomic_rmw.smin: Atomic signed minimum
;; Pattern: atomic_rmw.smin(addr, val) => LDSMIN
(rule (lower (atomic_rmw_smin addr val))
      (aarch64_ldsmin addr val))

;; atomic_rmw.umax: Atomic unsigned maximum
;; Pattern: atomic_rmw.umax(addr, val) => LDUMAX
(rule (lower (atomic_rmw_umax addr val))
      (aarch64_ldumax addr val))

;; atomic_rmw.umin: Atomic unsigned minimum
;; Pattern: atomic_rmw.umin(addr, val) => LDUMIN
(rule (lower (atomic_rmw_umin addr val))
      (aarch64_ldumin addr val))

;; atomic_cas: Compare-and-swap with acquire/release semantics
;; Pattern: atomic_cas(addr, expected, new) => CASAL
(rule (lower (atomic_cas addr expected new_val))
      (aarch64_casal addr expected new_val))

;; Memory barrier lowering

;; fence: Memory fence with sequential consistency
;; Pattern: fence(seq_cst) => DSB SY (full system barrier)
(rule (lower (fence AtomicOrdering.seq_cst))
      (aarch64_dsb ShareabilityDomain.SY))

;; fence: Memory fence with acquire semantics
;; Pattern: fence(acquire) => DMB ISHLD (inner shareable load barrier)
(rule (lower (fence AtomicOrdering.acquire))
      (aarch64_dmb ShareabilityDomain.ISHLD))

;; fence: Memory fence with release semantics
;; Pattern: fence(release) => DMB ISH (inner shareable barrier)
(rule (lower (fence AtomicOrdering.release))
      (aarch64_dmb ShareabilityDomain.ISH))

;; fence: Memory fence with acquire-release semantics
;; Pattern: fence(acq_rel) => DMB ISH (inner shareable barrier)
(rule (lower (fence AtomicOrdering.acq_rel))
      (aarch64_dmb ShareabilityDomain.ISH))

;; System register access lowering

;; SystemReg enum type for system registers
(type SystemReg (enum
  nzcv         ;; Condition flags (Negative, Zero, Carry, Overflow)
  fpcr         ;; Floating-point Control Register
  fpsr         ;; Floating-point Status Register
  tpidr_el0    ;; Thread Pointer/ID Register (User Read/Write)
  tpidrro_el0  ;; Thread Pointer/ID Register (User Read-Only)
))

;; Constructor declarations for system register access

;; MRS - Move from System Register (read system register)
(decl aarch64_mrs (SystemReg) Aarch64Inst)

;; MSR - Move to System Register (write system register)
(decl aarch64_msr (SystemReg Value) Aarch64Inst)

;; Helper functions
(decl negate_i64 (i64) i64)

;; Vector widening constructors

;; SSHLL - Signed shift-left-long (widen and optionally shift)
(decl aarch64_sshll (Value VecElemSize u8 bool) Aarch64Inst)

;; USHLL - Unsigned shift-left-long (widen and optionally shift)
(decl aarch64_ushll (Value VecElemSize u8 bool) Aarch64Inst)

;; Vector narrowing constructors (combined operations)

;; SQXTN + SQXTN2 - Signed narrow combined (narrow x to low, y to high)
(decl aarch64_sqxtn_combined (Value Value VecElemSize) Aarch64Inst)

;; SQXTUN + SQXTUN2 - Signed to unsigned narrow combined
(decl aarch64_sqxtun_combined (Value Value VecElemSize) Aarch64Inst)

;; UQXTN + UQXTN2 - Unsigned narrow combined
(decl aarch64_uqxtn_combined (Value Value VecElemSize) Aarch64Inst)

;; Vector narrowing constructors (combined operations)

;; SQXTN + SQXTN2 - Signed narrow combined (narrow x to low, y to high)
(decl aarch64_sqxtn_combined (Value Value VecElemSize) Aarch64Inst)

;; SQXTUN + SQXTUN2 - Signed to unsigned narrow combined
(decl aarch64_sqxtun_combined (Value Value VecElemSize) Aarch64Inst)

;; UQXTN + UQXTN2 - Unsigned narrow combined
(decl aarch64_uqxtn_combined (Value Value VecElemSize) Aarch64Inst)

;;;; Rules for `swiden_low` (signed widen low half) ;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;; I8X16 -> I16X8: widen low 8 lanes
(rule (lower (has_type $I16X8 (swiden_low x)))
      (aarch64_sshll x (VecElemSize.Size16x8) 0 false))

;; I16X8 -> I32X4: widen low 4 lanes
(rule (lower (has_type $I32X4 (swiden_low x)))
      (aarch64_sshll x (VecElemSize.Size32x4) 0 false))

;; I32X4 -> I64X2: widen low 2 lanes
(rule (lower (has_type $I64X2 (swiden_low x)))
      (aarch64_sshll x (VecElemSize.Size64x2) 0 false))

;;;; Rules for `swiden_high` (signed widen high half) ;;;;;;;;;;;;;;;;;;;;;;;;;;

;; I8X16 -> I16X8: widen high 8 lanes
(rule (lower (has_type $I16X8 (swiden_high x)))
      (aarch64_sshll x (VecElemSize.Size16x8) 0 true))

;; I16X8 -> I32X4: widen high 4 lanes
(rule (lower (has_type $I32X4 (swiden_high x)))
      (aarch64_sshll x (VecElemSize.Size32x4) 0 true))

;; I32X4 -> I64X2: widen high 2 lanes
(rule (lower (has_type $I64X2 (swiden_high x)))
      (aarch64_sshll x (VecElemSize.Size64x2) 0 true))

;;;; Rules for `uwiden_low` (unsigned widen low half) ;;;;;;;;;;;;;;;;;;;;;;;;;;

;; I8X16 -> I16X8: widen low 8 lanes
(rule (lower (has_type $I16X8 (uwiden_low x)))
      (aarch64_ushll x (VecElemSize.Size16x8) 0 false))

;; I16X8 -> I32X4: widen low 4 lanes
(rule (lower (has_type $I32X4 (uwiden_low x)))
      (aarch64_ushll x (VecElemSize.Size32x4) 0 false))

;; I32X4 -> I64X2: widen low 2 lanes
(rule (lower (has_type $I64X2 (uwiden_low x)))
      (aarch64_ushll x (VecElemSize.Size64x2) 0 false))

;;;; Rules for `uwiden_high` (unsigned widen high half) ;;;;;;;;;;;;;;;;;;;;;;;;

;; I8X16 -> I16X8: widen high 8 lanes
(rule (lower (has_type $I16X8 (uwiden_high x)))
      (aarch64_ushll x (VecElemSize.Size16x8) 0 true))

;; I16X8 -> I32X4: widen high 4 lanes
(rule (lower (has_type $I32X4 (uwiden_high x)))
      (aarch64_ushll x (VecElemSize.Size32x4) 0 true))

;; I32X4 -> I64X2: widen high 2 lanes
(rule (lower (has_type $I64X2 (uwiden_high x)))
      (aarch64_ushll x (VecElemSize.Size64x2) 0 true))

;;;; Rules for `snarrow` (signed narrow) ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;; I16X8 + I16X8 -> I8X16: narrow two vectors to one
(rule (lower (has_type $I8X16 (snarrow x y)))
      (aarch64_sqxtn_combined x y (VecElemSize.Size8x16)))

;; I32X4 + I32X4 -> I16X8: narrow two vectors to one
(rule (lower (has_type $I16X8 (snarrow x y)))
      (aarch64_sqxtn_combined x y (VecElemSize.Size16x8)))

;; I64X2 + I64X2 -> I32X4: narrow two vectors to one
(rule (lower (has_type $I32X4 (snarrow x y)))
      (aarch64_sqxtn_combined x y (VecElemSize.Size32x4)))

;;;; Rules for `unarrow` (unsigned narrow from signed) ;;;;;;;;;;;;;;;;;;;;;;;;;;

;; I16X8 + I16X8 -> I8X16: narrow signed to unsigned
(rule (lower (has_type $I8X16 (unarrow x y)))
      (aarch64_sqxtun_combined x y (VecElemSize.Size8x16)))

;; I32X4 + I32X4 -> I16X8: narrow signed to unsigned
(rule (lower (has_type $I16X8 (unarrow x y)))
      (aarch64_sqxtun_combined x y (VecElemSize.Size16x8)))

;; I64X2 + I64X2 -> I32X4: narrow signed to unsigned
(rule (lower (has_type $I32X4 (unarrow x y)))
      (aarch64_sqxtun_combined x y (VecElemSize.Size32x4)))

;;;; Rules for `uunarrow` (unsigned narrow from unsigned) ;;;;;;;;;;;;;;;;;;;;;;;

;; I16X8 + I16X8 -> I8X16: narrow unsigned to unsigned
(rule (lower (has_type $I8X16 (uunarrow x y)))
      (aarch64_uqxtn_combined x y (VecElemSize.Size8x16)))

;; I32X4 + I32X4 -> I16X8: narrow unsigned to unsigned
(rule (lower (has_type $I16X8 (uunarrow x y)))
      (aarch64_uqxtn_combined x y (VecElemSize.Size16x8)))

;; I64X2 + I64X2 -> I32X4: narrow unsigned to unsigned
(rule (lower (has_type $I32X4 (uunarrow x y)))
      (aarch64_uqxtn_combined x y (VecElemSize.Size32x4)))

;;;; Float vector conversions ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;; FCVTL - Float convert to higher precision (F32 -> F64)
(decl aarch64_fcvtl (Value bool) Aarch64Inst)

;; FCVTN - Float convert to lower precision (F64 -> F32)
(decl aarch64_fcvtn_combined (Value Value) Aarch64Inst)

;; F32X4 -> F64X2: promote low 2 lanes
(rule (lower (has_type $F64X2 (fvpromote_low x)))
      (aarch64_fcvtl x false))

;; F64X2 + F64X2 -> F32X4: demote two vectors
(rule (lower (has_type $F32X4 (fvdemote x y)))
      (aarch64_fcvtn_combined x y))

;; Trap operations
;; trap: Unconditional trap with trap code
(decl aarch64_trap (TrapCode) Aarch64Inst)
(rule (lower (trap trap_code))
      (aarch64_trap trap_code))

;; trapz: Trap if value is zero
(decl aarch64_trapz (Value TrapCode) Aarch64Inst)
(rule (lower (trapz val trap_code))
      (aarch64_trapz val trap_code))

;; trapnz: Trap if value is non-zero
(decl aarch64_trapnz (Value TrapCode) Aarch64Inst)
(rule (lower (trapnz val trap_code))
      (aarch64_trapnz val trap_code))

;; nop: No operation (emits nothing)
(rule (lower (nop))
      (invalid_reg))

;; bitcast: Type punning between same-size types
;; Float/Vector <-> Float/Vector: no-op (same register file)
(decl aarch64_bitcast_noop (Value) Aarch64Inst)
(rule 7 (lower (has_type (ty_float_or_vec _) (bitcast _ x @ (value_type (ty_float_or_vec _)))))
      (aarch64_bitcast_noop x))

;; GPR <-> GPR: no-op (same size)
(rule 6 (lower (has_type out_ty (bitcast _ x @ (value_type in_ty))))
      (if (ty_int_ref_scalar_64 out_ty))
      (if (ty_int_ref_scalar_64 in_ty))
      (aarch64_bitcast_noop x))

;; GPR -> Float/Vector: FMOV (GPR to FPR)
(decl aarch64_fmov_from_gpr (Value Type) Aarch64Inst)
(rule 5 (lower (has_type (ty_float_or_vec _) (bitcast _ x @ (value_type in_ty))))
      (if (ty_int_ref_scalar_64 in_ty))
      (aarch64_fmov_from_gpr x in_ty))

;; Float/Vector -> GPR: FMOV (FPR to GPR)
(decl aarch64_fmov_to_gpr (Value Type) Aarch64Inst)
(rule 4 (lower (has_type out_ty (bitcast _ x @ (value_type (fits_in_64 (ty_float_or_vec _))))))
      (if (ty_int_ref_scalar_64 out_ty))
      (aarch64_fmov_to_gpr x out_ty))

;; I128 <-> I128: no-op
(rule 3 (lower (has_type $I128 (bitcast _ x @ (value_type $I128))))
      (aarch64_bitcast_noop x))

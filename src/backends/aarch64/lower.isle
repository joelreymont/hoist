;; aarch64 ISLE lowering rules
;; Minimal bootstrap set for ARM64 instruction selection

;; Integer arithmetic lowering

;; iadd: Add two integers (default, lowest priority)
(rule 0 (lower (iadd ty x y))
      (aarch64_add_rr ty x y))

;; iadd with sign-extended byte operand (higher priority than generic)
;; Pattern: iadd(x, sextb(y)) => ADD Xd, Xn, Wm, SXTB
(rule 2 (lower (iadd ty x (aarch64_sxtb ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtb))

;; iadd with sign-extended halfword operand (higher priority than generic)
;; Pattern: iadd(x, sexth(y)) => ADD Xd, Xn, Wm, SXTH
(rule 2 (lower (iadd ty x (aarch64_sxth ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxth))

;; iadd with sign-extended word operand (higher priority than generic)
;; Pattern: iadd(x, sextw(y)) => ADD Xd, Xn, Wm, SXTW
(rule 2 (lower (iadd ty x (aarch64_sxtw y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtw))

;; iadd with zero-extended byte operand (higher priority than generic)
;; Pattern: iadd(x, uextb(y)) => ADD Xd, Xn, Wm, UXTB
(rule 2 (lower (iadd ty x (aarch64_uxtb ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxtb))

;; iadd with zero-extended halfword operand (higher priority than generic)
;; Pattern: iadd(x, uexth(y)) => ADD Xd, Xn, Wm, UXTH
(rule 2 (lower (iadd ty x (aarch64_uxth ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxth))

;; iadd with commuted operands (sign-extended byte, higher priority)
(rule 2 (lower (iadd ty (aarch64_sxtb ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtb))

;; iadd with commuted operands (sign-extended halfword, higher priority)
(rule 2 (lower (iadd ty (aarch64_sxth ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxth))

;; iadd with commuted operands (sign-extended word, higher priority)
(rule 2 (lower (iadd ty (aarch64_sxtw y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtw))

;; iadd with commuted operands (zero-extended byte, higher priority)
(rule 2 (lower (iadd ty (aarch64_uxtb ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxtb))

;; iadd with commuted operands (zero-extended halfword, higher priority)
(rule 2 (lower (iadd ty (aarch64_uxth ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxth))

;; isub: Subtract integers
(rule (lower (isub ty x y))
      (aarch64_sub_rr ty x y))

;; imul: Multiply integers
(rule (lower (imul ty x y))
      (aarch64_mul_rr ty x y))

;; Multiply-add fusion patterns (highest priority - fuse before generic ops)
;; iadd(a, imul(x, y)) => madd(x, y, a)
(rule 3 (lower (iadd ty (imul ty x y) a))
      (aarch64_madd ty x y a))

(rule 3 (lower (iadd ty a (imul ty x y)))
      (aarch64_madd ty x y a))

;; Multiply-subtract fusion patterns (highest priority - fuse before generic ops)
;; isub(a, imul(x, y)) => msub(x, y, a)
(rule 3 (lower (isub ty a (imul ty x y)))
      (aarch64_msub ty x y a))

;; smul_hi: Signed multiply returning upper bits (64×64→128, return high 64)
(rule (lower (smul_hi ty x y))
      (if-let $I64 ty)
      (aarch64_smulh x y))

;; umul_hi: Unsigned multiply returning upper bits (64×64→128, return high 64)
(rule (lower (umul_hi ty x y))
      (if-let $I64 ty)
      (aarch64_umulh x y))

;; sdiv: Signed divide
(rule (lower (sdiv ty x y))
      (aarch64_sdiv ty x y))

;; udiv: Unsigned divide
(rule (lower (udiv ty x y))
      (aarch64_udiv ty x y))

;; srem: Signed remainder (implemented as sdiv + msub)
;; srem(x, y) = x - (x / y) * y
(rule (lower (srem ty x y))
      (let ((quot Aarch64Inst (aarch64_sdiv ty x y)))
            (aarch64_msub ty quot y x)))

;; urem: Unsigned remainder (implemented as udiv + msub)
;; urem(x, y) = x - (x / y) * y
(rule (lower (urem ty x y))
      (let ((quot Aarch64Inst (aarch64_udiv ty x y)))
            (aarch64_msub ty quot y x)))

;; Type conversion operations lowering

;; sextend: Sign-extend to target type
;; I8 -> I16/I32/I64 => SXTB
(rule (lower (sextend dst_ty src_ty x))
      (if-let 8 (ty_bits src_ty))
      (aarch64_sxtb dst_ty x))

;; I16 -> I32/I64 => SXTH
(rule (lower (sextend dst_ty src_ty x))
      (if-let 16 (ty_bits src_ty))
      (aarch64_sxth dst_ty x))

;; I32 -> I64 => SXTW
(rule (lower (sextend dst_ty src_ty x))
      (if-let 32 (ty_bits src_ty))
      (if-let 64 (ty_bits dst_ty))
      (aarch64_sxtw x))

;; uextend: Zero-extend to target type
;; I8 -> I16/I32/I64 => UXTB
(rule (lower (uextend dst_ty src_ty x))
      (if-let 8 (ty_bits src_ty))
      (aarch64_uxtb dst_ty x))

;; I16 -> I32/I64 => UXTH
(rule (lower (uextend dst_ty src_ty x))
      (if-let 16 (ty_bits src_ty))
      (aarch64_uxth dst_ty x))

;; I32 -> I64 => UXTW (note: 32-bit ops auto zero-extend on ARM64)
(rule (lower (uextend dst_ty src_ty x))
      (if-let 32 (ty_bits src_ty))
      (if-let 64 (ty_bits dst_ty))
      (aarch64_uxtw x))

;; ireduce: Truncate integer to narrower type
;; On ARM64, this is implicit when using smaller register size
;; I64 -> I32/I16/I8: move to W register (32-bit) truncates
(rule (lower (ireduce dst_ty src_ty x))
      (aarch64_ireduce dst_ty x))

;; fcvt_from_sint: Convert signed integer to float (SCVTF)
(rule (lower (fcvt_from_sint dst_ty src_ty x))
      (aarch64_scvtf dst_ty src_ty x))

;; fcvt_from_uint: Convert unsigned integer to float (UCVTF)
(rule (lower (fcvt_from_uint dst_ty src_ty x))
      (aarch64_ucvtf dst_ty src_ty x))

;; fpromote: Promote f32 to f64 (FCVT)
(rule (lower (fpromote dst_ty src_ty x))
      (if-let 32 (ty_bits src_ty))
      (if-let 64 (ty_bits dst_ty))
      (aarch64_fpromote x))

;; fdemote: Demote f64 to f32 (FCVT)
(rule (lower (fdemote dst_ty src_ty x))
      (if-let 64 (ty_bits src_ty))
      (if-let 32 (ty_bits dst_ty))
      (aarch64_fdemote x))

;; nearest: Round to nearest (FRINTN)
(rule (lower (nearest ty x))
      (aarch64_nearest ty x))

;; trunc: Round toward zero (FRINTZ)
(rule (lower (trunc ty x))
      (aarch64_trunc ty x))

;; ceil: Round toward +infinity (FRINTP)
(rule (lower (ceil ty x))
      (aarch64_ceil ty x))

;; floor: Round toward -infinity (FRINTM)
(rule (lower (floor ty x))
      (aarch64_floor ty x))

;; iconcat: Concatenate two integers into wider type (I64 + I64 → I128)
;; On ARM64, this creates a register pair from two separate registers
(rule (lower (has_type $I128 (iconcat lo hi)))
      (value_regs_from_values lo hi))

;; Atomic operations lowering

;; atomic_load with acquire ordering: LDAR
(rule (lower (atomic_load ty addr AtomicOrdering.Acquire _flags))
      (aarch64_atomic_load_acquire ty addr))

;; atomic_load with sequential consistency: DMB + LDAR
;; Note: Simplified - full seq_cst may need DMB before as well
(rule (lower (atomic_load ty addr AtomicOrdering.SeqCst _flags))
      (aarch64_atomic_load_acquire ty addr))

;; atomic_store with release ordering: STLR
(rule (lower (atomic_store val addr AtomicOrdering.Release _flags))
      (let ((ty Type (value_type val)))
           (aarch64_atomic_store_release ty addr val)))

;; atomic_store with sequential consistency: STLR + DMB
;; Note: Simplified - full seq_cst may need DMB after as well
(rule (lower (atomic_store val addr AtomicOrdering.SeqCst _flags))
      (let ((ty Type (value_type val)))
           (aarch64_atomic_store_release ty addr val)))

;; fence: Memory barrier
(rule (lower (fence ordering))
      (aarch64_fence ordering))

;; Shift operations lowering

;; ishl: Integer shift left (register form)
(rule (lower (ishl ty x y))
      (aarch64_lsl_rr ty x y))

;; ishl: Integer shift left (immediate form)
(rule (lower (ishl ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_lsl_imm ty x amt))

;; ushr: Unsigned shift right (register form)
(rule (lower (ushr ty x y))
      (aarch64_lsr_rr ty x y))

;; ushr: Unsigned shift right (immediate form)
(rule (lower (ushr ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_lsr_imm ty x amt))

;; sshr: Signed shift right (register form)
(rule (lower (sshr ty x y))
      (aarch64_asr_rr ty x y))

;; sshr: Signed shift right (immediate form)
(rule (lower (sshr ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_asr_imm ty x amt))

;; rotr: Rotate right (register form)
(rule (lower (rotr ty x y))
      (aarch64_ror_rr ty x y))

;; rotr: Rotate right (immediate form)
(rule (lower (rotr ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_ror_imm ty x amt))

;; rotl: Rotate left - NOT IMPLEMENTED YET
;; Note: Needs register negation for variable amount
;; rotl(x, k) = rotr(x, width - k)

;; Comparison operations lowering

;; icmp: Integer comparison with Equal condition
(rule (lower (icmp IntCC.Equal ty x y))
      (aarch64_cmp_rr ty x y IntCC.Equal))

;; icmp: Integer comparison with NotEqual condition
(rule (lower (icmp IntCC.NotEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.NotEqual))

;; icmp: Integer comparison with SignedLessThan condition
(rule (lower (icmp IntCC.SignedLessThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedLessThan))

;; icmp: Integer comparison with SignedGreaterThanOrEqual condition
(rule (lower (icmp IntCC.SignedGreaterThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedGreaterThanOrEqual))

;; icmp: Integer comparison with SignedGreaterThan condition
(rule (lower (icmp IntCC.SignedGreaterThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedGreaterThan))

;; icmp: Integer comparison with SignedLessThanOrEqual condition
(rule (lower (icmp IntCC.SignedLessThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedLessThanOrEqual))

;; icmp: Integer comparison with UnsignedLessThan condition
(rule (lower (icmp IntCC.UnsignedLessThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedLessThan))

;; icmp: Integer comparison with UnsignedGreaterThanOrEqual condition
(rule (lower (icmp IntCC.UnsignedGreaterThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedGreaterThanOrEqual))

;; icmp: Integer comparison with UnsignedGreaterThan condition
(rule (lower (icmp IntCC.UnsignedGreaterThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedGreaterThan))

;; icmp: Integer comparison with UnsignedLessThanOrEqual condition
(rule (lower (icmp IntCC.UnsignedLessThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedLessThanOrEqual))

;; icmp with immediate form (12-bit unsigned immediate)
(rule (lower (icmp cc ty x (iconst k)))
      (if-let $true (uimm12 k))
      (aarch64_cmp_imm ty x k cc))

;; Bitwise operations lowering

;; band: Bitwise AND (register form)
(rule (lower (band ty x y))
      (aarch64_and_rr ty x y))

;; bor: Bitwise OR (register form)
(rule (lower (bor ty x y))
      (aarch64_orr_rr ty x y))

;; bxor: Bitwise XOR (register form)
(rule (lower (bxor ty x y))
      (aarch64_eor_rr ty x y))

;; bnot: Bitwise NOT (implemented as MVN)
(rule (lower (bnot ty x))
      (aarch64_mvn_rr ty x))

;; ineg: Integer negation (implemented as NEG)
(rule (lower (ineg ty x))
      (aarch64_neg ty x))

;; Conditional select lowering

;; select: Conditional select based on comparison result
;; select(cond, true_val, false_val) -> true_val if cond != 0, else false_val
(rule (lower (select ty (icmp cc cmp_ty x y) true_val false_val))
      (let ((cmp_inst Aarch64Inst (aarch64_cmp_rr cmp_ty x y cc)))
            (aarch64_csel ty true_val false_val cmp_inst cc)))

;; Immediate forms

;; iadd with 12-bit immediate (priority 1 - higher than generic, lower than extensions/fusions)
(rule 1 (lower (iadd ty x (iconst k)))
      (if-let $true (uimm12 k))
      (aarch64_add_imm ty x k))

;; isub with negated 12-bit immediate optimization
;; Lower (isub x iconst) as (iadd x -iconst) when -imm fits in 12-bit
;; This provides more flexibility in immediate selection
(rule (lower (isub ty x (iconst k)))
      (if-let $true (in_neg_uimm12_range k))
      (aarch64_add_imm ty x (negate_i64 k)))

;; isub with 12-bit immediate
(rule (lower (isub ty x (iconst k)))
      (if-let $true (uimm12 k))
      (aarch64_sub_imm ty x k))

;; Load/store lowering

;; load: Load from memory with register+immediate offset
;; Pattern: load(iadd(base, iconst(offset))) => LDR Xt, [Xn, #offset]
(rule (lower (load ty (iadd ptr_ty base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_ldr_imm_offset ty offset))
      (aarch64_ldr_imm ty base scaled_offset))

;; load: Load from memory with commuted register+immediate offset
(rule (lower (load ty (iadd ptr_ty (iconst offset) base) _flags _mem_offset))
      (if-let scaled_offset (valid_ldr_imm_offset ty offset))
      (aarch64_ldr_imm ty base scaled_offset))

;; load: Load from memory with register+register offset
;; Pattern: load(iadd(base, index)) => LDR Xt, [Xn, Xm]
(rule (lower (load ty (iadd ptr_ty base index) _flags _mem_offset))
      (aarch64_ldr_reg ty base index))

;; load: Load from memory with register+extended register offset
;; Pattern: load(iadd(base, sxtw(index))) => LDR Xt, [Xn, Wm, SXTW]
(rule (lower (load ty (iadd ptr_ty base (aarch64_sxtw index)) _flags _mem_offset))
      (aarch64_ldr_ext ty base index ExtendOp.sxtw))

;; load: Load from memory with commuted register+extended register offset
(rule (lower (load ty (iadd ptr_ty (aarch64_sxtw index) base) _flags _mem_offset))
      (aarch64_ldr_ext ty base index ExtendOp.sxtw))

;; load: Load from memory with register+shifted register offset
;; Pattern: load(iadd(base, ishl(index, iconst(shift)))) => LDR Xt, [Xn, Xm, LSL #shift]
(rule (lower (load ty (iadd ptr_ty base (ishl idx_ty index (iconst shift))) _flags _mem_offset))
      (if-let shift_amt (valid_ldr_shift ty shift))
      (aarch64_ldr_shifted ty base index shift_amt))

;; load: Load from memory with commuted register+shifted register offset
(rule (lower (load ty (iadd ptr_ty (ishl idx_ty index (iconst shift)) base) _flags _mem_offset))
      (if-let shift_amt (valid_ldr_shift ty shift))
      (aarch64_ldr_shifted ty base index shift_amt))

;; load: Load from memory (base register only)
(rule (lower (load ty addr _flags _offset))
      (aarch64_ldr ty addr))

;; store: Store to memory with register+immediate offset
;; Pattern: store(val, iadd(base, iconst(offset))) => STR Xt, [Xn, #offset]
(rule (lower (store val (iadd ptr_ty base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_str_imm_offset val offset))
      (aarch64_str_imm val base scaled_offset))

;; store: Store to memory with commuted register+immediate offset
(rule (lower (store val (iadd ptr_ty (iconst offset) base) _flags _mem_offset))
      (if-let scaled_offset (valid_str_imm_offset val offset))
      (aarch64_str_imm val base scaled_offset))

;; store: Store to memory with register+register offset
;; Pattern: store(val, iadd(base, index)) => STR Xt, [Xn, Xm]
(rule (lower (store val (iadd ptr_ty base index) _flags _mem_offset))
      (aarch64_str_reg val base index))

;; store: Store to memory with register+extended register offset
;; Pattern: store(val, iadd(base, sxtw(index))) => STR Xt, [Xn, Wm, SXTW]
(rule (lower (store val (iadd ptr_ty base (aarch64_sxtw index)) _flags _mem_offset))
      (aarch64_str_ext val base index ExtendOp.sxtw))

;; store: Store to memory with commuted register+extended register offset
(rule (lower (store val (iadd ptr_ty (aarch64_sxtw index) base) _flags _mem_offset))
      (aarch64_str_ext val base index ExtendOp.sxtw))

;; store: Store to memory with register+shifted register offset
;; Pattern: store(val, iadd(base, ishl(index, iconst(shift)))) => STR Xt, [Xn, Xm, LSL #shift]
(rule (lower (store val (iadd ptr_ty base (ishl idx_ty index (iconst shift))) _flags _mem_offset))
      (if-let shift_amt (valid_str_shift val shift))
      (aarch64_str_shifted val base index shift_amt))

;; store: Store to memory with commuted register+shifted register offset
(rule (lower (store val (iadd ptr_ty (ishl idx_ty index (iconst shift)) base) _flags _mem_offset))
      (if-let shift_amt (valid_str_shift val shift))
      (aarch64_str_shifted val base index shift_amt))

;; store: Store to memory (base register only)
(rule (lower (store val addr _flags _offset))
      (aarch64_str val addr))

;; Pre-index addressing modes

;; load with pre-index: base+=offset, then load
;; Pattern: load(pre_inc(base, offset)) => LDR Xt, [Xn, #offset]!
(rule (lower (load ty (pre_inc base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_ldr_imm_offset ty offset))
      (aarch64_ldr_pre ty base scaled_offset))

;; store with pre-index: base+=offset, then store
;; Pattern: store(val, pre_inc(base, offset)) => STR Xt, [Xn, #offset]!
(rule (lower (store val (pre_inc base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_str_imm_offset val offset))
      (aarch64_str_pre val base scaled_offset))

;; Post-index addressing modes

;; load with post-index: load, then base+=offset
;; Pattern: load(post_inc(base, offset)) => LDR Xt, [Xn], #offset
(rule (lower (load ty (post_inc base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_ldr_imm_offset ty offset))
      (aarch64_ldr_post ty base scaled_offset))

;; store with post-index: store, then base+=offset
;; Pattern: store(val, post_inc(base, offset)) => STR Xt, [Xn], #offset
(rule (lower (store val (post_inc base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_str_imm_offset val offset))
      (aarch64_str_post val base scaled_offset))

;; Load pair optimization (LDP)
;; Pattern: two adjacent loads with 8-byte offset
;; load(base+0) and load(base+8) => LDP X0, X1, [base]
(rule (lower (load_pair ty base offset1 offset2))
      (if-let $true (is_ldp_valid_offset offset1 offset2))
      (aarch64_ldp ty base offset1))

;; Store pair optimization (STP)
;; Pattern: two adjacent stores with 8-byte offset
;; store(val1, base+0) and store(val2, base+8) => STP X0, X1, [base]
(rule (lower (store_pair val1 val2 base offset1 offset2))
      (if-let $true (is_stp_valid_offset offset1 offset2))
      (aarch64_stp val1 val2 base offset1))

;; Control flow lowering

;; jump: Unconditional branch
(rule (lower (jump target))
      (aarch64_b target))

;; brif: Conditional branch
(rule (lower (brif cond target))
      (aarch64_b_cond cond target))

;; cbz: Compare-and-branch-zero optimization
;; Pattern: if (x == 0) goto target
(rule (lower (brif (icmp eq x (iconst 0)) target))
      (aarch64_cbz x target))

;; cbnz: Compare-and-branch-nonzero optimization
;; Pattern: if (x != 0) goto target
(rule (lower (brif (icmp ne x (iconst 0)) target))
      (aarch64_cbnz x target))

;; tbz: Test-bit-and-branch-zero optimization
;; Pattern: if ((x & (1 << bit)) == 0) goto target
(rule (lower (brif (icmp eq (band x (ishl (iconst 1) bit)) (iconst 0)) target))
      (aarch64_tbz x bit target))

;; tbnz: Test-bit-and-branch-nonzero optimization
;; Pattern: if ((x & (1 << bit)) != 0) goto target
(rule (lower (brif (icmp ne (band x (ishl (iconst 1) bit)) (iconst 0)) target))
      (aarch64_tbnz x bit target))

;; return: Return from function
(rule (lower (return))
      (aarch64_ret))

;; Floating-point conversion lowering

;; fcvt_to_uint: Convert float to unsigned integer (FCVTZU)
(rule (lower (fcvt_to_uint (value_type $F32) x (value_type $I32)))
      (aarch64_fcvtzu_32 x))

(rule (lower (fcvt_to_uint (value_type $F32) x (value_type $I64)))
      (aarch64_fcvtzu_64 x))

(rule (lower (fcvt_to_uint (value_type $F64) x (value_type $I32)))
      (aarch64_fcvtzu_32 x))

(rule (lower (fcvt_to_uint (value_type $F64) x (value_type $I64)))
      (aarch64_fcvtzu_64 x))

;; fcvt_to_sint: Convert float to signed integer (FCVTZS)
(rule (lower (fcvt_to_sint (value_type $F32) x (value_type $I32)))
      (aarch64_fcvtzs_32 x))

(rule (lower (fcvt_to_sint (value_type $F32) x (value_type $I64)))
      (aarch64_fcvtzs_64 x))

(rule (lower (fcvt_to_sint (value_type $F64) x (value_type $I32)))
      (aarch64_fcvtzs_32 x))

(rule (lower (fcvt_to_sint (value_type $F64) x (value_type $I64)))
      (aarch64_fcvtzs_64 x))

;; fcvt_from_uint: Convert unsigned integer to float (UCVTF)
(rule (lower (fcvt_from_uint (value_type $I32) x (value_type $F32)))
      (aarch64_ucvtf_32_to_f32 x))

(rule (lower (fcvt_from_uint (value_type $I32) x (value_type $F64)))
      (aarch64_ucvtf_32_to_f64 x))

(rule (lower (fcvt_from_uint (value_type $I64) x (value_type $F32)))
      (aarch64_ucvtf_64_to_f32 x))

(rule (lower (fcvt_from_uint (value_type $I64) x (value_type $F64)))
      (aarch64_ucvtf_64_to_f64 x))

;; fcvt_from_sint: Convert signed integer to float (SCVTF)
(rule (lower (fcvt_from_sint (value_type $I32) x (value_type $F32)))
      (aarch64_scvtf_32_to_f32 x))

(rule (lower (fcvt_from_sint (value_type $I32) x (value_type $F64)))
      (aarch64_scvtf_32_to_f64 x))

(rule (lower (fcvt_from_sint (value_type $I64) x (value_type $F32)))
      (aarch64_scvtf_64_to_f32 x))

(rule (lower (fcvt_from_sint (value_type $I64) x (value_type $F64)))
      (aarch64_scvtf_64_to_f64 x))

;; fdemote: Convert f64 to f32 (FCVT)
(rule (lower (fdemote x))
      (aarch64_fcvt_f64_to_f32 x))

;; fpromote: Convert f32 to f64 (FCVT)
(rule (lower (fpromote x))
      (aarch64_fcvt_f32_to_f64 x))

;; Floating-point arithmetic

;; fadd: FP addition (FADD)
(rule (lower (fadd ty x y))
      (aarch64_fadd ty x y))

;; fsub: FP subtraction (FSUB)
(rule (lower (fsub ty x y))
      (aarch64_fsub ty x y))

;; fmul: FP multiplication (FMUL)
(rule (lower (fmul ty x y))
      (aarch64_fmul ty x y))

;; fdiv: FP division (FDIV)
(rule (lower (fdiv ty x y))
      (aarch64_fdiv ty x y))

;; fsqrt: FP square root (FSQRT)
(rule (lower (fsqrt ty x))
      (aarch64_fsqrt ty x))

;; fma: FP fused multiply-add (FMADD)
;; fma(x, y, z) = x * y + z
(rule (lower (fma ty x y z))
      (aarch64_fmadd ty x y z))

;; fcmp: FP comparison (FCMP)
(rule (lower (fcmp cond x y))
      (aarch64_fcmp x y))

;; fabs: FP absolute value (FABS)
(rule (lower (fabs ty x))
      (aarch64_fabs ty x))

;; fneg: FP negation (FNEG)
(rule (lower (fneg ty x))
      (aarch64_fneg ty x))

;; Constant materialization

;; iconst: Integer constant (16-bit immediate via MOVZ)
(rule (lower (iconst ty k))
      (if-let $true (uimm16 k))
      (aarch64_movz ty k))

;; iconst: Larger constants (needs multiple instructions)
(rule (lower (iconst ty k))
      (aarch64_iconst ty k))

;; Helper extractors

;; Check if value fits in unsigned 12-bit
(extractor (uimm12 val)
  (if (<= 0 val 4095)
      val))

;; Check if value fits in unsigned 16-bit
(extractor (uimm16 val)
  (if (<= 0 val 65535)
      val))

;; Check if value is a valid shift amount (0-63, suitable for both I32 and I64)
(extractor (valid_shift_imm val)
  (if (<= 0 val 63)
      val))

;; Check if value is in range where negation fits in unsigned 12-bit
;; Returns $true if -4095 <= val <= -1 (i.e., -val fits in 0-4095)
(extractor (in_neg_uimm12_range val)
  (if (<= -4095 val -1)
      $true))

;; Addressing mode extractors

;; Check if offset is valid for load immediate addressing (simplified)
;; For now, accept offsets 0-32760 (max for I64 8-byte aligned access)
(extractor (valid_ldr_imm_offset ty offset)
  (if (<= 0 offset 32760)
      offset))

;; Check if offset is valid for store immediate addressing (simplified)
(extractor (valid_str_imm_offset val offset)
  (if (<= 0 offset 32760)
      offset))

;; Check if shift is valid for load (must be 0-3)
(extractor (valid_ldr_shift ty shift)
  (if (<= 0 shift 3)
      shift))

;; Check if shift is valid for store (must be 0-3)
(extractor (valid_str_shift val shift)
  (if (<= 0 shift 3)
      shift))

;; Type declarations

;; Integer comparison condition codes
(type IntCC (enum
  Equal
  NotEqual
  SignedLessThan
  SignedLessThanOrEqual
  SignedGreaterThan
  SignedGreaterThanOrEqual
  UnsignedLessThan
  UnsignedLessThanOrEqual
  UnsignedGreaterThan
  UnsignedGreaterThanOrEqual
))

;; Extend operations for extended register operands
(type ExtendOp (enum
  uxtb  ;; Zero-extend byte (8-bit)
  uxth  ;; Zero-extend halfword (16-bit)
  uxtw  ;; Zero-extend word (32-bit to 64-bit)
  uxtx  ;; Zero-extend doubleword (no-op for 64-bit)
  sxtb  ;; Sign-extend byte (8-bit)
  sxth  ;; Sign-extend halfword (16-bit)
  sxtw  ;; Sign-extend word (32-bit to 64-bit)
  sxtx  ;; Sign-extend doubleword (no-op for 64-bit)
))

;; Multi-register value type for wide operations (I128)
;; Maps to Zig's ValueRegs(VReg) type
(type ValueRegs extern)

;; Constructor terms for aarch64 instructions

(type Aarch64Inst (enum))

;; Arithmetic
(decl aarch64_add_rr (Type Value Value) Aarch64Inst)
(decl aarch64_add_imm (Type Value i64) Aarch64Inst)
(decl aarch64_add_extended (Type Value Value ExtendOp) Aarch64Inst)
(decl aarch64_sub_rr (Type Value Value) Aarch64Inst)
(decl aarch64_sub_imm (Type Value i64) Aarch64Inst)
(decl aarch64_mul_rr (Type Value Value) Aarch64Inst)
(decl aarch64_madd (Type Value Value Value) Aarch64Inst)
(decl aarch64_msub (Type Value Value Value) Aarch64Inst)
(decl aarch64_smulh (Value Value) Aarch64Inst)
(decl aarch64_umulh (Value Value) Aarch64Inst)
(decl aarch64_sdiv (Type Value Value) Aarch64Inst)
(decl aarch64_udiv (Type Value Value) Aarch64Inst)

;; Shift operations
(decl aarch64_lsl_rr (Type Value Value) Aarch64Inst)
(decl aarch64_lsl_imm (Type Value i64) Aarch64Inst)
(decl aarch64_lsr_rr (Type Value Value) Aarch64Inst)
(decl aarch64_lsr_imm (Type Value i64) Aarch64Inst)
(decl aarch64_asr_rr (Type Value Value) Aarch64Inst)
(decl aarch64_asr_imm (Type Value i64) Aarch64Inst)
(decl aarch64_ror_rr (Type Value Value) Aarch64Inst)
(decl aarch64_ror_imm (Type Value i64) Aarch64Inst)

;; Bitwise operations
(decl aarch64_and_rr (Type Value Value) Aarch64Inst)
(decl aarch64_orr_rr (Type Value Value) Aarch64Inst)
(decl aarch64_eor_rr (Type Value Value) Aarch64Inst)
(decl aarch64_mvn_rr (Type Value) Aarch64Inst)

;; Negation operations
(decl aarch64_neg (Type Value) Aarch64Inst)
(decl aarch64_ngc (Type Value) Aarch64Inst)

;; Sign/zero extend operations
(decl aarch64_sxtb (Type Value) Aarch64Inst)
(decl aarch64_sxth (Type Value) Aarch64Inst)
(decl aarch64_sxtw (Value) Aarch64Inst)
(decl aarch64_uxtb (Type Value) Aarch64Inst)
(decl aarch64_uxth (Type Value) Aarch64Inst)

;; Comparison operations
(decl aarch64_cmp_rr (Type Value Value IntCC) Aarch64Inst)
(decl aarch64_cmp_imm (Type Value i64 IntCC) Aarch64Inst)

;; Conditional select operations
(decl aarch64_csel (Type Value Value Value IntCC) Aarch64Inst)
(decl aarch64_csinc (Type Value Value Value IntCC) Aarch64Inst)
(decl aarch64_csinv (Type Value Value Value IntCC) Aarch64Inst)
(decl aarch64_csneg (Type Value Value Value IntCC) Aarch64Inst)

;; Memory
(decl aarch64_ldr (Type Value) Aarch64Inst)
(decl aarch64_ldr_imm (Type Value i64) Aarch64Inst)
(decl aarch64_ldr_reg (Type Value Value) Aarch64Inst)
(decl aarch64_ldr_ext (Type Value Value ExtendOp) Aarch64Inst)
(decl aarch64_ldr_shifted (Type Value Value i64) Aarch64Inst)
(decl aarch64_ldr_pre (Type Value i64) Aarch64Inst)   ;; Pre-index: base+=offset, then load
(decl aarch64_ldr_post (Type Value i64) Aarch64Inst)  ;; Post-index: load, then base+=offset
(decl aarch64_str (Value Value) Aarch64Inst)
(decl aarch64_str_imm (Value Value i64) Aarch64Inst)
(decl aarch64_str_reg (Value Value Value) Aarch64Inst)
(decl aarch64_str_ext (Value Value Value ExtendOp) Aarch64Inst)
(decl aarch64_str_shifted (Value Value Value i64) Aarch64Inst)
(decl aarch64_str_pre (Value Value i64) Aarch64Inst)  ;; Pre-index: base+=offset, then store
(decl aarch64_str_post (Value Value i64) Aarch64Inst) ;; Post-index: store, then base+=offset

;; Control flow
(decl aarch64_b (Block) Aarch64Inst)
(decl aarch64_b_cond (Value Block) Aarch64Inst)
(decl aarch64_ret () Aarch64Inst)

;; Constants
(decl aarch64_movz (Type i64) Aarch64Inst)
(decl aarch64_iconst (Type i64) Aarch64Inst)

;; Floating-point conversions
(decl aarch64_fcvtzu_32 (Value) Aarch64Inst)  ;; FCVTZU: float to unsigned i32
(decl aarch64_fcvtzu_64 (Value) Aarch64Inst)  ;; FCVTZU: float to unsigned i64
(decl aarch64_fcvtzs_32 (Value) Aarch64Inst)  ;; FCVTZS: float to signed i32
(decl aarch64_fcvtzs_64 (Value) Aarch64Inst)  ;; FCVTZS: float to signed i64
(decl aarch64_ucvtf_32_to_f32 (Value) Aarch64Inst)  ;; UCVTF: u32 to f32
(decl aarch64_ucvtf_32_to_f64 (Value) Aarch64Inst)  ;; UCVTF: u32 to f64
(decl aarch64_ucvtf_64_to_f32 (Value) Aarch64Inst)  ;; UCVTF: u64 to f32
(decl aarch64_ucvtf_64_to_f64 (Value) Aarch64Inst)  ;; UCVTF: u64 to f64
(decl aarch64_scvtf_32_to_f32 (Value) Aarch64Inst)  ;; SCVTF: i32 to f32
(decl aarch64_scvtf_32_to_f64 (Value) Aarch64Inst)  ;; SCVTF: i32 to f64
(decl aarch64_scvtf_64_to_f32 (Value) Aarch64Inst)  ;; SCVTF: i64 to f32
(decl aarch64_scvtf_64_to_f64 (Value) Aarch64Inst)  ;; SCVTF: i64 to f64
(decl aarch64_fcvt_f64_to_f32 (Value) Aarch64Inst)  ;; FCVT: f64 to f32
(decl aarch64_fcvt_f32_to_f64 (Value) Aarch64Inst)  ;; FCVT: f32 to f64

;; General floating-point conversion (type-generic)
(decl aarch64_scvtf (Type Type Value) Aarch64Inst)  ;; SCVTF: signed int to float
(decl aarch64_ucvtf (Type Type Value) Aarch64Inst)  ;; UCVTF: unsigned int to float
(decl aarch64_fpromote (Value) Aarch64Inst)         ;; FCVT: f32 to f64
(decl aarch64_fdemote (Value) Aarch64Inst)          ;; FCVT: f64 to f32

;; Floating-point rounding operations
(decl aarch64_nearest (Type Value) Aarch64Inst)     ;; FRINTN: round to nearest
(decl aarch64_trunc (Type Value) Aarch64Inst)       ;; FRINTZ: round toward zero
(decl aarch64_ceil (Type Value) Aarch64Inst)        ;; FRINTP: round toward +infinity
(decl aarch64_floor (Type Value) Aarch64Inst)       ;; FRINTM: round toward -infinity

;; Multi-register value operations
(decl value_regs_from_values (Value Value) ValueRegs)  ;; Construct ValueRegs from two I64 values
(extern constructor value_regs_from_values value_regs_from_values)

;; Atomic operations
(decl aarch64_atomic_load_acquire (Type Value) Aarch64Inst)   ;; LDAR: atomic load with acquire
(decl aarch64_atomic_store_release (Type Value Value) Aarch64Inst)  ;; STLR: atomic store with release
(decl aarch64_fence (AtomicOrdering) Aarch64Inst)  ;; DMB: memory fence

;; Floating-point arithmetic
(decl aarch64_fadd (Type Value Value) Aarch64Inst)
(decl aarch64_fsub (Type Value Value) Aarch64Inst)
(decl aarch64_fmul (Type Value Value) Aarch64Inst)
(decl aarch64_fdiv (Type Value Value) Aarch64Inst)
(decl aarch64_fsqrt (Type Value) Aarch64Inst)
(decl aarch64_fmadd (Type Value Value Value) Aarch64Inst)
(decl aarch64_fcmp (Value Value) Aarch64Inst)
(decl aarch64_fabs (Type Value) Aarch64Inst)
(decl aarch64_fneg (Type Value) Aarch64Inst)

;; SIMD/Vector operations
(decl aarch64_vec_add (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_sub (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_mul (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_smin (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_smax (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_umin (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_umax (VectorSize Value Value) Aarch64Inst)

;; SIMD/Vector reductions (horizontal operations)
(decl aarch64_addv (VectorSize Value) Aarch64Inst)     ;; Horizontal add across vector
(decl aarch64_sminv (VectorSize Value) Aarch64Inst)    ;; Horizontal signed min
(decl aarch64_smaxv (VectorSize Value) Aarch64Inst)    ;; Horizontal signed max
(decl aarch64_uminv (VectorSize Value) Aarch64Inst)    ;; Horizontal unsigned min
(decl aarch64_umaxv (VectorSize Value) Aarch64Inst)    ;; Horizontal unsigned max

;; Atomic operations (LSE - Large System Extensions)
(decl aarch64_ldar (Value) Aarch64Inst)                ;; Load-acquire register
(decl aarch64_stlr (Value Value) Aarch64Inst)          ;; Store-release register
(decl aarch64_ldadd (Value Value) Aarch64Inst)         ;; Atomic add with acquire/release
(decl aarch64_ldclr (Value Value) Aarch64Inst)         ;; Atomic clear bits with acquire/release
(decl aarch64_ldeor (Value Value) Aarch64Inst)         ;; Atomic XOR with acquire/release
(decl aarch64_ldset (Value Value) Aarch64Inst)         ;; Atomic OR (set bits) with acquire/release
(decl aarch64_ldsmax (Value Value) Aarch64Inst)        ;; Atomic signed max with acquire/release
(decl aarch64_ldsmin (Value Value) Aarch64Inst)        ;; Atomic signed min with acquire/release
(decl aarch64_ldumax (Value Value) Aarch64Inst)        ;; Atomic unsigned max with acquire/release
(decl aarch64_ldumin (Value Value) Aarch64Inst)        ;; Atomic unsigned min with acquire/release
(decl aarch64_swpal (Value Value) Aarch64Inst)         ;; Atomic swap with acquire/release
(decl aarch64_casal (Value Value Value) Aarch64Inst)   ;; Compare-and-swap with acquire/release

;; LSE Atomic helper implementations
;; These generate atomic instructions with sequentially-consistent ordering

;; aarch64_ldadd: Atomic add - LDADDAL (acquire-release for seq_cst)
(rule (aarch64_ldadd addr val)
      ;; TODO: Select ordering variant based on context
      ;; For now, use sequentially-consistent (acquire-release)
      (Aarch64Inst.ldaddal val addr))

;; aarch64_ldclr: Atomic clear bits - LDCLRAL
(rule (aarch64_ldclr addr val)
      (Aarch64Inst.ldclral val addr))

;; aarch64_ldeor: Atomic XOR - LDEORAL
(rule (aarch64_ldeor addr val)
      (Aarch64Inst.ldeoral val addr))

;; aarch64_ldset: Atomic OR - LDSETAL
(rule (aarch64_ldset addr val)
      (Aarch64Inst.ldsetal val addr))

;; aarch64_ldsmax: Atomic signed maximum - LDSMAXAL
(rule (aarch64_ldsmax addr val)
      (Aarch64Inst.ldsmaxal val addr))

;; aarch64_ldsmin: Atomic signed minimum - LDSMINAL
(rule (aarch64_ldsmin addr val)
      (Aarch64Inst.ldsminal val addr))

;; aarch64_ldumax: Atomic unsigned maximum - LDUMAXAL
(rule (aarch64_ldumax addr val)
      (Aarch64Inst.ldumaxal val addr))

;; aarch64_ldumin: Atomic unsigned minimum - LDUMINAL
(rule (aarch64_ldumin addr val)
      (Aarch64Inst.lduminal val addr))

;; aarch64_swpal: Atomic swap - SWPAL
(rule (aarch64_swpal addr val)
      (Aarch64Inst.swpal val addr))

;; aarch64_casal: Compare-and-swap - CASAL
(rule (aarch64_casal addr expected new_val)
      (Aarch64Inst.casal expected new_val addr))

;; Memory barrier operations
(decl aarch64_dmb (ShareabilityDomain) Aarch64Inst)    ;; Data Memory Barrier
(decl aarch64_dsb (ShareabilityDomain) Aarch64Inst)    ;; Data Synchronization Barrier
(decl aarch64_isb () Aarch64Inst)                      ;; Instruction Synchronization Barrier

;; Shareability domain for memory barrier instructions
(type ShareabilityDomain (enum
  SY      ;; Full system - all agents
  ISH     ;; Inner shareable - processors in same inner shareable domain
  ISHLD   ;; Inner shareable load - acquire barrier for inner shareable
  ISHST   ;; Inner shareable store - release barrier for inner shareable
  NSH     ;; Non-shareable - only this processor
  OSH     ;; Outer shareable - processors in same outer shareable domain
))

;; Vector type enum
(type VectorSize (enum
  V8B   ;; 8 bytes (8x i8)
  V16B  ;; 16 bytes (16x i8)
  V4H   ;; 4 halfwords (4x i16)
  V8H   ;; 8 halfwords (8x i16)
  V2S   ;; 2 singles (2x i32/f32)
  V4S   ;; 4 singles (4x i32/f32)
  V2D   ;; 2 doubles (2x i64/f64)
))

;; SIMD/Vector lowering rules

;; viadd: Vector integer addition
(rule (lower (viadd (vector_type V8B) x y))
      (aarch64_vec_add V8B x y))

(rule (lower (viadd (vector_type V16B) x y))
      (aarch64_vec_add V16B x y))

(rule (lower (viadd (vector_type V4H) x y))
      (aarch64_vec_add V4H x y))

(rule (lower (viadd (vector_type V8H) x y))
      (aarch64_vec_add V8H x y))

(rule (lower (viadd (vector_type V2S) x y))
      (aarch64_vec_add V2S x y))

(rule (lower (viadd (vector_type V4S) x y))
      (aarch64_vec_add V4S x y))

(rule (lower (viadd (vector_type V2D) x y))
      (aarch64_vec_add V2D x y))

;; visub: Vector integer subtraction
(rule (lower (visub (vector_type V8B) x y))
      (aarch64_vec_sub V8B x y))

(rule (lower (visub (vector_type V16B) x y))
      (aarch64_vec_sub V16B x y))

(rule (lower (visub (vector_type V4H) x y))
      (aarch64_vec_sub V4H x y))

(rule (lower (visub (vector_type V8H) x y))
      (aarch64_vec_sub V8H x y))

(rule (lower (visub (vector_type V2S) x y))
      (aarch64_vec_sub V2S x y))

(rule (lower (visub (vector_type V4S) x y))
      (aarch64_vec_sub V4S x y))

(rule (lower (visub (vector_type V2D) x y))
      (aarch64_vec_sub V2D x y))

;; vimul: Vector integer multiplication
(rule (lower (vimul (vector_type V8B) x y))
      (aarch64_vec_mul V8B x y))

(rule (lower (vimul (vector_type V16B) x y))
      (aarch64_vec_mul V16B x y))

(rule (lower (vimul (vector_type V4H) x y))
      (aarch64_vec_mul V4H x y))

(rule (lower (vimul (vector_type V8H) x y))
      (aarch64_vec_mul V8H x y))

(rule (lower (vimul (vector_type V2S) x y))
      (aarch64_vec_mul V2S x y))

(rule (lower (vimul (vector_type V4S) x y))
      (aarch64_vec_mul V4S x y))

;; vimin: Vector signed minimum
(rule (lower (vimin (vector_type V8B) x y))
      (aarch64_vec_smin V8B x y))

(rule (lower (vimin (vector_type V16B) x y))
      (aarch64_vec_smin V16B x y))

(rule (lower (vimin (vector_type V4H) x y))
      (aarch64_vec_smin V4H x y))

(rule (lower (vimin (vector_type V8H) x y))
      (aarch64_vec_smin V8H x y))

(rule (lower (vimin (vector_type V2S) x y))
      (aarch64_vec_smin V2S x y))

(rule (lower (vimin (vector_type V4S) x y))
      (aarch64_vec_smin V4S x y))

;; vimax: Vector signed maximum
(rule (lower (vimax (vector_type V8B) x y))
      (aarch64_vec_smax V8B x y))

(rule (lower (vimax (vector_type V16B) x y))
      (aarch64_vec_smax V16B x y))

(rule (lower (vimax (vector_type V4H) x y))
      (aarch64_vec_smax V4H x y))

(rule (lower (vimax (vector_type V8H) x y))
      (aarch64_vec_smax V8H x y))

(rule (lower (vimax (vector_type V2S) x y))
      (aarch64_vec_smax V2S x y))

(rule (lower (vimax (vector_type V4S) x y))
      (aarch64_vec_smax V4S x y))

;; Vector reduction lowering rules

;; vreduce_add: Horizontal vector addition (sum all elements)
(rule (lower (vreduce_add (vector_type V8B) x))
      (aarch64_addv V8B x))

(rule (lower (vreduce_add (vector_type V16B) x))
      (aarch64_addv V16B x))

(rule (lower (vreduce_add (vector_type V4H) x))
      (aarch64_addv V4H x))

(rule (lower (vreduce_add (vector_type V8H) x))
      (aarch64_addv V8H x))

(rule (lower (vreduce_add (vector_type V2S) x))
      (aarch64_addv V2S x))

(rule (lower (vreduce_add (vector_type V4S) x))
      (aarch64_addv V4S x))

;; vreduce_smin: Horizontal signed minimum
(rule (lower (vreduce_smin (vector_type V8B) x))
      (aarch64_sminv V8B x))

(rule (lower (vreduce_smin (vector_type V16B) x))
      (aarch64_sminv V16B x))

(rule (lower (vreduce_smin (vector_type V4H) x))
      (aarch64_sminv V4H x))

(rule (lower (vreduce_smin (vector_type V8H) x))
      (aarch64_sminv V8H x))

(rule (lower (vreduce_smin (vector_type V4S) x))
      (aarch64_sminv V4S x))

;; vreduce_smax: Horizontal signed maximum
(rule (lower (vreduce_smax (vector_type V8B) x))
      (aarch64_smaxv V8B x))

(rule (lower (vreduce_smax (vector_type V16B) x))
      (aarch64_smaxv V16B x))

(rule (lower (vreduce_smax (vector_type V4H) x))
      (aarch64_smaxv V4H x))

(rule (lower (vreduce_smax (vector_type V8H) x))
      (aarch64_smaxv V8H x))

(rule (lower (vreduce_smax (vector_type V4S) x))
      (aarch64_smaxv V4S x))

;; vreduce_umin: Horizontal unsigned minimum
(rule (lower (vreduce_umin (vector_type V8B) x))
      (aarch64_uminv V8B x))

(rule (lower (vreduce_umin (vector_type V16B) x))
      (aarch64_uminv V16B x))

(rule (lower (vreduce_umin (vector_type V4H) x))
      (aarch64_uminv V4H x))

(rule (lower (vreduce_umin (vector_type V8H) x))
      (aarch64_uminv V8H x))

(rule (lower (vreduce_umin (vector_type V4S) x))
      (aarch64_uminv V4S x))

;; vreduce_umax: Horizontal unsigned maximum
(rule (lower (vreduce_umax (vector_type V8B) x))
      (aarch64_umaxv V8B x))

(rule (lower (vreduce_umax (vector_type V16B) x))
      (aarch64_umaxv V16B x))

(rule (lower (vreduce_umax (vector_type V4H) x))
      (aarch64_umaxv V4H x))

(rule (lower (vreduce_umax (vector_type V8H) x))
      (aarch64_umaxv V8H x))

(rule (lower (vreduce_umax (vector_type V4S) x))
      (aarch64_umaxv V4S x))

;; Atomic operations lowering

;; atomic_load: Load with acquire semantics
;; Pattern: atomic_load(addr) => LDAR
(rule (lower (atomic_load addr _flags))
      (aarch64_ldar addr))

;; atomic_store: Store with release semantics
;; Pattern: atomic_store(val, addr) => STLR
(rule (lower (atomic_store val addr _flags))
      (aarch64_stlr val addr))

;; atomic_rmw: Atomic read-modify-write operations
;; These use LSE (Large System Extensions) instructions when available

;; atomic_rmw.add: Atomic add
;; Pattern: atomic_rmw.add(addr, val) => LDADD / LDADDAL
(rule (lower (atomic_rmw_add addr val))
      (aarch64_ldadd addr val))

;; atomic_rmw.sub: Atomic subtract (implemented as add with negated value)
;; Pattern: atomic_rmw.sub(addr, val) => LDADD with -val
(rule (lower (atomic_rmw_sub addr val))
      (let ((neg_val Aarch64Inst (aarch64_neg $I64 val)))
            (aarch64_ldadd addr neg_val)))

;; atomic_rmw.and: Atomic AND (implemented as clear with complement)
;; Pattern: atomic_rmw.and(addr, val) => LDCLR with ~val
(rule (lower (atomic_rmw_and addr val))
      (let ((not_val Aarch64Inst (aarch64_mvn_rr $I64 val)))
            (aarch64_ldclr addr not_val)))

;; atomic_rmw.or: Atomic OR
;; Pattern: atomic_rmw.or(addr, val) => LDSET
(rule (lower (atomic_rmw_or addr val))
      (aarch64_ldset addr val))

;; atomic_rmw.xor: Atomic XOR
;; Pattern: atomic_rmw.xor(addr, val) => LDEOR
(rule (lower (atomic_rmw_xor addr val))
      (aarch64_ldeor addr val))

;; atomic_rmw.xchg: Atomic exchange
;; Pattern: atomic_rmw.xchg(addr, val) => SWPAL
(rule (lower (atomic_rmw_xchg addr val))
      (aarch64_swpal addr val))

;; atomic_rmw.smax: Atomic signed maximum
;; Pattern: atomic_rmw.smax(addr, val) => LDSMAX
(rule (lower (atomic_rmw_smax addr val))
      (aarch64_ldsmax addr val))

;; atomic_rmw.smin: Atomic signed minimum
;; Pattern: atomic_rmw.smin(addr, val) => LDSMIN
(rule (lower (atomic_rmw_smin addr val))
      (aarch64_ldsmin addr val))

;; atomic_rmw.umax: Atomic unsigned maximum
;; Pattern: atomic_rmw.umax(addr, val) => LDUMAX
(rule (lower (atomic_rmw_umax addr val))
      (aarch64_ldumax addr val))

;; atomic_rmw.umin: Atomic unsigned minimum
;; Pattern: atomic_rmw.umin(addr, val) => LDUMIN
(rule (lower (atomic_rmw_umin addr val))
      (aarch64_ldumin addr val))

;; atomic_cas: Compare-and-swap with acquire/release semantics
;; Pattern: atomic_cas(addr, expected, new) => CASAL
(rule (lower (atomic_cas addr expected new_val))
      (aarch64_casal addr expected new_val))

;; Memory barrier lowering

;; fence: Memory fence with sequential consistency
;; Pattern: fence(seq_cst) => DSB SY (full system barrier)
(rule (lower (fence AtomicOrdering.seq_cst))
      (aarch64_dsb ShareabilityDomain.SY))

;; fence: Memory fence with acquire semantics
;; Pattern: fence(acquire) => DMB ISHLD (inner shareable load barrier)
(rule (lower (fence AtomicOrdering.acquire))
      (aarch64_dmb ShareabilityDomain.ISHLD))

;; fence: Memory fence with release semantics
;; Pattern: fence(release) => DMB ISH (inner shareable barrier)
(rule (lower (fence AtomicOrdering.release))
      (aarch64_dmb ShareabilityDomain.ISH))

;; fence: Memory fence with acquire-release semantics
;; Pattern: fence(acq_rel) => DMB ISH (inner shareable barrier)
(rule (lower (fence AtomicOrdering.acq_rel))
      (aarch64_dmb ShareabilityDomain.ISH))

;; System register access lowering

;; SystemReg enum type for system registers
(type SystemReg (enum
  nzcv         ;; Condition flags (Negative, Zero, Carry, Overflow)
  fpcr         ;; Floating-point Control Register
  fpsr         ;; Floating-point Status Register
  tpidr_el0    ;; Thread Pointer/ID Register (User Read/Write)
  tpidrro_el0  ;; Thread Pointer/ID Register (User Read-Only)
))

;; Constructor declarations for system register access

;; MRS - Move from System Register (read system register)
(decl aarch64_mrs (SystemReg) Aarch64Inst)

;; MSR - Move to System Register (write system register)
(decl aarch64_msr (SystemReg Value) Aarch64Inst)

;; Helper functions
(decl negate_i64 (i64) i64)

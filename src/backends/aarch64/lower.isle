;; aarch64 ISLE lowering rules
;; Minimal bootstrap set for ARM64 instruction selection

;; Primitive types (required by ISLE compiler)
(type Unit (primitive Unit))
(type Type (primitive Type))
(type Value (primitive Value))
(type Inst (primitive Inst))

;; Relocation distance for calls/symbols
(type RelocDistance extern (enum (Near) (Far)))

;; Vector ALU operation which modifies a source register
(type VecALUModOp extern (enum (Fmla) (Fmls)))

;; Flags infrastructure for instructions that produce/consume CPU flags
;; Used for I128 arithmetic with carry (ADDS+ADC, SUBS+SBC)
(type ProducesFlags (enum
  ;; Instruction produces flags and returns a result that can be consumed
  (ProducesFlagsReturnsResultWithConsumer (inst Aarch64Inst) (result Reg))
  ;; Instruction produces flags as side effect only
  (ProducesFlagsSideEffect (inst Aarch64Inst))))

(type ConsumesFlags (enum
  ;; Instruction consumes flags and returns result, to be paired with producer
  (ConsumesFlagsReturnsResultWithProducer (inst Aarch64Inst) (result Reg))
  ;; Instruction consumes flags, returns result, but stands alone
  (ConsumesFlagsReturnsReg (inst Aarch64Inst) (result Reg))
  ;; Instruction consumes flags, two instructions returning ValueRegs
  (ConsumesFlagsTwiceReturnsValueRegs (inst1 Aarch64Inst) (inst2 Aarch64Inst) (result ValueRegs))))

;; Type extractors (foundation for type-parameterized patterns)
(decl multi_lane (Type) (u32 u32))
(extern extractor multi_lane multi_lane)

(decl fits_in_64 (Type) Type)
(extern extractor fits_in_64 fits_in_64)

(decl lane_fits_in_32 (Type) Type)
(extern extractor lane_fits_in_32 lane_fits_in_32)

(decl ty_vec128 (Type) Type)
(extern extractor ty_vec128 ty_vec128)

(decl ty_vec128_int (Type) Type)
(extern extractor ty_vec128_int ty_vec128_int)

(decl not_i64x2 (Type) Type)
(extern extractor not_i64x2 not_i64x2)

(decl pure ty_bits (Type) u8)
(extern constructor ty_bits ty_bits)

(decl pure shift_masked_imm (Type u64) u8)
(extern constructor shift_masked_imm shift_masked_imm)

;; Vector size helper - maps multi_lane patterns to VectorSize enum
(decl vector_size (Type) VectorSize)
(rule 1 (vector_size (multi_lane 8 8)) (VectorSize.V8B))
(rule 1 (vector_size (multi_lane 8 16)) (VectorSize.V16B))
(rule 1 (vector_size (multi_lane 16 4)) (VectorSize.V4H))
(rule 1 (vector_size (multi_lane 16 8)) (VectorSize.V8H))
(rule 1 (vector_size (multi_lane 32 2)) (VectorSize.V2S))
(rule 1 (vector_size (multi_lane 32 4)) (VectorSize.V4S))
(rule 1 (vector_size (multi_lane 64 2)) (VectorSize.V2D))

;; Vector arithmetic helpers (wrappers for type-parameterized rules)
(decl add_vec (Value Value VectorSize) Aarch64Inst)
(rule (add_vec x y size) (aarch64_vec_add size x y))

(decl sub_vec (Value Value VectorSize) Aarch64Inst)
(rule (sub_vec x y size) (aarch64_vec_sub size x y))

(decl mul_vec (Value Value VectorSize) Aarch64Inst)
(rule (mul_vec x y size) (aarch64_vec_mul size x y))

(decl min_vec (Value Value VectorSize) Aarch64Inst)
(rule (min_vec x y size) (aarch64_vec_smin size x y))

(decl max_vec (Value Value VectorSize) Aarch64Inst)
(rule (max_vec x y size) (aarch64_vec_smax size x y))

;; Vector bitwise helpers (wrappers for type-parameterized rules)
(decl and_vec (Value Value VectorSize) Aarch64Inst)
(rule (and_vec x y size) (aarch64_vec_and size x y))

(decl or_vec (Value Value VectorSize) Aarch64Inst)
(rule (or_vec x y size) (aarch64_vec_orr size x y))

(decl xor_vec (Value Value VectorSize) Aarch64Inst)
(rule (xor_vec x y size) (aarch64_vec_eor size x y))

(decl not_vec (Value VectorSize) Aarch64Inst)
(rule (not_vec x size) (aarch64_vec_not size x))

(decl bic_vec (Value Value VectorSize) Aarch64Inst)
(rule (bic_vec x y size) (aarch64_vec_bic size x y))

(decl neg_vec (Value VectorSize) Aarch64Inst)
(rule (neg_vec x size) (aarch64_vec_neg size x))

;; Vector reduction helpers (wrappers for type-parameterized rules)
(decl reduce_add_vec (Value VectorSize) Aarch64Inst)
(rule (reduce_add_vec x size) (aarch64_addv size x))

(decl reduce_smin_vec (Value VectorSize) Aarch64Inst)
(rule (reduce_smin_vec x size) (aarch64_sminv size x))

(decl reduce_smax_vec (Value VectorSize) Aarch64Inst)
(rule (reduce_smax_vec x size) (aarch64_smaxv size x))

(decl reduce_umin_vec (Value VectorSize) Aarch64Inst)
(rule (reduce_umin_vec x size) (aarch64_uminv size x))

(decl reduce_umax_vec (Value VectorSize) Aarch64Inst)
(rule (reduce_umax_vec x size) (aarch64_umaxv size x))

;; Vector shift by immediate helpers
(decl vec_shift_imm (VecShiftImmOp u8 Value VectorSize) Aarch64Inst)
(rule (vec_shift_imm op imm src size)
      (aarch64_vec_shift_imm op imm src size))

(decl ushl_vec_imm (Value u8 VectorSize) Aarch64Inst)
(rule (ushl_vec_imm x amt size)
      (vec_shift_imm (VecShiftImmOp.Shl) amt x size))

(decl ushr_vec_imm (Value u8 VectorSize) Aarch64Inst)
(rule (ushr_vec_imm x amt size)
      (vec_shift_imm (VecShiftImmOp.Ushr) amt x size))

(decl sshr_vec_imm (Value u8 VectorSize) Aarch64Inst)
(rule (sshr_vec_imm x amt size)
      (vec_shift_imm (VecShiftImmOp.Sshr) amt x size))

;; Flags infrastructure combinators

;; Combine a flags-producing instruction with a flags-consuming instruction
;; This handles the common case: producer returns result, consumer returns result
(decl with_flags (ProducesFlags ConsumesFlags) ValueRegs)
(rule (with_flags (ProducesFlags.ProducesFlagsReturnsResultWithConsumer producer_inst producer_result)
                  (ConsumesFlags.ConsumesFlagsReturnsResultWithProducer consumer_inst consumer_result))
      (value_regs producer_result consumer_result))

;; Helper for ADDS instruction (add with flags, returns result + produces flags)
(decl add_with_flags_paired (Type Reg Reg) ProducesFlags)
(rule (add_with_flags_paired ty src1 src2)
      (let ((dst Reg (temp_reg $I64)))
        (ProducesFlags.ProducesFlagsReturnsResultWithConsumer
         (aarch64_adds_rr ty dst src1 src2)
         dst)))

;; Helper for ADC instruction (add with carry, consumes flags + returns result)
(decl adc_paired (Type Reg Reg) ConsumesFlags)
(rule (adc_paired ty src1 src2)
      (let ((dst Reg (temp_reg $I64)))
        (ConsumesFlags.ConsumesFlagsReturnsResultWithProducer
         (aarch64_adc_rr ty dst src1 src2)
         dst)))

;; Helper for SUBS instruction (subtract with flags, returns result + produces flags)
(decl sub_with_flags_paired (Type Reg Reg) ProducesFlags)
(rule (sub_with_flags_paired ty src1 src2)
      (let ((dst Reg (temp_reg $I64)))
        (ProducesFlags.ProducesFlagsReturnsResultWithConsumer
         (aarch64_subs_rr ty dst src1 src2)
         dst)))

;; Helper for SBC instruction (subtract with carry, consumes flags + returns result)
(decl sbc_paired (Type Reg Reg) ConsumesFlags)
(rule (sbc_paired ty src1 src2)
      (let ((dst Reg (temp_reg $I64)))
        (ConsumesFlags.ConsumesFlagsReturnsResultWithProducer
         (aarch64_sbc_rr ty dst src1 src2)
         dst)))

;; Helper to concatenate two flags-consuming instructions into ValueRegs
(decl consumes_flags_concat (ConsumesFlags ConsumesFlags) ConsumesFlags)
(rule (consumes_flags_concat (ConsumesFlags.ConsumesFlagsReturnsReg inst1 reg1)
                             (ConsumesFlags.ConsumesFlagsReturnsReg inst2 reg2))
      (ConsumesFlags.ConsumesFlagsTwiceReturnsValueRegs inst1 inst2 (value_regs reg1 reg2)))

;; Zero register helper
(decl zero_reg () Reg)
(rule (zero_reg) (iconst_to_reg $I64 0))

;; Type extractors for select condition handling
(decl fits_in_32 (Type) Type)
(extern extractor fits_in_32 fits_in_32)

;; Zero-extension helpers for select conditions
(decl put_in_reg_zext32 (Value) Reg)
(extern constructor put_in_reg_zext32 put_in_reg_zext32)

(decl put_in_reg_zext64 (Value) Reg)
(extern constructor put_in_reg_zext64 put_in_reg_zext64)

;; Comparison helper (produces flags)
(decl cmp (Type Reg Reg) ProducesFlags)
(extern constructor cmp cmp)

;; Register-form shift helpers that wrap ISLE instructions
(decl lsl (Type Reg Reg) Reg)
(rule (lsl ty x amt) (aarch64_lsl_rr ty x amt))

(decl lsr (Type Reg Reg) Reg)
(rule (lsr ty x amt) (aarch64_lsr_rr ty x amt))

(decl asr (Type Reg Reg) Reg)
(rule (asr ty x amt) (aarch64_asr_rr ty x amt))

;; Immediate-form shift helpers
(decl lsl_imm (Type Reg i64) Reg)
(rule (lsl_imm ty x imm) (aarch64_lsl_imm ty x imm))

(decl lsr_imm (Type Reg i64) Reg)
(rule (lsr_imm ty x imm) (aarch64_lsr_imm ty x imm))

;; ORR-NOT helper: orr dst, src1, NOT(src2)
(decl orr_not (Type Reg Reg) Reg)
(rule (orr_not ty src1 src2) (aarch64_orn_rr ty src1 src2))

;; ORR helper: orr dst, src1, src2
(decl orr (Type Reg Reg) Reg)
(rule (orr ty x y) (aarch64_orr_rr ty x y))

;; SUB helper: sub dst, src1, src2
(decl sub (Type Reg Reg) Reg)
(rule (sub ty x y) (aarch64_sub_rr ty x y))

;; TST immediate: test bits and set flags
(decl tst_imm (Type Reg i64) ProducesFlags)
(rule (tst_imm ty reg imm)
      (ProducesFlags.ProducesFlagsSideEffect
       (aarch64_tst_imm ty reg imm)))

;; Conditional select: returns reg based on condition flags
(decl csel (IntCC Reg Reg) ConsumesFlags)
(rule (csel cc true_val false_val)
      (let ((dst Reg (temp_reg $I64)))
        (ConsumesFlags.ConsumesFlagsReturnsReg
         (aarch64_csel $I64 true_val false_val (iconst_to_reg $I64 0) cc)
         dst)))

;; Helper for I128 bitwise operations (no carry, just operate on low/high independently)
(decl i128_bitwise (Value Value (Type Value Value) Aarch64Inst) ValueRegs)
(rule (i128_bitwise x y op)
      (let ((x_regs ValueRegs x)
            (x_lo Reg (value_regs_get x_regs 0))
            (x_hi Reg (value_regs_get x_regs 1))
            (y_regs ValueRegs y)
            (y_lo Reg (value_regs_get y_regs 0))
            (y_hi Reg (value_regs_get y_regs 1)))
        (value_regs
          (op $I64 x_lo y_lo)
          (op $I64 x_hi y_hi))))

;; Integer arithmetic lowering

;; iadd: Add two integers (default, lowest priority)
(rule 0 (lower (iadd ty x y))
      (aarch64_add_rr ty x y))

;; iadd with sign-extended byte operand (higher priority than generic)
;; Pattern: iadd(x, sextb(y)) => ADD Xd, Xn, Wm, SXTB
(rule 2 (lower (iadd ty x (aarch64_sxtb ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtb))

;; iadd with sign-extended halfword operand (higher priority than generic)
;; Pattern: iadd(x, sexth(y)) => ADD Xd, Xn, Wm, SXTH
(rule 2 (lower (iadd ty x (aarch64_sxth ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxth))

;; iadd with sign-extended word operand (higher priority than generic)
;; Pattern: iadd(x, sextw(y)) => ADD Xd, Xn, Wm, SXTW
(rule 2 (lower (iadd ty x (aarch64_sxtw y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtw))

;; iadd with zero-extended byte operand (higher priority than generic)
;; Pattern: iadd(x, uextb(y)) => ADD Xd, Xn, Wm, UXTB
(rule 2 (lower (iadd ty x (aarch64_uxtb ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxtb))

;; iadd with zero-extended halfword operand (higher priority than generic)
;; Pattern: iadd(x, uexth(y)) => ADD Xd, Xn, Wm, UXTH
(rule 2 (lower (iadd ty x (aarch64_uxth ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxth))

;; iadd with commuted operands (sign-extended byte, higher priority)
(rule 2 (lower (iadd ty (aarch64_sxtb ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtb))

;; iadd with commuted operands (sign-extended halfword, higher priority)
(rule 2 (lower (iadd ty (aarch64_sxth ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxth))

;; iadd with commuted operands (sign-extended word, higher priority)
(rule 2 (lower (iadd ty (aarch64_sxtw y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtw))

;; iadd with commuted operands (zero-extended byte, higher priority)
(rule 2 (lower (iadd ty (aarch64_uxtb ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxtb))

;; iadd with commuted operands (zero-extended halfword, higher priority)
(rule 2 (lower (iadd ty (aarch64_uxth ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxth))

;; iadd with left-shifted operand (higher priority than generic)
;; Pattern: iadd(x, ishl(y, iconst(shift))) => ADD Xd, Xn, Xm, LSL #shift
(rule 3 (lower (iadd ty x (ishl shift_ty y (iconst (u64_from_imm64 shift)))))
      (if-let $I64 ty)
      (if (u64_lteq shift 63))
      (aarch64_add_shifted ty x y ShiftOp.lsl (u64_to_u6 shift)))

;; iadd with commuted left-shifted operand
(rule 3 (lower (iadd ty (ishl shift_ty y (iconst (u64_from_imm64 shift))) x))
      (if-let $I64 ty)
      (if (u64_lteq shift 63))
      (aarch64_add_shifted ty x y ShiftOp.lsl (u64_to_u6 shift)))

;; iadd with right-shifted operand (logical)
;; Pattern: iadd(x, ushr(y, iconst(shift))) => ADD Xd, Xn, Xm, LSR #shift
(rule 3 (lower (iadd ty x (ushr shift_ty y (iconst (u64_from_imm64 shift)))))
      (if-let $I64 ty)
      (if (u64_lteq shift 63))
      (aarch64_add_shifted ty x y ShiftOp.lsr (u64_to_u6 shift)))

;; iadd with commuted right-shifted operand (logical)
(rule 3 (lower (iadd ty (ushr shift_ty y (iconst (u64_from_imm64 shift))) x))
      (if-let $I64 ty)
      (if (u64_lteq shift 63))
      (aarch64_add_shifted ty x y ShiftOp.lsr (u64_to_u6 shift)))

;; iadd with arithmetic right-shifted operand
;; Pattern: iadd(x, sshr(y, iconst(shift))) => ADD Xd, Xn, Xm, ASR #shift
(rule 3 (lower (iadd ty x (sshr shift_ty y (iconst (u64_from_imm64 shift)))))
      (if-let $I64 ty)
      (if (u64_lteq shift 63))
      (aarch64_add_shifted ty x y ShiftOp.asr (u64_to_u6 shift)))

;; iadd with commuted arithmetic right-shifted operand
(rule 3 (lower (iadd ty (sshr shift_ty y (iconst (u64_from_imm64 shift))) x))
      (if-let $I64 ty)
      (if (u64_lteq shift 63))
      (aarch64_add_shifted ty x y ShiftOp.asr (u64_to_u6 shift)))

;; iadd: I128 addition using register pairs (ADDS + ADC)
;; Priority -3 to match before vector and scalar rules
(rule -3 (lower (has_type $I128 (iadd x y)))
      (let (;; Get the high/low registers for x
            (x_regs ValueRegs x)
            (x_lo Reg (value_regs_get x_regs 0))
            (x_hi Reg (value_regs_get x_regs 1))
            ;; Get the high/low registers for y
            (y_regs ValueRegs y)
            (y_lo Reg (value_regs_get y_regs 0))
            (y_hi Reg (value_regs_get y_regs 1)))
        ;; ADDS (low 64 bits, sets carry) + ADC (high 64 bits, uses carry)
        (with_flags
          (add_with_flags_paired $I64 x_lo y_lo)
          (adc_paired $I64 x_hi y_hi))))

;; iadd: Vector integer addition (type-parameterized via multi_lane)
(rule 1 (lower (has_type ty @ (multi_lane _ _) (iadd x y)))
      (add_vec x y (vector_size ty)))

;; isub: I128 subtraction using register pairs (SUBS + SBC)
;; Priority -3 to match before vector and scalar rules
(rule -3 (lower (has_type $I128 (isub x y)))
      (let (;; Get the high/low registers for x
            (x_regs ValueRegs x)
            (x_lo Reg (value_regs_get x_regs 0))
            (x_hi Reg (value_regs_get x_regs 1))
            ;; Get the high/low registers for y
            (y_regs ValueRegs y)
            (y_lo Reg (value_regs_get y_regs 0))
            (y_hi Reg (value_regs_get y_regs 1)))
        ;; SUBS (low 64 bits, sets borrow) + SBC (high 64 bits, uses borrow)
        (with_flags
          (sub_with_flags_paired $I64 x_lo y_lo)
          (sbc_paired $I64 x_hi y_hi))))

;; isub: Subtract integers (scalar)
(rule (lower (isub ty x y))
      (aarch64_sub_rr ty x y))

;; isub with left-shifted operand (higher priority than generic)
;; Pattern: isub(x, ishl(y, iconst(shift))) => SUB Xd, Xn, Xm, LSL #shift
(rule 2 (lower (isub ty x (ishl shift_ty y (iconst (u64_from_imm64 shift)))))
      (if-let $I64 ty)
      (if (u64_lteq shift 63))
      (aarch64_sub_shifted ty x y ShiftOp.lsl (u64_to_u6 shift)))

;; isub with right-shifted operand (logical)
;; Pattern: isub(x, ushr(y, iconst(shift))) => SUB Xd, Xn, Xm, LSR #shift
(rule 2 (lower (isub ty x (ushr shift_ty y (iconst (u64_from_imm64 shift)))))
      (if-let $I64 ty)
      (if (u64_lteq shift 63))
      (aarch64_sub_shifted ty x y ShiftOp.lsr (u64_to_u6 shift)))

;; isub with arithmetic right-shifted operand
;; Pattern: isub(x, sshr(y, iconst(shift))) => SUB Xd, Xn, Xm, ASR #shift
(rule 2 (lower (isub ty x (sshr shift_ty y (iconst (u64_from_imm64 shift)))))
      (if-let $I64 ty)
      (if (u64_lteq shift 63))
      (aarch64_sub_shifted ty x y ShiftOp.asr (u64_to_u6 shift)))

;; isub with sign-extended byte operand (higher priority than generic)
;; Pattern: isub(x, sextb(y)) => SUB Xd, Xn, Wm, SXTB
(rule 2 (lower (isub ty x (aarch64_sxtb ext_ty y)))
      (if-let $I64 ty)
      (aarch64_sub_extended ty x y ExtendOp.sxtb))

;; isub with sign-extended halfword operand (higher priority than generic)
;; Pattern: isub(x, sexth(y)) => SUB Xd, Xn, Wm, SXTH
(rule 2 (lower (isub ty x (aarch64_sxth ext_ty y)))
      (if-let $I64 ty)
      (aarch64_sub_extended ty x y ExtendOp.sxth))

;; isub with sign-extended word operand (higher priority than generic)
;; Pattern: isub(x, sextw(y)) => SUB Xd, Xn, Wm, SXTW
(rule 2 (lower (isub ty x (aarch64_sxtw y)))
      (if-let $I64 ty)
      (aarch64_sub_extended ty x y ExtendOp.sxtw))

;; isub with zero-extended byte operand (higher priority than generic)
;; Pattern: isub(x, uextb(y)) => SUB Xd, Xn, Wm, UXTB
(rule 2 (lower (isub ty x (aarch64_uxtb ext_ty y)))
      (if-let $I64 ty)
      (aarch64_sub_extended ty x y ExtendOp.uxtb))

;; isub with zero-extended halfword operand (higher priority than generic)
;; Pattern: isub(x, uexth(y)) => SUB Xd, Xn, Wm, UXTH
(rule 2 (lower (isub ty x (aarch64_uxth ext_ty y)))
      (if-let $I64 ty)
      (aarch64_sub_extended ty x y ExtendOp.uxth))

;; isub: Vector integer subtraction (type-parameterized via multi_lane)
(rule 1 (lower (has_type ty @ (multi_lane _ _) (isub x y)))
      (sub_vec x y (vector_size ty)))

;; Saturating arithmetic for 128-bit vectors
(rule (lower (has_type (ty_vec128 ty) (uadd_sat x y)))
      (uqadd x y (vector_size ty)))

(rule (lower (has_type (ty_vec128 ty) (sadd_sat x y)))
      (sqadd x y (vector_size ty)))

(rule (lower (has_type (ty_vec128 ty) (usub_sat x y)))
      (uqsub x y (vector_size ty)))

(rule (lower (has_type (ty_vec128 ty) (ssub_sat x y)))
      (sqsub x y (vector_size ty)))

;; imul: I128 multiplication using register pairs
;; Formula: dst_lo = x_lo * y_lo
;;          dst_hi = umulh(x_lo, y_lo) + (x_lo * y_hi) + (x_hi * y_lo)
;; Implemented as:
;;   dst_hi = umulh(x_lo, y_lo)
;;   dst_hi = madd(x_lo, y_hi, dst_hi)
;;   dst_hi = madd(x_hi, y_lo, dst_hi)
;;   dst_lo = madd(x_lo, y_lo, 0)
(rule -1 (lower (has_type $I128 (imul x y)))
      (let ((x_regs ValueRegs x)
            (x_lo Reg (value_regs_get x_regs 0))
            (x_hi Reg (value_regs_get x_regs 1))
            (y_regs ValueRegs y)
            (y_lo Reg (value_regs_get y_regs 0))
            (y_hi Reg (value_regs_get y_regs 1))
            (zero Reg (iconst_to_reg $I64 0))
            ;; Compute high part contributions
            (dst_hi1 Reg (aarch64_umulh x_lo y_lo))
            (dst_hi2 Reg (aarch64_madd $I64 x_lo y_hi dst_hi1))
            (dst_hi Reg (aarch64_madd $I64 x_hi y_lo dst_hi2))
            ;; Compute low part
            (dst_lo Reg (aarch64_madd $I64 x_lo y_lo zero)))
        (value_regs dst_lo dst_hi)))

;; imul: Multiply integers (scalar)
(rule (lower (imul ty x y))
      (aarch64_mul_rr ty x y))

;; imul: Vector integer multiplication for 128-bit vectors (not I64X2)
;; I64X2 needs special handling with 32-bit operations
(rule -2 (lower (has_type (ty_vec128 (not_i64x2 ty)) (imul x y)))
      (mul_vec x y (vector_size ty)))

;; imul: Vector integer multiplication (type-parameterized via multi_lane)
(rule 1 (lower (has_type ty @ (multi_lane _ _) (imul x y)))
      (mul_vec x y (vector_size ty)))

;; imin: Vector signed integer minimum (type-parameterized via multi_lane)
(rule 1 (lower (has_type ty @ (multi_lane _ _) (imin x y)))
      (min_vec x y (vector_size ty)))

;; imax: Vector signed integer maximum (type-parameterized via multi_lane)
(rule 1 (lower (has_type ty @ (multi_lane _ _) (imax x y)))
      (max_vec x y (vector_size ty)))

;; umin: Vector unsigned integer minimum
;; Use native UMIN instruction for all lanes except I64X2
(rule (lower (has_type ty @ (not_i64x2) (umin x y)))
      (vec_rrr (VecALUOp.Umin) x y (vector_size ty)))

;; umin: I64X2 unsigned minimum (emulated via compare + bitselect)
;; No native I64X2 UMIN, use: BSL (x > y), x, y
(rule 1 (lower (has_type $I64X2 (umin x y)))
      (aarch64_bitselect (vec_rrr (VecALUOp.Cmhi) y x (VectorSize.Size64x2)) x y))

;; umax: Vector unsigned integer maximum
;; Use native UMAX instruction for all lanes except I64X2
(rule (lower (has_type ty @ (not_i64x2) (umax x y)))
      (vec_rrr (VecALUOp.Umax) x y (vector_size ty)))

;; umax: I64X2 unsigned maximum (emulated via compare + bitselect)
;; No native I64X2 UMAX, use: BSL (x > y), x, y
(rule 1 (lower (has_type $I64X2 (umax x y)))
      (aarch64_bitselect (vec_rrr (VecALUOp.Cmhi) x y (VectorSize.Size64x2)) x y))

;; Vector reduction operations (type-parameterized via multi_lane)

;; reduce_add: Horizontal vector addition (sum all elements)
(rule 1 (lower (has_type ty @ (multi_lane _ _) (reduce_add x)))
      (reduce_add_vec x (vector_size ty)))

;; reduce_smin: Horizontal signed minimum
(rule 1 (lower (has_type ty @ (multi_lane _ _) (reduce_smin x)))
      (reduce_smin_vec x (vector_size ty)))

;; reduce_smax: Horizontal signed maximum
(rule 1 (lower (has_type ty @ (multi_lane _ _) (reduce_smax x)))
      (reduce_smax_vec x (vector_size ty)))

;; reduce_umin: Horizontal unsigned minimum
(rule 1 (lower (has_type ty @ (multi_lane _ _) (reduce_umin x)))
      (reduce_umin_vec x (vector_size ty)))

;; reduce_umax: Horizontal unsigned maximum
(rule 1 (lower (has_type ty @ (multi_lane _ _) (reduce_umax x)))
      (reduce_umax_vec x (vector_size ty)))

;; Multiply-add fusion patterns (highest priority - fuse before generic ops)
;; iadd(a, imul(x, y)) => madd(x, y, a)
(rule 3 (lower (iadd ty (imul ty x y) a))
      (aarch64_madd ty x y a))

(rule 3 (lower (iadd ty a (imul ty x y)))
      (aarch64_madd ty x y a))

;; Multiply-subtract fusion patterns (highest priority - fuse before generic ops)
;; isub(a, imul(x, y)) => msub(x, y, a)
(rule 3 (lower (isub ty a (imul ty x y)))
      (aarch64_msub ty x y a))

;; smul_hi: Signed multiply returning upper bits (64×64→128, return high 64)
(rule (lower (smul_hi ty x y))
      (if-let $I64 ty)
      (aarch64_smulh x y))

;; umul_hi: Unsigned multiply returning upper bits (64×64→128, return high 64)
(rule (lower (umul_hi ty x y))
      (if-let $I64 ty)
      (aarch64_umulh x y))

;; smulhi: Signed multiply returning upper bits (64×64→128, return high 64)
(rule (lower (has_type $I64 (smulhi x y)))
      (aarch64_smulh x y))

;; umulhi: Unsigned multiply returning upper bits (64×64→128, return high 64)
(rule (lower (has_type $I64 (umulhi x y)))
      (aarch64_umulh x y))

;; sdiv: Signed divide
(rule (lower (sdiv ty x y))
      (aarch64_sdiv ty x y))

;; udiv: Unsigned divide
(rule (lower (udiv ty x y))
      (aarch64_udiv ty x y))

;; srem: Signed remainder (implemented as sdiv + msub)
;; srem(x, y) = x - (x / y) * y
(rule (lower (srem ty x y))
      (let ((quot Aarch64Inst (aarch64_sdiv ty x y)))
            (aarch64_msub ty quot y x)))

;; urem: Unsigned remainder (implemented as udiv + msub)
;; urem(x, y) = x - (x / y) * y
(rule (lower (urem ty x y))
      (let ((quot Aarch64Inst (aarch64_udiv ty x y)))
            (aarch64_msub ty quot y x)))

;; Type conversion operations lowering

;; sextend: I128 sign-extend (64-bit to 128-bit)
;; Sign-extend by copying sign bit (bit 63) to all bits of high 64
(rule -2 (lower (has_type $I128 (sextend x)))
      (let ((lo Reg (value_regs_get x 0))
            (hi Reg (aarch64_asr_imm $I64 lo 63)))
        (value_regs lo hi)))

;; sextend+load fusion: Fuse sign-extension with load into extending load instruction
;; This avoids separate load + extend instructions

;; sextend(load I8) => LDRSB (sign-extending load byte)
(rule 5 (lower (sextend _ $I8 (load $I8 addr _flags _offset)))
      (aarch64_sload8 addr))

;; sextend(load I16) => LDRSH (sign-extending load halfword)
(rule 5 (lower (sextend _ $I16 (load $I16 addr _flags _offset)))
      (aarch64_sload16 addr))

;; sextend(load I32) => LDRSW (sign-extending load word to 64)
(rule 5 (lower (sextend $I64 $I32 (load $I32 addr _flags _offset)))
      (aarch64_sload32 addr))

;; sextend: Sign-extend to target type
;; I8 -> I16/I32/I64 => SXTB
(rule (lower (sextend dst_ty src_ty x))
      (if-let 8 (ty_bits src_ty))
      (aarch64_sxtb dst_ty x))

;; I16 -> I32/I64 => SXTH
(rule (lower (sextend dst_ty src_ty x))
      (if-let 16 (ty_bits src_ty))
      (aarch64_sxth dst_ty x))

;; I32 -> I64 => SXTW
(rule (lower (sextend dst_ty src_ty x))
      (if-let 32 (ty_bits src_ty))
      (if-let 64 (ty_bits dst_ty))
      (aarch64_sxtw x))

;; uextend: I128 zero-extend (64-bit to 128-bit)
;; Zero-extend by setting high 64 bits to 0
(rule -1 (lower (has_type $I128 (uextend x)))
      (value_regs (value_regs_get x 0) (iconst_to_reg $I64 0)))

;; uextend+load fusion: Fuse zero-extension with load into extending load instruction
;; This avoids separate load + extend instructions

;; uextend(load I8) => LDRB (zero-extending load byte)
(rule 5 (lower (uextend _ $I8 (load $I8 addr _flags _offset)))
      (aarch64_uload8 addr))

;; uextend(load I16) => LDRH (zero-extending load halfword)
(rule 5 (lower (uextend _ $I16 (load $I16 addr _flags _offset)))
      (aarch64_uload16 addr))

;; uextend(load I32) => LDR Wd (zero-extending load word to 64)
(rule 5 (lower (uextend $I64 $I32 (load $I32 addr _flags _offset)))
      (aarch64_uload32 addr))

;; uextend: Zero-extend to target type
;; I8 -> I16/I32/I64 => UXTB
(rule (lower (uextend dst_ty src_ty x))
      (if-let 8 (ty_bits src_ty))
      (aarch64_uxtb dst_ty x))

;; I16 -> I32/I64 => UXTH
(rule (lower (uextend dst_ty src_ty x))
      (if-let 16 (ty_bits src_ty))
      (aarch64_uxth dst_ty x))

;; I32 -> I64 => UXTW (note: 32-bit ops auto zero-extend on ARM64)
(rule (lower (uextend dst_ty src_ty x))
      (if-let 32 (ty_bits src_ty))
      (if-let 64 (ty_bits dst_ty))
      (aarch64_uxtw x))

;; ireduce: Truncate integer to narrower type
;; On ARM64, this is implicit when using smaller register size
;; I64 -> I32/I16/I8: move to W register (32-bit) truncates
(rule (lower (ireduce dst_ty src_ty x))
      (aarch64_ireduce dst_ty x))

;; fadd: Floating-point addition (FADD)
(rule (lower (fadd ty x y))
      (aarch64_fadd ty x y))

;; fsub: Floating-point subtraction (FSUB)
(rule (lower (fsub ty x y))
      (aarch64_fsub ty x y))

;; fmul: Floating-point multiplication (FMUL)
(rule (lower (fmul ty x y))
      (aarch64_fmul ty x y))

;; fdiv: Floating-point division (FDIV)
(rule (lower (fdiv ty x y))
      (aarch64_fdiv ty x y))

;; fmin: Floating-point minimum (FMIN)
(rule (lower (fmin ty x y))
      (aarch64_fmin ty x y))

;; fmax: Floating-point maximum (FMAX)
(rule (lower (fmax ty x y))
      (aarch64_fmax ty x y))

;; sqrt: Floating-point square root (FSQRT)
(rule (lower (sqrt ty x))
      (aarch64_fsqrt ty x))

;; splat: Duplicate scalar to all vector lanes (DUP)
;; splat: Optimized patterns for specific cases

;; splat: Float constant - use immediate splat
(rule (lower (has_type ty (splat (f32const k)))))
      (aarch64_splat_const ty k))

(rule (lower (has_type ty (splat (f64const k)))))
      (aarch64_splat_const ty k))

;; splat: Integer constant - use immediate splat
(rule (lower (has_type ty (splat (iconst k)))))
      (aarch64_splat_const ty k))

;; splat: Integer constant with ireduce - use immediate splat
(rule (lower (has_type ty (splat (ireduce (iconst k))))))
      (aarch64_splat_const ty k))

;; splat: Scalar float - use vec_dup_from_fpu
(rule -2 (lower (has_type ty (splat x @ (value_type (ty_scalar_float _))))))
      (vec_dup_from_fpu x (vector_size ty) 0))

;; splat: Generic fallback (lowest priority)
(rule 0 (lower (splat ty x))
      (aarch64_splat ty x))

;; extractlane: Extract vector lane to scalar (UMOV)
(rule (lower (extractlane ty vec lane))
      (aarch64_extractlane ty vec lane))

;; insertlane: Insert scalar into vector lane (INS)
(rule (lower (insertlane ty vec x lane))
      (aarch64_insertlane ty vec x lane))

;; fcvt_from_sint: Convert signed integer to float (SCVTF)
(rule (lower (fcvt_from_sint dst_ty src_ty x))
      (aarch64_scvtf dst_ty src_ty x))

;; fcvt_from_uint: Convert unsigned integer to float (UCVTF)
(rule (lower (fcvt_from_uint dst_ty src_ty x))
      (aarch64_ucvtf dst_ty src_ty x))

;; fpromote: Promote f32 to f64 (FCVT)
(rule (lower (fpromote dst_ty src_ty x))
      (if-let 32 (ty_bits src_ty))
      (if-let 64 (ty_bits dst_ty))
      (aarch64_fpromote x))

;; fdemote: Demote f64 to f32 (FCVT)
(rule (lower (fdemote dst_ty src_ty x))
      (if-let 64 (ty_bits src_ty))
      (if-let 32 (ty_bits dst_ty))
      (aarch64_fdemote x))

;; nearest: Round to nearest (FRINTN)
(rule (lower (nearest ty x))
      (aarch64_nearest ty x))

;; trunc: Round toward zero (FRINTZ)
(rule (lower (trunc ty x))
      (aarch64_trunc ty x))

;; ceil: Round toward +infinity (FRINTP)
(rule (lower (ceil ty x))
      (aarch64_ceil ty x))

;; floor: Round toward -infinity (FRINTM)
(rule (lower (floor ty x))
      (aarch64_floor ty x))

;; iconcat: Concatenate two integers into wider type (I64 + I64 → I128)
;; On ARM64, this creates a register pair from two separate registers
(rule (lower (has_type $I128 (iconcat lo hi)))
      (value_regs_from_values lo hi))

;; isplit: Split wider integer into two parts (I128 → I64 + I64)
;; Returns ValueRegs with low and high parts
(rule (lower (isplit x))
      (aarch64_isplit x))

;; Atomic operations lowering

;; atomic_load with acquire ordering: LDAR
(rule (lower (atomic_load ty addr AtomicOrdering.Acquire _flags))
      (aarch64_atomic_load_acquire ty addr))

;; atomic_load with sequential consistency: DMB + LDAR
;; Note: Simplified - full seq_cst may need DMB before as well
(rule (lower (atomic_load ty addr AtomicOrdering.SeqCst _flags))
      (aarch64_atomic_load_acquire ty addr))

;; atomic_store with release ordering: STLR
(rule (lower (atomic_store val addr AtomicOrdering.Release _flags))
      (let ((ty Type (value_type val)))
           (aarch64_atomic_store_release ty addr val)))

;; atomic_store with sequential consistency: STLR + DMB
;; Note: Simplified - full seq_cst may need DMB after as well
(rule (lower (atomic_store val addr AtomicOrdering.SeqCst _flags))
      (let ((ty Type (value_type val)))
           (aarch64_atomic_store_release ty addr val)))

;; fence: Memory barrier
(rule (lower (fence ordering))
      (aarch64_fence ordering))

;; Shift operations lowering

;; ishl: I128 left shift using register pairs
;;     lsl     lo_lshift, src_lo, amt
;;     lsl     hi_lshift, src_hi, amt
;;     mvn     inv_amt, amt
;;     lsr     lo_rshift, src_lo, #1
;;     lsr     lo_rshift, lo_rshift, inv_amt
;;     orr     maybe_hi, hi_lshift, lo_rshift
;;     tst     amt, #0x40
;;     csel    dst_hi, lo_lshift, maybe_hi, ne
;;     csel    dst_lo, xzr, lo_lshift, ne
(decl lower_shl128 (ValueRegs Reg) ValueRegs)
(rule (lower_shl128 src amt)
      (let ((src_lo Reg (value_regs_get src 0))
            (src_hi Reg (value_regs_get src 1))
            (lo_lshift Reg (lsl $I64 src_lo amt))
            (hi_lshift Reg (lsl $I64 src_hi amt))
            (inv_amt Reg (orr_not $I32 (zero_reg) amt))
            (lo_rshift Reg (lsr $I64 (lsr_imm $I64 src_lo 1) inv_amt))
            (maybe_hi Reg (orr $I64 hi_lshift lo_rshift)))
        (with_flags
         (tst_imm $I64 amt 64)
         (consumes_flags_concat
          (csel (IntCC.Ne) (zero_reg) lo_lshift)
          (csel (IntCC.Ne) lo_lshift maybe_hi)))))

(rule (lower (has_type $I128 (ishl x y)))
      (lower_shl128 x (value_regs_get y 0)))

;; ishl: Integer shift left (register form)
(rule (lower (ishl ty x y))
      (aarch64_lsl_rr ty x y))

;; ishl: Integer shift left (immediate form)
(rule (lower (ishl ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_lsl_imm ty x amt))

;; ishl: Vector shift left by immediate (ty_vec128)
(rule -2 (lower (has_type (ty_vec128 ty) (ishl x (iconst (u64_from_imm64 n)))))
      (ushl_vec_imm x (shift_masked_imm ty n) (vector_size ty)))

;; ushr: I128 unsigned right shift using register pairs
;;     lsr       lo_rshift, src_lo, amt
;;     lsr       hi_rshift, src_hi, amt
;;     mvn       inv_amt, amt
;;     lsl       hi_lshift, src_hi, #1
;;     lsl       hi_lshift, hi_lshift, inv_amt
;;     tst       amt, #0x40
;;     orr       maybe_lo, lo_rshift, hi_lshift
;;     csel      dst_hi, xzr, hi_rshift, ne
;;     csel      dst_lo, hi_rshift, maybe_lo, ne
(decl lower_ushr128 (ValueRegs Reg) ValueRegs)
(rule (lower_ushr128 src amt)
      (let ((src_lo Reg (value_regs_get src 0))
            (src_hi Reg (value_regs_get src 1))
            (lo_rshift Reg (lsr $I64 src_lo amt))
            (hi_rshift Reg (lsr $I64 src_hi amt))
            (inv_amt Reg (orr_not $I32 (zero_reg) amt))
            (hi_lshift Reg (lsl $I64 (lsl_imm $I64 src_hi 1) inv_amt))
            (maybe_lo Reg (orr $I64 lo_rshift hi_lshift)))
        (with_flags
         (tst_imm $I64 amt 64)
         (consumes_flags_concat
          (csel (IntCC.Ne) hi_rshift maybe_lo)
          (csel (IntCC.Ne) (zero_reg) hi_rshift)))))

(rule (lower (has_type $I128 (ushr x y)))
      (lower_ushr128 x (value_regs_get y 0)))

;; ushr: Unsigned shift right (register form)
(rule (lower (ushr ty x y))
      (aarch64_lsr_rr ty x y))

;; ushr: Unsigned shift right (immediate form)
(rule (lower (ushr ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_lsr_imm ty x amt))

;; ushr: Vector unsigned shift right by immediate (ty_vec128)
(rule -3 (lower (has_type (ty_vec128 ty) (ushr x (iconst (u64_from_imm64 n)))))
      (ushr_vec_imm x (shift_masked_imm ty n) (vector_size ty)))

;; ushr: Vector shift-by-zero optimization (returns input unchanged)
(rule -2 (lower (has_type (ty_vec128 ty) (ushr x (iconst (u64_from_imm64 n)))))
      (if-let 0 (shift_masked_imm ty n))
      x)

;; sshr: I128 signed right shift using register pairs
;;     lsr       lo_rshift, src_lo, amt
;;     asr       hi_rshift, src_hi, amt
;;     mvn       inv_amt, amt
;;     lsl       hi_lshift, src_hi, #1
;;     lsl       hi_lshift, hi_lshift, inv_amt
;;     asr       hi_sign, src_hi, #63
;;     orr       maybe_lo, lo_rshift, hi_lshift
;;     tst       amt, #0x40
;;     csel      dst_hi, hi_sign, hi_rshift, ne
;;     csel      dst_lo, hi_rshift, maybe_lo, ne
(decl lower_sshr128 (ValueRegs Reg) ValueRegs)
(rule (lower_sshr128 src amt)
      (let ((src_lo Reg (value_regs_get src 0))
            (src_hi Reg (value_regs_get src 1))
            (lo_rshift Reg (lsr $I64 src_lo amt))
            (hi_rshift Reg (asr $I64 src_hi amt))
            (inv_amt Reg (orr_not $I32 (zero_reg) amt))
            (hi_lshift Reg (lsl $I64 (lsl_imm $I64 src_hi 1) inv_amt))
            (hi_sign Reg (aarch64_asr_imm $I64 src_hi 63))
            (maybe_lo Reg (orr $I64 lo_rshift hi_lshift)))
        (with_flags
         (tst_imm $I64 amt 64)
         (consumes_flags_concat
          (csel (IntCC.Ne) hi_rshift maybe_lo)
          (csel (IntCC.Ne) hi_sign hi_rshift)))))

(rule (lower (has_type $I128 (sshr x y)))
      (lower_sshr128 x (value_regs_get y 0)))

;; sshr: Signed shift right (register form)
(rule (lower (sshr ty x y))
      (aarch64_asr_rr ty x y))

;; sshr: Signed shift right (immediate form)
(rule (lower (sshr ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_asr_imm ty x amt))

;; sshr: Vector signed shift right by immediate (ty_vec128)
(rule -2 (lower (has_type (ty_vec128 ty) (sshr x (iconst (u64_from_imm64 n)))))
      (sshr_vec_imm x (shift_masked_imm ty n) (vector_size ty)))

;; sshr: Vector shift-by-zero optimization (returns input unchanged)
(rule -1 (lower (has_type (ty_vec128 ty) (sshr x (iconst (u64_from_imm64 n)))))
      (if-let 0 (shift_masked_imm ty n))
      x)

;; rotr: Rotate right (register form)
(rule (lower (rotr ty x y))
      (aarch64_ror_rr ty x y))

;; rotr: Rotate right (immediate form)
(rule (lower (rotr ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_ror_imm ty x amt))

;; rotr: I128 rotate right (rotateright(x, amt) = (x >> amt) | (x << (128 - amt)))
(rule (lower (has_type $I128 (rotr x y)))
      (let ((val ValueRegs x)
            (amt Reg (value_regs_get y 0))
            (const_128 Reg (iconst_to_reg $I64 128))
            (neg_amt Reg (sub $I64 const_128 amt))
            (rshift ValueRegs (lower_ushr128 val amt))
            (lshift ValueRegs (lower_shl128 val neg_amt))
            (hi Reg (orr $I64 (value_regs_get rshift 1) (value_regs_get lshift 1)))
            (lo Reg (orr $I64 (value_regs_get rshift 0) (value_regs_get lshift 0))))
        (value_regs lo hi)))

;; rotl: Rotate left (register form) - implemented as rotr(x, bits - y)
(rule (lower (rotl ty x y))
      (aarch64_rotl_rr ty x y))

;; rotl: Rotate left (immediate form) - implemented as rotr(x, bits - amount)
(rule (lower (rotl ty x (iconst k)))
      (if-let amt (valid_rotl_imm ty k))
      (aarch64_ror_imm ty x amt))
;; rotl(x, k) = rotr(x, width - k)

;; rotl: I128 rotate left (rotateleft(x, amt) = (x << amt) | (x >> (128 - amt)))
(rule (lower (has_type $I128 (rotl x y)))
      (let ((val ValueRegs x)
            (amt Reg (value_regs_get y 0))
            (const_128 Reg (iconst_to_reg $I64 128))
            (neg_amt Reg (sub $I64 const_128 amt))
            (lshift ValueRegs (lower_shl128 val amt))
            (rshift ValueRegs (lower_ushr128 val neg_amt)))
        (value_regs
          (orr $I64 (value_regs_get lshift 0) (value_regs_get rshift 0))
          (orr $I64 (value_regs_get lshift 1) (value_regs_get rshift 1)))))

;; smin: Signed minimum (CMP + CSEL)
(rule (lower (smin ty x y))
      (aarch64_smin ty x y))

;; umin: Unsigned minimum (CMP + CSEL)
(rule (lower (umin ty x y))
      (aarch64_umin ty x y))

;; smax: Signed maximum (CMP + CSEL)
(rule (lower (smax ty x y))
      (aarch64_smax ty x y))

;; umax: Unsigned maximum (CMP + CSEL)
(rule (lower (umax ty x y))
      (aarch64_umax ty x y))

;; sadd_sat: Signed saturating add (SQADD)
(rule (lower (has_type $I8 (sadd_sat x y)))
      (aarch64_sqadd_8 x y))

(rule (lower (has_type $I16 (sadd_sat x y)))
      (aarch64_sqadd_16 x y))

(rule (lower (has_type $I32 (sadd_sat x y)))
      (aarch64_sqadd_32 x y))

(rule (lower (has_type $I64 (sadd_sat x y)))
      (aarch64_sqadd_64 x y))

;; ssub_sat: Signed saturating subtract (SQSUB)
(rule (lower (has_type $I8 (ssub_sat x y)))
      (aarch64_sqsub_8 x y))

(rule (lower (has_type $I16 (ssub_sat x y)))
      (aarch64_sqsub_16 x y))

(rule (lower (has_type $I32 (ssub_sat x y)))
      (aarch64_sqsub_32 x y))

(rule (lower (has_type $I64 (ssub_sat x y)))
      (aarch64_sqsub_64 x y))

;; uadd_sat: Unsigned saturating add (UQADD)
(rule (lower (has_type $I8 (uadd_sat x y)))
      (aarch64_uqadd_8 x y))

(rule (lower (has_type $I16 (uadd_sat x y)))
      (aarch64_uqadd_16 x y))

(rule (lower (has_type $I32 (uadd_sat x y)))
      (aarch64_uqadd_32 x y))

(rule (lower (has_type $I64 (uadd_sat x y)))
      (aarch64_uqadd_64 x y))

;; usub_sat: Unsigned saturating subtract (UQSUB)
(rule (lower (has_type $I8 (usub_sat x y)))
      (aarch64_uqsub_8 x y))

(rule (lower (has_type $I16 (usub_sat x y)))
      (aarch64_uqsub_16 x y))

(rule (lower (has_type $I32 (usub_sat x y)))
      (aarch64_uqsub_32 x y))

(rule (lower (has_type $I64 (usub_sat x y)))
      (aarch64_uqsub_64 x y))

;; bitselect: Bitwise select ((x & c) | (y & ~c))
(rule 1 (lower (has_type (ty_vec128 _) (bitselect c x y)))
      (aarch64_bitselect c x y))

(rule (lower (bitselect c x y))
      (aarch64_bitselect c x y))

;; iabs: Integer absolute value (CMP + NEG + CSEL)
(rule (lower (iabs ty x))
      (aarch64_iabs ty x))

;; Comparison operations lowering

;; icmp: Integer comparison with Equal condition
(rule (lower (icmp IntCC.Equal ty x y))
      (aarch64_cmp_rr ty x y IntCC.Equal))

;; icmp: Integer comparison with NotEqual condition
(rule (lower (icmp IntCC.NotEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.NotEqual))

;; icmp: Integer comparison with SignedLessThan condition
(rule (lower (icmp IntCC.SignedLessThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedLessThan))

;; icmp: Integer comparison with SignedGreaterThanOrEqual condition
(rule (lower (icmp IntCC.SignedGreaterThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedGreaterThanOrEqual))

;; icmp: Integer comparison with SignedGreaterThan condition
(rule (lower (icmp IntCC.SignedGreaterThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedGreaterThan))

;; icmp: Integer comparison with SignedLessThanOrEqual condition
(rule (lower (icmp IntCC.SignedLessThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedLessThanOrEqual))

;; icmp: Integer comparison with UnsignedLessThan condition
(rule (lower (icmp IntCC.UnsignedLessThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedLessThan))

;; icmp: Integer comparison with UnsignedGreaterThanOrEqual condition
(rule (lower (icmp IntCC.UnsignedGreaterThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedGreaterThanOrEqual))

;; icmp: Integer comparison with UnsignedGreaterThan condition
(rule (lower (icmp IntCC.UnsignedGreaterThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedGreaterThan))

;; icmp: Integer comparison with UnsignedLessThanOrEqual condition
(rule (lower (icmp IntCC.UnsignedLessThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedLessThanOrEqual))

;; icmp with immediate form (12-bit unsigned immediate)
(rule (lower (icmp cc ty x (iconst k)))
      (if-let $true (uimm12 k))
      (aarch64_cmp_imm ty x k cc))

;; Bitwise operations lowering

;; band: Bitwise AND (register form)

;; Vector AND (64-bit vectors)
(rule -2 (lower (has_type ty @ (ty_vec64 _) (band x y)))
      (and_vec x y (vector_size ty)))

;; Vector AND (128-bit vectors)
(rule -1 (lower (has_type ty @ (ty_vec128 _) (band x y)))
      (and_vec x y (vector_size ty)))

;; band: I128 bitwise AND (operate on low/high parts independently)
(rule (lower (has_type $I128 (band x y)))
      (let ((x_regs ValueRegs x)
            (x_lo Reg (value_regs_get x_regs 0))
            (x_hi Reg (value_regs_get x_regs 1))
            (y_regs ValueRegs y)
            (y_lo Reg (value_regs_get y_regs 0))
            (y_hi Reg (value_regs_get y_regs 1)))
        (value_regs
          (aarch64_and_rr $I64 x_lo y_lo)
          (aarch64_and_rr $I64 x_hi y_hi))))

;; Scalar AND
(rule (lower (band ty x y))
      (aarch64_and_rr ty x y))

;; band: Bitwise AND (immediate form with logical immediate)
(rule 1 (lower (band ty x (iconst k)))
      (if-let imm (imm_logic_from_u64 ty k))
      (aarch64_and_imm ty x imm))

;; band: Fusion with bnot -> BIC instruction

;; Vector BIC (64-bit vectors)
(rule 5 (lower (has_type ty @ (ty_vec64 _) (band x (bnot y))))
      (bic_vec x y (vector_size ty)))

(rule 6 (lower (has_type ty @ (ty_vec64 _) (band (bnot y) x)))
      (bic_vec x y (vector_size ty)))

;; Vector BIC (128-bit vectors)
(rule 7 (lower (has_type ty @ (ty_vec128 _) (band x (bnot y))))
      (bic_vec x y (vector_size ty)))

(rule 8 (lower (has_type ty @ (ty_vec128 _) (band (bnot y) x)))
      (bic_vec x y (vector_size ty)))

;; Scalar BIC
(rule 1 (lower (has_type ty @ (fits_in_64 _) (band x (bnot _ y))))
      (aarch64_bic_rr ty x y))

(rule 2 (lower (has_type ty @ (fits_in_64 _) (band (bnot _ x) y)))
      (aarch64_bic_rr ty y x))

;; bor: Bitwise OR

;; Vector OR (64-bit vectors)
(rule -2 (lower (has_type ty @ (ty_vec64 _) (bor x y)))
      (or_vec x y (vector_size ty)))

;; Vector OR (128-bit vectors)
(rule -1 (lower (has_type ty @ (ty_vec128 _) (bor x y)))
      (or_vec x y (vector_size ty)))

;; bor: I128 bitwise OR (operate on low/high parts independently)
(rule (lower (has_type $I128 (bor x y)))
      (let ((x_regs ValueRegs x)
            (x_lo Reg (value_regs_get x_regs 0))
            (x_hi Reg (value_regs_get x_regs 1))
            (y_regs ValueRegs y)
            (y_lo Reg (value_regs_get y_regs 0))
            (y_hi Reg (value_regs_get y_regs 1)))
        (value_regs
          (aarch64_orr_rr $I64 x_lo y_lo)
          (aarch64_orr_rr $I64 x_hi y_hi))))

;; bor: Bitwise OR (register form)
(rule (lower (bor ty x y))
      (aarch64_orr_rr ty x y))

;; bor: Bitwise OR (immediate form with logical immediate)
(rule 1 (lower (bor ty x (iconst k)))
      (if-let imm (imm_logic_from_u64 ty k))
      (aarch64_orr_imm ty x imm))

;; bor: Fusion with bnot -> ORN instruction
;; bor(x, bnot(y)) -> bor_not(x, y) -> ORN
(rule 2 (lower (has_type ty @ (fits_in_64 _) (bor x (bnot _ y))))
      (aarch64_orn_rr ty x y))

;; bor: Fusion with bnot (commuted) -> ORN instruction
;; bor(bnot(x), y) -> bor_not(y, x) -> ORN
(rule 3 (lower (has_type ty @ (fits_in_64 _) (bor (bnot _ x) y)))
      (aarch64_orn_rr ty y x))

;; bxor: Bitwise XOR

;; Vector XOR (64-bit vectors)
(rule -2 (lower (has_type ty @ (ty_vec64 _) (bxor x y)))
      (xor_vec x y (vector_size ty)))

;; Vector XOR (128-bit vectors)
(rule -1 (lower (has_type ty @ (ty_vec128 _) (bxor x y)))
      (xor_vec x y (vector_size ty)))

;; bxor: I128 bitwise XOR (operate on low/high parts independently)
(rule (lower (has_type $I128 (bxor x y)))
      (let ((x_regs ValueRegs x)
            (x_lo Reg (value_regs_get x_regs 0))
            (x_hi Reg (value_regs_get x_regs 1))
            (y_regs ValueRegs y)
            (y_lo Reg (value_regs_get y_regs 0))
            (y_hi Reg (value_regs_get y_regs 1)))
        (value_regs
          (aarch64_eor_rr $I64 x_lo y_lo)
          (aarch64_eor_rr $I64 x_hi y_hi))))

;; Scalar XOR
(rule (lower (bxor ty x y))
      (aarch64_eor_rr ty x y))

;; bxor: Bitwise XOR (immediate form with logical immediate)
(rule 1 (lower (bxor ty x (iconst k)))
      (if-let imm (imm_logic_from_u64 ty k))
      (aarch64_eor_imm ty x imm))

;; bxor: Fusion with bnot -> EON instruction
;; bxor(x, bnot(y)) -> bxor_not(x, y) -> EON
(rule 2 (lower (has_type ty @ (fits_in_64 _) (bxor x (bnot _ y))))
      (aarch64_eon_rr ty x y))

;; bxor: Fusion with bnot (commuted) -> EON instruction
;; bxor(bnot(x), y) -> bxor_not(y, x) -> EON
(rule 3 (lower (has_type ty @ (fits_in_64 _) (bxor (bnot _ x) y)))
      (aarch64_eon_rr ty y x))

;; bnot: Bitwise NOT

;; Vector NOT (64-bit vectors)
(rule -2 (lower (has_type ty @ (ty_vec64 _) (bnot x)))
      (not_vec x (vector_size ty)))

;; Vector NOT (128-bit vectors)
(rule -1 (lower (has_type ty @ (ty_vec128 _) (bnot x)))
      (not_vec x (vector_size ty)))

;; bnot: I128 bitwise NOT (operate on low/high parts independently)
(rule (lower (has_type $I128 (bnot x)))
      (let ((x_regs ValueRegs x)
            (x_lo Reg (value_regs_get x_regs 0))
            (x_hi Reg (value_regs_get x_regs 1)))
        (value_regs
          (aarch64_mvn_rr $I64 x_lo)
          (aarch64_mvn_rr $I64 x_hi))))

;; Scalar NOT (implemented as MVN)
(rule (lower (bnot ty x))
      (aarch64_mvn_rr ty x))

;; band_not: Bitwise AND NOT (BIC)
(rule (lower (band_not ty x y))
      (aarch64_bic_rr ty x y))

;; bor_not: Bitwise OR NOT (ORN)
(rule (lower (bor_not ty x y))
      (aarch64_orn_rr ty x y))

;; bxor_not: Bitwise XOR NOT (EON)
(rule (lower (bxor_not ty x y))
      (aarch64_eon_rr ty x y))

;; Bit manipulation operations

;; clz: Count leading zeros (CLZ)
(rule (lower (has_type $I32 (clz x)))
      (aarch64_clz_32 x))

(rule (lower (has_type $I64 (clz x)))
      (aarch64_clz_64 x))

;; cls: Count leading sign bits (CLS)
(rule (lower (has_type $I32 (cls x)))
      (aarch64_cls_32 x))

(rule (lower (has_type $I64 (cls x)))
      (aarch64_cls_64 x))

;; ctz: Count trailing zeros (RBIT + CLZ)
;; ARM64 doesn't have CTZ, so we reverse bits then count leading zeros
(rule (lower (has_type $I32 (ctz x)))
      (aarch64_ctz_32 x))

(rule (lower (has_type $I64 (ctz x)))
      (aarch64_ctz_64 x))

;; bitrev: Reverse bits (RBIT)
(rule (lower (has_type $I32 (bitrev x)))
      (aarch64_rbit_32 x))

(rule (lower (has_type $I64 (bitrev x)))
      (aarch64_rbit_64 x))

;; bitrev I128: Reverse bits in each half, then swap order
(rule (lower (has_type $I128 (bitrev x)))
      (let ((lo Reg (value_regs_get x 0))
            (hi Reg (value_regs_get x 1))
            (lo_rev Reg (aarch64_rbit_64 lo))
            (hi_rev Reg (aarch64_rbit_64 hi)))
        (value_regs hi_rev lo_rev)))

;; bswap: Byte swap (REV16, REV32, REV64)
(rule (lower (has_type $I16 (bswap x)))
      (aarch64_bswap_16 x))

(rule (lower (has_type $I32 (bswap x)))
      (aarch64_bswap_32 x))

(rule (lower (has_type $I64 (bswap x)))
      (aarch64_bswap_64 x))

;; bswap I128: Reverse bytes in each half, then swap order
(rule (lower (has_type $I128 (bswap x)))
      (value_regs
       (aarch64_bswap_64 (value_regs_get x 1))
       (aarch64_bswap_64 (value_regs_get x 0))))

;; popcnt: Population count (count set bits)
(rule (lower (has_type $I8 (popcnt x)))
      (aarch64_popcnt_8 x))

(rule (lower (has_type $I16 (popcnt x)))
      (aarch64_popcnt_16 x))

(rule (lower (has_type $I32 (popcnt x)))
      (aarch64_popcnt_32 x))

(rule (lower (has_type $I64 (popcnt x)))
      (aarch64_popcnt_64 x))


;; bswap I128: Reverse bytes in each half, then swap order
(rule (lower (has_type $I128 (bswap x)))
      (value_regs
       (aarch64_bswap_64 (value_regs_get x 1))
       (aarch64_bswap_64 (value_regs_get x 0))))

;; clz I128: Count leading zeros across both registers
;; Uses lower_clz128 helper from isle_helpers.zig
(rule (lower (has_type $I128 (clz x)))
      (lower_clz128 x))

;; ctz I128: Count trailing zeros by reversing bits then counting leading zeros
(rule (lower (has_type $I128 (ctz x)))
      (let ((lo Reg (value_regs_get x 0))
            (hi Reg (value_regs_get x 1))
            (lo_rev Reg (aarch64_rbit_64 lo))
            (hi_rev Reg (aarch64_rbit_64 hi)))
        (lower_clz128 (value_regs hi_rev lo_rev))))

;; cls I128: Count leading sign bits
;; Uses lower_cls128 helper from isle_helpers.zig
(rule (lower (has_type $I128 (cls x)))
      (lower_cls128 x))

;; popcnt I128: Count set bits across both registers
;; Uses lower_popcnt128 helper from isle_helpers.zig
(rule (lower (has_type $I128 (popcnt x)))
      (lower_popcnt128 x))
;; ineg: Integer negation

;; Vector negation (128-bit vectors)
(rule -1 (lower (has_type ty @ (ty_vec128 _) (ineg x)))
      (neg_vec x (vector_size ty)))

;; ineg: I128 negation using subtraction from zero (0 - x)
;; Priority 2 to match before generic ineg
(rule 2 (lower (has_type $I128 (ineg x)))
      (let ((zero_lo Reg (iconst_to_reg $I64 0))
            (zero_hi Reg (iconst_to_reg $I64 0))
            (zero_regs ValueRegs (value_regs zero_lo zero_hi))
            (x_regs ValueRegs x)
            (x_lo Reg (value_regs_get x_regs 0))
            (x_hi Reg (value_regs_get x_regs 1)))
        ;; 0 - x using SUBS + SBC
        (with_flags
          (sub_with_flags_paired $I64 zero_lo x_lo)
          (sbc_paired $I64 zero_hi x_hi))))

;; Scalar negation
(rule (lower (ineg ty x))
      (aarch64_neg ty x))

;; iabs: I128 absolute value
;; Algorithm: sign = x_hi >> 63 (arithmetic); result = (x ^ sign) - sign
(rule (lower (has_type $I128 (iabs x)))
      (let ((x_regs ValueRegs x)
            (x_lo Reg (value_regs_get x_regs 0))
            (x_hi Reg (value_regs_get x_regs 1))
            ;; Extract sign bit: arithmetic shift right by 63
            (asr_reg Reg (aarch64_asr_imm $I64 x_hi 63))
            ;; XOR both parts with sign
            (eor_hi Reg (aarch64_eor_rr $I64 x_hi asr_reg))
            (eor_lo Reg (aarch64_eor_rr $I64 x_lo asr_reg)))
        ;; Subtract sign from XOR result: (x ^ sign) - sign
        (with_flags
          (sub_with_flags_paired $I64 eor_lo asr_reg)
          (sbc_paired $I64 eor_hi asr_reg))))

;; iabs: Vector integer absolute value
(rule -1 (lower (has_type ty @ (multi_lane _ _) (iabs x)))
      (vec_misc (VecMisc2.Abs) x (vector_size ty)))

;; sqmul_round_sat: Signed saturating rounding doubling multiply high
(rule (lower (has_type ty @ (multi_lane _ _) (sqmul_round_sat x y)))
      (vec_rrr (VecALUOp.Sqrdmulh) x y (vector_size ty)))

;; iabs: Integer absolute value
;; Implemented as (x XOR sign) - sign, where sign = x >> (bits-1)
(rule (lower (has_type $I8 (iabs x)))
      (aarch64_iabs_8 x))

(rule (lower (has_type $I16 (iabs x)))
      (aarch64_iabs_16 x))

(rule (lower (has_type $I32 (iabs x)))
      (aarch64_iabs_32 x))

(rule (lower (has_type $I64 (iabs x)))
      (aarch64_iabs_64 x))

;; Conditional select lowering

;; select: Conditional select based on comparison result
;; select(cond, true_val, false_val) -> true_val if cond != 0, else false_val

;; Type-specific select rules for boolean conditions (non-icmp)
;; These handle select with a boolean value as condition

;; I8 condition: Test with immediate mask (check if non-zero)
(rule -1 (lower (has_type ty (select rcond @ (value_type $I8) rn rm)))
      (lower_select
        (tst_imm $I32 rcond 255)
        (IntCC.Ne)
        ty
        rn
        rm))

;; I32 condition: Zero-extend to 32-bit and compare
(rule -2 (lower (has_type ty (select rcond @ (value_type (fits_in_32 _)) rn rm)))
      (let ((rcond_reg Reg (put_in_reg_zext32 rcond)))
       (lower_select
        (cmp $I32 rcond_reg (zero_reg))
        (IntCC.Ne)
        ty
        rn
        rm)))

;; I64 condition: Zero-extend to 64-bit and compare
(rule -3 (lower (has_type ty (select rcond @ (value_type (fits_in_64 _)) rn rm)))
      (let ((rcond_reg Reg (put_in_reg_zext64 rcond)))
       (lower_select
        (cmp $I64 rcond_reg (zero_reg))
        (IntCC.Ne)
        ty
        rn
        rm)))

;; I128 condition: OR lo and hi parts, then compare
(rule -4 (lower (has_type ty (select rcond @ (value_type $I128) rn rm)))
      (let ((c ValueRegs (put_in_regs rcond))
            (c_lo Reg (value_regs_get c 0))
            (c_hi Reg (value_regs_get c 1))
            (rt Reg (orr $I64 c_lo c_hi)))
       (lower_select
        (cmp $I64 rt (zero_reg))
        (IntCC.Ne)
        ty
        rn
        rm)))

;; Optimized: select with icmp condition (inline comparison)
(rule (lower (select ty (icmp cc cmp_ty x y) true_val false_val))
      (let ((cmp_inst Aarch64Inst (aarch64_cmp_rr cmp_ty x y cc)))
            (aarch64_csel ty true_val false_val cmp_inst cc)))

;;;; Rules for `select_spectre_guard` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;; I64 condition with Spectre mitigation
(rule -1 (lower (has_type ty (select_spectre_guard rcond @ (value_type (fits_in_64 _)) rn rm)))
      (let ((rcond_reg Reg (put_in_reg_zext64 rcond)))
       (lower_select
        (cmp $I64 rcond_reg (zero_reg))
        (IntCC.Ne)
        ty
        rn
        rm)))

;; I128 condition with Spectre mitigation: OR lo and hi parts, then compare
(rule -2 (lower (has_type ty (select_spectre_guard rcond @ (value_type $I128) rn rm)))
      (let ((c ValueRegs (put_in_regs rcond))
            (c_lo Reg (value_regs_get c 0))
            (c_hi Reg (value_regs_get c 1))
            (rt Reg (orr $I64 c_lo c_hi)))
       (lower_select
        (cmp $I64 rt (zero_reg))
        (IntCC.Ne)
        ty
        rn
        rm)))

;; Immediate forms

;; iadd with 12-bit immediate (priority 1 - higher than generic, lower than extensions/fusions)
(rule 1 (lower (iadd ty x (iconst k)))
      (if-let $true (uimm12 k))
      (aarch64_add_imm ty x k))

;; iadd with commuted immediate (iconst on left)
(rule 1 (lower (iadd ty (iconst k) x))
      (if-let $true (uimm12 k))
      (aarch64_add_imm ty x k))

;; isub with negated 12-bit immediate optimization
;; Lower (isub x iconst) as (iadd x -iconst) when -imm fits in 12-bit
;; This provides more flexibility in immediate selection
(rule (lower (isub ty x (iconst k)))
      (if-let $true (in_neg_uimm12_range k))
      (aarch64_add_imm ty x (negate_i64 k)))

;; isub with 12-bit immediate
(rule (lower (isub ty x (iconst k)))
      (if-let $true (uimm12 k))
      (aarch64_sub_imm ty x k))

;; Load/store lowering

;; load: Load from memory with register+immediate offset
;; Pattern: load(iadd(base, iconst(offset))) => LDR Xt, [Xn, #offset]
(rule (lower (load ty (iadd ptr_ty base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_ldr_imm_offset ty offset))
      (aarch64_ldr_imm ty base scaled_offset))

;; load: Load from memory with commuted register+immediate offset
(rule (lower (load ty (iadd ptr_ty (iconst offset) base) _flags _mem_offset))
      (if-let scaled_offset (valid_ldr_imm_offset ty offset))
      (aarch64_ldr_imm ty base scaled_offset))

;; load: Load from memory with register+register offset
;; Pattern: load(iadd(base, index)) => LDR Xt, [Xn, Xm]
(rule (lower (load ty (iadd ptr_ty base index) _flags _mem_offset))
      (aarch64_ldr_reg ty base index))

;; load: Load from memory with register+extended register offset
;; Pattern: load(iadd(base, sxtw(index))) => LDR Xt, [Xn, Wm, SXTW]
(rule (lower (load ty (iadd ptr_ty base (aarch64_sxtw index)) _flags _mem_offset))
      (aarch64_ldr_ext ty base index ExtendOp.sxtw))

;; load: Load from memory with commuted register+extended register offset
(rule (lower (load ty (iadd ptr_ty (aarch64_sxtw index) base) _flags _mem_offset))
      (aarch64_ldr_ext ty base index ExtendOp.sxtw))

;; load: Load from memory with register+shifted register offset
;; Pattern: load(iadd(base, ishl(index, iconst(shift)))) => LDR Xt, [Xn, Xm, LSL #shift]
(rule (lower (load ty (iadd ptr_ty base (ishl idx_ty index (iconst shift))) _flags _mem_offset))
      (if-let shift_amt (valid_ldr_shift ty shift))
      (aarch64_ldr_shifted ty base index shift_amt))

;; load: Load from memory with commuted register+shifted register offset
(rule (lower (load ty (iadd ptr_ty (ishl idx_ty index (iconst shift)) base) _flags _mem_offset))
      (if-let shift_amt (valid_ldr_shift ty shift))
      (aarch64_ldr_shifted ty base index shift_amt))

;; Extending loads: Use specialized load instructions for narrow types
;; These have higher priority than the generic load to use LDRB/LDRH/LDRSB/LDRSH/LDRSW

;; load I8: Use LDRB (zero-extending load byte)
(rule 1 (lower (load $I8 addr _flags _offset))
      (aarch64_uload8 addr))

;; load I16: Use LDRH (zero-extending load halfword)
(rule 1 (lower (load $I16 addr _flags _offset))
      (aarch64_uload16 addr))

;; load I32: Use LDR Wd (zero-extending load word)
(rule 1 (lower (load $I32 addr _flags _offset))
      (aarch64_uload32 addr))

;; load I64: Use LDR Xd (load doubleword)
(rule 1 (lower (load $I64 addr _flags _offset))
      (aarch64_uload64 addr))

;; load: Load from memory (base register only) - fallback
(rule (lower (load ty addr _flags _offset))
      (aarch64_ldr ty addr))

;; store: Store to memory with register+immediate offset
;; Pattern: store(val, iadd(base, iconst(offset))) => STR Xt, [Xn, #offset]
(rule (lower (store val (iadd ptr_ty base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_str_imm_offset val offset))
      (aarch64_str_imm val base scaled_offset))

;; store: Store to memory with commuted register+immediate offset
(rule (lower (store val (iadd ptr_ty (iconst offset) base) _flags _mem_offset))
      (if-let scaled_offset (valid_str_imm_offset val offset))
      (aarch64_str_imm val base scaled_offset))

;; store: Store to memory with register+register offset
;; Pattern: store(val, iadd(base, index)) => STR Xt, [Xn, Xm]
(rule (lower (store val (iadd ptr_ty base index) _flags _mem_offset))
      (aarch64_str_reg val base index))

;; store: Store to memory with register+extended register offset
;; Pattern: store(val, iadd(base, sxtw(index))) => STR Xt, [Xn, Wm, SXTW]
(rule (lower (store val (iadd ptr_ty base (aarch64_sxtw index)) _flags _mem_offset))
      (aarch64_str_ext val base index ExtendOp.sxtw))

;; store: Store to memory with commuted register+extended register offset
(rule (lower (store val (iadd ptr_ty (aarch64_sxtw index) base) _flags _mem_offset))
      (aarch64_str_ext val base index ExtendOp.sxtw))

;; store: Store to memory with register+shifted register offset
;; Pattern: store(val, iadd(base, ishl(index, iconst(shift)))) => STR Xt, [Xn, Xm, LSL #shift]
(rule (lower (store val (iadd ptr_ty base (ishl idx_ty index (iconst shift))) _flags _mem_offset))
      (if-let shift_amt (valid_str_shift val shift))
      (aarch64_str_shifted val base index shift_amt))

;; store: Store to memory with commuted register+shifted register offset
(rule (lower (store val (iadd ptr_ty (ishl idx_ty index (iconst shift)) base) _flags _mem_offset))
      (if-let shift_amt (valid_str_shift val shift))
      (aarch64_str_shifted val base index shift_amt))

;; store: Store to memory (base register only)
(rule (lower (store val addr _flags _offset))
      (aarch64_str val addr))

;; Pre-index addressing modes

;; load with pre-index: base+=offset, then load
;; Pattern: load(pre_inc(base, offset)) => LDR Xt, [Xn, #offset]!
(rule (lower (load ty (pre_inc base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_ldr_imm_offset ty offset))
      (aarch64_ldr_pre ty base scaled_offset))

;; store with pre-index: base+=offset, then store
;; Pattern: store(val, pre_inc(base, offset)) => STR Xt, [Xn, #offset]!
(rule (lower (store val (pre_inc base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_str_imm_offset val offset))
      (aarch64_str_pre val base scaled_offset))

;; Post-index addressing modes

;; load with post-index: load, then base+=offset
;; Pattern: load(post_inc(base, offset)) => LDR Xt, [Xn], #offset
(rule (lower (load ty (post_inc base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_ldr_imm_offset ty offset))
      (aarch64_ldr_post ty base scaled_offset))

;; store with post-index: store, then base+=offset
;; Pattern: store(val, post_inc(base, offset)) => STR Xt, [Xn], #offset
(rule (lower (store val (post_inc base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_str_imm_offset val offset))
      (aarch64_str_post val base scaled_offset))

;; Load pair optimization (LDP)
;; Pattern: two adjacent loads with 8-byte offset
;; load(base+0) and load(base+8) => LDP X0, X1, [base]
(rule (lower (load_pair ty base offset1 offset2))
      (if-let $true (is_ldp_valid_offset offset1 offset2))
      (aarch64_ldp ty base offset1))

;; Store pair optimization (STP)
;; Pattern: two adjacent stores with 8-byte offset
;; store(val1, base+0) and store(val2, base+8) => STP X0, X1, [base]
(rule (lower (store_pair val1 val2 base offset1 offset2))
      (if-let $true (is_stp_valid_offset offset1 offset2))
      (aarch64_stp val1 val2 base offset1))

;; Control flow lowering

;; jump: Unconditional branch
(rule (lower (jump target))
      (aarch64_b target))

;; brif: Conditional branch
(rule (lower (brif cond target))
      (aarch64_b_cond cond target))

;; cbz: Compare-and-branch-zero optimization
;; Pattern: if (x == 0) goto target
(rule (lower (brif (icmp eq x (iconst 0)) target))
      (aarch64_cbz x target))

;; cbnz: Compare-and-branch-nonzero optimization
;; Pattern: if (x != 0) goto target
(rule (lower (brif (icmp ne x (iconst 0)) target))
      (aarch64_cbnz x target))

;; tbz: Test-bit-and-branch-zero optimization
;; Pattern: if ((x & (1 << bit)) == 0) goto target
(rule (lower (brif (icmp eq (band x (ishl (iconst 1) bit)) (iconst 0)) target))
      (aarch64_tbz x bit target))

;; tbnz: Test-bit-and-branch-nonzero optimization
;; Pattern: if ((x & (1 << bit)) != 0) goto target
(rule (lower (brif (icmp ne (band x (ishl (iconst 1) bit)) (iconst 0)) target))
      (aarch64_tbnz x bit target))

;; CMP+branch fusion for general comparisons (higher priority than materialization)
;; Pattern: brif(icmp(cc, x, y), target) => CMP x, y; B.cc target
(rule 3 (lower (brif (icmp cc ty x y) target))
      (aarch64_cmp_and_branch ty x y cc target))

;; CMP+branch fusion with immediate (higher priority than materialization)
;; Pattern: brif(icmp(cc, x, imm), target) => CMP x, #imm; B.cc target
(rule 3 (lower (brif (icmp cc ty x (iconst k)) target))
      (if-let $true (uimm12 k))
      (aarch64_cmp_imm_and_branch ty x k cc target))

;; return: Return from function
(rule (lower (return))
      (aarch64_ret))

;; Floating-point conversion lowering

;; fcvt_to_uint: Convert float to unsigned integer (FCVTZU)
(rule (lower (fcvt_to_uint (value_type $F32) x (value_type $I32)))
      (aarch64_fcvtzu_32 x))

(rule (lower (fcvt_to_uint (value_type $F32) x (value_type $I64)))
      (aarch64_fcvtzu_64 x))

(rule (lower (fcvt_to_uint (value_type $F64) x (value_type $I32)))
      (aarch64_fcvtzu_32 x))

(rule (lower (fcvt_to_uint (value_type $F64) x (value_type $I64)))
      (aarch64_fcvtzu_64 x))

;; fcvt_to_sint: Convert float to signed integer (FCVTZS)
(rule (lower (fcvt_to_sint (value_type $F32) x (value_type $I32)))
      (aarch64_fcvtzs_32 x))

(rule (lower (fcvt_to_sint (value_type $F32) x (value_type $I64)))
      (aarch64_fcvtzs_64 x))

(rule (lower (fcvt_to_sint (value_type $F64) x (value_type $I32)))
      (aarch64_fcvtzs_32 x))

(rule (lower (fcvt_to_sint (value_type $F64) x (value_type $I64)))
      (aarch64_fcvtzs_64 x))

;; Vector float conversions
(rule -1 (lower (has_type ty @ (multi_lane 32 _) (fcvt_from_uint x @ (value_type (multi_lane 32 _)))))
      (vec_misc (VecMisc2.Ucvtf) x (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane 64 _) (fcvt_from_uint x @ (value_type (multi_lane 64 _)))))
      (vec_misc (VecMisc2.Ucvtf) x (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane 32 _) (fcvt_from_sint x @ (value_type (multi_lane 32 _)))))
      (vec_misc (VecMisc2.Scvtf) x (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane 64 _) (fcvt_from_sint x @ (value_type (multi_lane 64 _)))))
      (vec_misc (VecMisc2.Scvtf) x (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane 32 _) (fcvt_to_uint_sat x @ (value_type (multi_lane 32 _)))))
      (vec_misc (VecMisc2.Fcvtzu) x (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane 64 _) (fcvt_to_uint_sat x @ (value_type (multi_lane 64 _)))))
      (vec_misc (VecMisc2.Fcvtzu) x (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane 32 _) (fcvt_to_sint_sat x @ (value_type (multi_lane 32 _)))))
      (vec_misc (VecMisc2.Fcvtzs) x (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane 64 _) (fcvt_to_sint_sat x @ (value_type (multi_lane 64 _)))))
      (vec_misc (VecMisc2.Fcvtzs) x (vector_size ty)))

;; fcvt_from_uint: Convert unsigned integer to float (UCVTF)
(rule (lower (fcvt_from_uint (value_type $I32) x (value_type $F32)))
      (aarch64_ucvtf_32_to_f32 x))

(rule (lower (fcvt_from_uint (value_type $I32) x (value_type $F64)))
      (aarch64_ucvtf_32_to_f64 x))

(rule (lower (fcvt_from_uint (value_type $I64) x (value_type $F32)))
      (aarch64_ucvtf_64_to_f32 x))

(rule (lower (fcvt_from_uint (value_type $I64) x (value_type $F64)))
      (aarch64_ucvtf_64_to_f64 x))

;; fcvt_from_sint: Convert signed integer to float (SCVTF)
(rule (lower (fcvt_from_sint (value_type $I32) x (value_type $F32)))
      (aarch64_scvtf_32_to_f32 x))

(rule (lower (fcvt_from_sint (value_type $I32) x (value_type $F64)))
      (aarch64_scvtf_32_to_f64 x))

(rule (lower (fcvt_from_sint (value_type $I64) x (value_type $F32)))
      (aarch64_scvtf_64_to_f32 x))

(rule (lower (fcvt_from_sint (value_type $I64) x (value_type $F64)))
      (aarch64_scvtf_64_to_f64 x))

;; fcvt_to_sint_sat: Convert float to signed integer with saturation
;; ARM64 FCVTZS natively saturates: NaN→0, overflow→INT_MAX/MIN
(rule (lower (has_type $I32 (fcvt_to_sint_sat x @ (value_type $F32))))
      (aarch64_fcvtzs_32_to_32 x))

(rule (lower (has_type $I32 (fcvt_to_sint_sat x @ (value_type $F64))))
      (aarch64_fcvtzs_64_to_32 x))

(rule (lower (has_type $I64 (fcvt_to_sint_sat x @ (value_type $F32))))
      (aarch64_fcvtzs_32_to_64 x))

(rule (lower (has_type $I64 (fcvt_to_sint_sat x @ (value_type $F64))))
      (aarch64_fcvtzs_64_to_64 x))

;; fcvt_to_uint_sat: Convert float to unsigned integer with saturation
;; ARM64 FCVTZU natively saturates: NaN→0, negative→0, overflow→UINT_MAX
(rule (lower (has_type $I32 (fcvt_to_uint_sat x @ (value_type $F32))))
      (aarch64_fcvtzu_32_to_32 x))

(rule (lower (has_type $I32 (fcvt_to_uint_sat x @ (value_type $F64))))
      (aarch64_fcvtzu_64_to_32 x))

(rule (lower (has_type $I64 (fcvt_to_uint_sat x @ (value_type $F32))))
      (aarch64_fcvtzu_32_to_64 x))

(rule (lower (has_type $I64 (fcvt_to_uint_sat x @ (value_type $F64))))
      (aarch64_fcvtzu_64_to_64 x))

;; fdemote: Convert f64 to f32 (FCVT)
(rule (lower (fdemote x))
      (aarch64_fcvt_f64_to_f32 x))

;; fpromote: Convert f32 to f64 (FCVT)
(rule (lower (fpromote x))
      (aarch64_fcvt_f32_to_f64 x))

;; Floating-point arithmetic

;; fadd: FP addition (FADD)
(rule (lower (fadd ty x y))
      (aarch64_fadd ty x y))

;; fsub: FP subtraction (FSUB)
(rule (lower (fsub ty x y))
      (aarch64_fsub ty x y))

;; fmul: FP multiplication (FMUL)
(rule (lower (fmul ty x y))
      (aarch64_fmul ty x y))

;; fdiv: FP division (FDIV)
(rule (lower (fdiv ty x y))
      (aarch64_fdiv ty x y))

;; fsqrt: FP square root (FSQRT)
(rule (lower (fsqrt ty x))
      (aarch64_fsqrt ty x))

;; fma: FP fused multiply-add (FMADD)
;; fma(x, y, z) = x * y + z

;; Vector FMA - delegate to lower_fmla helper (priority 1)
(rule 1 (lower (has_type ty @ (multi_lane _ _) (fma x y z)))
      (lower_fmla (VecALUModOp.Fmla) x y z (vector_size ty)))

;; Scalar FMA (priority 0 - fallback)
(rule (lower (fma ty x y z))
      (aarch64_fmadd ty x y z))

;; Vector float binary operations
(rule -1 (lower (has_type ty @ (multi_lane _ _) (fadd x y)))
      (vec_rrr (VecALUOp.Fadd) x y (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane _ _) (fsub x y)))
      (vec_rrr (VecALUOp.Fsub) x y (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane _ _) (fmul x y)))
      (vec_rrr (VecALUOp.Fmul) x y (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane _ _) (fdiv x y)))
      (vec_rrr (VecALUOp.Fdiv) x y (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane _ _) (fmin x y)))
      (vec_rrr (VecALUOp.Fmin) x y (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane _ _) (fmax x y)))
      (vec_rrr (VecALUOp.Fmax) x y (vector_size ty)))

;;;; Rules for `fcmp` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;; Vector fcmp: zero-optimized rules (y == 0, not-equal case)
(rule 4 (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond_not_eq cond) x y)))
      (if (zero_value y))
      (let ((rn Reg x)
            (vec_size VectorSize (vector_size ty)))
          (value_reg (not (fcmeq0 rn vec_size) vec_size))))

;; Vector fcmp: zero-optimized rules (y == 0, other conditions)
(rule 3 (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond cond) x y)))
      (if (zero_value y))
      (let ((rn Reg x)
            (vec_size VectorSize (vector_size ty)))
          (value_reg (float_cmp_zero cond rn vec_size))))

;; Vector fcmp: zero-optimized rules (x == 0, not-equal case)
(rule 2 (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond_not_eq cond) x y)))
      (if (zero_value x))
      (let ((rn Reg y)
            (vec_size VectorSize (vector_size ty)))
          (value_reg (not (fcmeq0 rn vec_size) vec_size))))

;; Vector fcmp: zero-optimized rules (x == 0, other conditions)
(rule 1 (lower (has_type ty @ (multi_lane _ _) (fcmp (fcmp_zero_cond cond) x y)))
      (if (zero_value x))
      (let ((rn Reg y)
            (vec_size VectorSize (vector_size ty)))
          (value_reg (float_cmp_zero_swap cond rn vec_size))))

;; Vector fcmp: general case for vector floats
(rule -1 (lower (has_type out_ty (fcmp cond x @ (value_type in_ty) y)))
      (if (ty_vector_float in_ty))
      (vec_cmp x y in_ty (fp_cond_code cond)))

;; Scalar fcmp: FP comparison (FCMP)
(rule (lower (fcmp cond x y))
      (aarch64_fcmp x y))

;;;; Rules for `icmp` ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;; Vector icmp: zero-optimized rules (y == 0, not-equal case)
(rule 3 (lower (has_type ty @ (multi_lane _ _) (icmp (icmp_zero_cond_not_eq cond) x y)))
      (if (zero_value y))
      (let ((rn Reg x)
            (vec_size VectorSize (vector_size ty)))
          (value_reg (not (cmeq0 rn vec_size) vec_size))))

;; Vector icmp: zero-optimized rules (y == 0, other conditions)
(rule 2 (lower (has_type ty @ (multi_lane _ _) (icmp (icmp_zero_cond cond) x y)))
      (if (zero_value y))
      (let ((rn Reg x)
            (vec_size VectorSize (vector_size ty)))
          (value_reg (int_cmp_zero cond rn vec_size))))

;; Vector icmp: zero-optimized rules (x == 0, not-equal case)
(rule 1 (lower (has_type ty @ (multi_lane _ _) (icmp (icmp_zero_cond_not_eq cond) x y)))
      (if (zero_value x))
      (let ((rn Reg y)
            (vec_size VectorSize (vector_size ty)))
          (value_reg (not (cmeq0 rn vec_size) vec_size))))

;; Vector icmp: zero-optimized rules (x == 0, other conditions)
(rule 0 (lower (has_type ty @ (multi_lane _ _) (icmp (icmp_zero_cond cond) x y)))
      (if (zero_value x))
      (let ((rn Reg y)
            (vec_size VectorSize (vector_size ty)))
          (value_reg (int_cmp_zero_swap cond rn vec_size))))

;; Vector float unary operations
(rule -1 (lower (has_type ty @ (multi_lane _ _) (sqrt x)))
      (vec_misc (VecMisc2.Fsqrt) x (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane _ _) (fneg x)))
      (vec_misc (VecMisc2.Fneg) x (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane _ _) (fabs x)))
      (vec_misc (VecMisc2.Fabs) x (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane _ _) (ceil x)))
      (vec_misc (VecMisc2.Frintp) x (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane _ _) (floor x)))
      (vec_misc (VecMisc2.Frintm) x (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane _ _) (trunc x)))
      (vec_misc (VecMisc2.Frintz) x (vector_size ty)))

(rule -1 (lower (has_type ty @ (multi_lane _ _) (nearest x)))
      (vec_misc (VecMisc2.Frintn) x (vector_size ty)))

;; fabs: FP absolute value (FABS)
(rule (lower (fabs ty x))
      (aarch64_fabs ty x))

;; fneg: FP negation (FNEG)
(rule (lower (fneg ty x))
      (aarch64_fneg ty x))

;; fcopysign: Copy sign from y to magnitude of x
;; Implemented as: abs_x = fabs(x), result = y < 0 ? -abs_x : abs_x
(rule (lower (has_type $F32 (fcopysign x y)))
      (aarch64_fcopysign_32 x y))

(rule (lower (has_type $F64 (fcopysign x y)))
      (aarch64_fcopysign_64 x y))

;; Constant materialization

;; iconst: Integer constant (16-bit immediate via MOVZ)
(rule (lower (iconst ty k))
      (if-let $true (uimm16 k))
      (aarch64_movz ty k))

;; iconst: Larger constants (needs multiple instructions)
(rule (lower (iconst ty k))
      (aarch64_iconst ty k))

;; f32const: 32-bit floating-point constant
(decl constant_f32 (u32) Aarch64Inst)
(extern constructor constant_f32 constant_f32)

(rule (lower (f32const (u32_from_ieee32 n)))
      (constant_f32 n))

;; f64const: 64-bit floating-point constant
(decl constant_f64 (u64) Aarch64Inst)
(extern constructor constant_f64 constant_f64)

(rule (lower (f64const (u64_from_ieee64 n)))
      (constant_f64 n))

;; Helper extractors

;; Immediate value extractors (extern - implemented in isle_helpers.zig)

;; Check if value fits in unsigned 12-bit (0-4095)
(decl uimm12 (u64) u64)
(extern extractor uimm12 uimm12)

;; Check if value fits in unsigned 16-bit (0-65535)
(decl uimm16 (u64) u64)
(extern extractor uimm16 uimm16)

;; Check if value is a valid shift amount (0-63)
(decl valid_shift_imm (u64) u64)
(extern extractor valid_shift_imm valid_shift_imm)

;; Extract rotl immediate and convert to rotr immediate
;; rotl(x, k) = rotr(x, width - k)
(decl valid_rotl_imm (u32 u64) u32)
(extern extractor valid_rotl_imm valid_rotl_imm)

;; Check if value is in range where negation fits in unsigned 12-bit
;; Returns true if -4095 <= val <= -1
(decl in_neg_uimm12_range (i64) Unit)
(extern extractor in_neg_uimm12_range in_neg_uimm12_range)

;; Addressing mode extractors

;; Check if offset is valid for load immediate addressing
;; Accepts offsets 0-32760 (max for I64 8-byte aligned access)
(decl valid_ldr_imm_offset (Type u64) u64)
(extern extractor valid_ldr_imm_offset valid_ldr_imm_offset)

;; Check if offset is valid for store immediate addressing
(decl valid_str_imm_offset (Value u64) u64)
(extern extractor valid_str_imm_offset valid_str_imm_offset)

;; Check if shift is valid for load (must be 0-3)
(decl valid_ldr_shift (Type u64) u64)
(extern extractor valid_ldr_shift valid_ldr_shift)

;; Check if shift is valid for store (must be 0-3)
(decl valid_str_shift (Value u64) u64)
(extern extractor valid_str_shift valid_str_shift)

;; Type declarations

;; Integer comparison condition codes
(type IntCC (enum
  Equal
  NotEqual
  SignedLessThan
  SignedLessThanOrEqual
  SignedGreaterThan
  SignedGreaterThanOrEqual
  UnsignedLessThan
  UnsignedLessThanOrEqual
  UnsignedGreaterThan
  UnsignedGreaterThanOrEqual
))

;; Extend operations for extended register operands
(type ExtendOp (enum
  uxtb  ;; Zero-extend byte (8-bit)
  uxth  ;; Zero-extend halfword (16-bit)
  uxtw  ;; Zero-extend word (32-bit to 64-bit)
  uxtx  ;; Zero-extend doubleword (no-op for 64-bit)
  sxtb  ;; Sign-extend byte (8-bit)
  sxth  ;; Sign-extend halfword (16-bit)
  sxtw  ;; Sign-extend word (32-bit to 64-bit)
  sxtx  ;; Sign-extend doubleword (no-op for 64-bit)
))

;; Multi-register value type for wide operations (I128)
;; Maps to Zig's ValueRegs(VReg) type
(type ValueRegs extern (enum))

;; Constructor terms for aarch64 instructions

(type Aarch64Inst (enum))

;; Arithmetic
(decl aarch64_add_rr (Type Value Value) Aarch64Inst)
(decl aarch64_add_imm (Type Value i64) Aarch64Inst)
(decl aarch64_add_extended (Type Value Value ExtendOp) Aarch64Inst)
(decl aarch64_add_shifted (Type Value Value ShiftOp u6) Aarch64Inst)
(decl aarch64_sub_rr (Type Value Value) Aarch64Inst)
(decl aarch64_sub_imm (Type Value i64) Aarch64Inst)
(decl aarch64_sub_shifted (Type Value Value ShiftOp u6) Aarch64Inst)
(decl aarch64_sub_extended (Type Value Value ExtendOp) Aarch64Inst)

;; Arithmetic instructions that set flags (for I128 operations)
(decl aarch64_adds_rr (Type Reg Reg Reg) Aarch64Inst)
(decl aarch64_adc_rr (Type Reg Reg Reg) Aarch64Inst)
(decl aarch64_subs_rr (Type Reg Reg Reg) Aarch64Inst)
(decl aarch64_sbc_rr (Type Reg Reg Reg) Aarch64Inst)

(decl aarch64_mul_rr (Type Value Value) Aarch64Inst)
(decl aarch64_madd (Type Value Value Value) Aarch64Inst)
(decl aarch64_msub (Type Value Value Value) Aarch64Inst)
(decl aarch64_smulh (Value Value) Aarch64Inst)
(decl aarch64_umulh (Value Value) Aarch64Inst)
(decl aarch64_sdiv (Type Value Value) Aarch64Inst)
(decl aarch64_udiv (Type Value Value) Aarch64Inst)
(decl aarch64_smin (Type Value Value) Aarch64Inst)
(decl aarch64_umin (Type Value Value) Aarch64Inst)
(decl aarch64_smax (Type Value Value) Aarch64Inst)
(decl aarch64_umax (Type Value Value) Aarch64Inst)
(decl aarch64_bitselect (Value Value Value) Aarch64Inst)
(decl aarch64_iabs (Type Value) Aarch64Inst)
(decl aarch64_isplit (Value) ValueRegs)

;; Saturating arithmetic operations
(decl aarch64_sqadd_8 (Value Value) Aarch64Inst)
(decl aarch64_sqadd_16 (Value Value) Aarch64Inst)
(decl aarch64_sqadd_32 (Value Value) Aarch64Inst)
(decl aarch64_sqadd_64 (Value Value) Aarch64Inst)
(decl aarch64_sqsub_8 (Value Value) Aarch64Inst)
(decl aarch64_sqsub_16 (Value Value) Aarch64Inst)
(decl aarch64_sqsub_32 (Value Value) Aarch64Inst)
(decl aarch64_sqsub_64 (Value Value) Aarch64Inst)
(decl aarch64_uqadd_8 (Value Value) Aarch64Inst)
(decl aarch64_uqadd_16 (Value Value) Aarch64Inst)
(decl aarch64_uqadd_32 (Value Value) Aarch64Inst)
(decl aarch64_uqadd_64 (Value Value) Aarch64Inst)
(decl aarch64_uqsub_8 (Value Value) Aarch64Inst)
(decl aarch64_uqsub_16 (Value Value) Aarch64Inst)
(decl aarch64_uqsub_32 (Value Value) Aarch64Inst)
(decl aarch64_uqsub_64 (Value Value) Aarch64Inst)

;; Shift operations
(decl aarch64_lsl_rr (Type Value Value) Aarch64Inst)
(decl aarch64_lsl_imm (Type Value i64) Aarch64Inst)
(decl aarch64_lsr_rr (Type Value Value) Aarch64Inst)
(decl aarch64_lsr_imm (Type Value i64) Aarch64Inst)
(decl aarch64_asr_rr (Type Value Value) Aarch64Inst)
(decl aarch64_asr_imm (Type Value i64) Aarch64Inst)
(decl aarch64_ror_rr (Type Value Value) Aarch64Inst)
(decl aarch64_ror_imm (Type Value i64) Aarch64Inst)
(decl aarch64_rotl_rr (Type Value Value) Aarch64Inst)

;; Bitwise operations
(decl aarch64_and_rr (Type Value Value) Aarch64Inst)
(decl aarch64_and_imm (Type Value ImmLogic) Aarch64Inst)
(decl aarch64_orr_rr (Type Value Value) Aarch64Inst)
(decl aarch64_orr_imm (Type Value ImmLogic) Aarch64Inst)
(decl aarch64_eor_rr (Type Value Value) Aarch64Inst)
(decl aarch64_eor_imm (Type Value ImmLogic) Aarch64Inst)
(decl aarch64_mvn_rr (Type Value) Aarch64Inst)
(decl aarch64_bic_rr (Type Value Value) Aarch64Inst)
(decl aarch64_orn_rr (Type Value Value) Aarch64Inst)
(decl aarch64_eon_rr (Type Value Value) Aarch64Inst)

;; Bit manipulation operations
(decl aarch64_clz_32 (Value) Aarch64Inst)
(decl aarch64_clz_64 (Value) Aarch64Inst)
(decl aarch64_cls_32 (Value) Aarch64Inst)
(decl aarch64_cls_64 (Value) Aarch64Inst)
(decl aarch64_ctz_32 (Value) Aarch64Inst)
(decl aarch64_ctz_64 (Value) Aarch64Inst)
(decl aarch64_rbit_32 (Value) Aarch64Inst)
(decl aarch64_rbit_64 (Value) Aarch64Inst)
(decl aarch64_bswap_16 (Value) Aarch64Inst)
(decl aarch64_bswap_32 (Value) Aarch64Inst)
(decl aarch64_bswap_64 (Value) Aarch64Inst)

;; Population count operations
(decl aarch64_popcnt_8 (Value) Aarch64Inst)
(decl aarch64_popcnt_16 (Value) Aarch64Inst)
(decl aarch64_popcnt_32 (Value) Aarch64Inst)
(decl aarch64_popcnt_64 (Value) Aarch64Inst)
(decl lower_clz128 (ValueRegs) ValueRegs)
(decl lower_cls128 (ValueRegs) ValueRegs)
(decl lower_popcnt128 (ValueRegs) ValueRegs)

;; Negation operations
(decl aarch64_neg (Type Value) Aarch64Inst)
(decl aarch64_ngc (Type Value) Aarch64Inst)
(decl aarch64_iabs_8 (Value) Aarch64Inst)
(decl aarch64_iabs_16 (Value) Aarch64Inst)
(decl aarch64_iabs_32 (Value) Aarch64Inst)
(decl aarch64_iabs_64 (Value) Aarch64Inst)

;; Sign/zero extend operations
(decl aarch64_sxtb (Type Value) Aarch64Inst)
(decl aarch64_sxth (Type Value) Aarch64Inst)
(decl aarch64_sxtw (Value) Aarch64Inst)
(decl aarch64_uxtb (Type Value) Aarch64Inst)
(decl aarch64_uxth (Type Value) Aarch64Inst)

;; Comparison operations
(decl aarch64_cmp_rr (Type Value Value IntCC) Aarch64Inst)
(decl aarch64_cmp_imm (Type Value i64 IntCC) Aarch64Inst)
(decl aarch64_tst_imm (Type Value i64) Aarch64Inst)

;; Conditional select operations
(decl aarch64_csel (Type Value Value Value IntCC) Aarch64Inst)
(decl aarch64_csinc (Type Value Value Value IntCC) Aarch64Inst)
(decl aarch64_csinv (Type Value Value Value IntCC) Aarch64Inst)
(decl aarch64_csneg (Type Value Value Value IntCC) Aarch64Inst)

;; Memory
(decl aarch64_ldr (Type Value) Aarch64Inst)
(decl aarch64_ldr_imm (Type Value i64) Aarch64Inst)
(decl aarch64_ldr_reg (Type Value Value) Aarch64Inst)
(decl aarch64_ldr_ext (Type Value Value ExtendOp) Aarch64Inst)
(decl aarch64_ldr_shifted (Type Value Value i64) Aarch64Inst)
(decl aarch64_ldr_pre (Type Value i64) Aarch64Inst)   ;; Pre-index: base+=offset, then load
(decl aarch64_ldr_post (Type Value i64) Aarch64Inst)  ;; Post-index: load, then base+=offset
(decl aarch64_str (Value Value) Aarch64Inst)
(decl aarch64_str_imm (Value Value i64) Aarch64Inst)
(decl aarch64_str_reg (Value Value Value) Aarch64Inst)
(decl aarch64_str_ext (Value Value Value ExtendOp) Aarch64Inst)
(decl aarch64_str_shifted (Value Value Value i64) Aarch64Inst)
(decl aarch64_str_pre (Value Value i64) Aarch64Inst)  ;; Pre-index: base+=offset, then store
(decl aarch64_str_post (Value Value i64) Aarch64Inst) ;; Post-index: store, then base+=offset

;; Extending loads (load with zero/sign extension)
(decl aarch64_uload8 (Value) Aarch64Inst)   ;; LDRB: load byte, zero-extend to 32/64
(decl aarch64_uload16 (Value) Aarch64Inst)  ;; LDRH: load halfword, zero-extend to 32/64
(decl aarch64_uload32 (Value) Aarch64Inst)  ;; LDR (W): load word, zero-extend to 64
(decl aarch64_uload64 (Value) Aarch64Inst)  ;; LDR (X): load doubleword
(decl aarch64_sload8 (Value) Aarch64Inst)   ;; LDRSB: load signed byte, sign-extend to 32/64
(decl aarch64_sload16 (Value) Aarch64Inst)  ;; LDRSH: load signed halfword, sign-extend to 32/64
(decl aarch64_sload32 (Value) Aarch64Inst)  ;; LDRSW: load signed word, sign-extend to 64

;; Control flow
(decl aarch64_b (Block) Aarch64Inst)
(decl aarch64_b_cond (Value Block) Aarch64Inst)
(decl aarch64_cmp_and_branch (Type Value Value IntCC Block) Aarch64Inst)
(decl aarch64_cmp_imm_and_branch (Type Value i64 IntCC Block) Aarch64Inst)
(decl aarch64_ret () Aarch64Inst)

;; Constants
(decl aarch64_movz (Type i64) Aarch64Inst)
(decl aarch64_iconst (Type i64) Aarch64Inst)

;; Floating-point arithmetic
(decl aarch64_fadd (Type Value Value) Aarch64Inst)
(decl aarch64_fsub (Type Value Value) Aarch64Inst)
(decl aarch64_fmul (Type Value Value) Aarch64Inst)
(decl aarch64_fdiv (Type Value Value) Aarch64Inst)
(decl aarch64_fmin (Type Value Value) Aarch64Inst)
(decl aarch64_fmax (Type Value Value) Aarch64Inst)
(decl aarch64_fsqrt (Type Value) Aarch64Inst)
(decl aarch64_splat (Type Value) Aarch64Inst)
(decl aarch64_extractlane (Type Value Value) Aarch64Inst)
(decl aarch64_insertlane (Type Value Value Value) Aarch64Inst)

;; Floating-point conversions
(decl aarch64_fcvtzu_32 (Value) Aarch64Inst)  ;; FCVTZU: float to unsigned i32
(decl aarch64_fcvtzu_64 (Value) Aarch64Inst)  ;; FCVTZU: float to unsigned i64
(decl aarch64_fcvtzs_32 (Value) Aarch64Inst)  ;; FCVTZS: float to signed i32
(decl aarch64_fcvtzs_64 (Value) Aarch64Inst)  ;; FCVTZS: float to signed i64
(decl aarch64_ucvtf_32_to_f32 (Value) Aarch64Inst)  ;; UCVTF: u32 to f32
(decl aarch64_ucvtf_32_to_f64 (Value) Aarch64Inst)  ;; UCVTF: u32 to f64
(decl aarch64_ucvtf_64_to_f32 (Value) Aarch64Inst)  ;; UCVTF: u64 to f32
(decl aarch64_ucvtf_64_to_f64 (Value) Aarch64Inst)  ;; UCVTF: u64 to f64
(decl aarch64_scvtf_32_to_f32 (Value) Aarch64Inst)  ;; SCVTF: i32 to f32
(decl aarch64_scvtf_32_to_f64 (Value) Aarch64Inst)  ;; SCVTF: i32 to f64
(decl aarch64_scvtf_64_to_f32 (Value) Aarch64Inst)  ;; SCVTF: i64 to f32
(decl aarch64_scvtf_64_to_f64 (Value) Aarch64Inst)  ;; SCVTF: i64 to f64
(decl aarch64_fcvt_f64_to_f32 (Value) Aarch64Inst)  ;; FCVT: f64 to f32
(decl aarch64_fcvt_f32_to_f64 (Value) Aarch64Inst)  ;; FCVT: f32 to f64

;; Float to integer conversions with saturation
(decl aarch64_fcvtzs_32_to_32 (Value) Aarch64Inst)  ;; FCVTZS: f32 to i32 (saturating)
(decl aarch64_fcvtzs_64_to_32 (Value) Aarch64Inst)  ;; FCVTZS: f64 to i32 (saturating)
(decl aarch64_fcvtzs_32_to_64 (Value) Aarch64Inst)  ;; FCVTZS: f32 to i64 (saturating)
(decl aarch64_fcvtzs_64_to_64 (Value) Aarch64Inst)  ;; FCVTZS: f64 to i64 (saturating)
(decl aarch64_fcvtzu_32_to_32 (Value) Aarch64Inst)  ;; FCVTZU: f32 to u32 (saturating)
(decl aarch64_fcvtzu_64_to_32 (Value) Aarch64Inst)  ;; FCVTZU: f64 to u32 (saturating)
(decl aarch64_fcvtzu_32_to_64 (Value) Aarch64Inst)  ;; FCVTZU: f32 to u64 (saturating)
(decl aarch64_fcvtzu_64_to_64 (Value) Aarch64Inst)  ;; FCVTZU: f64 to u64 (saturating)

;; General floating-point conversion (type-generic)
(decl aarch64_scvtf (Type Type Value) Aarch64Inst)  ;; SCVTF: signed int to float
(decl aarch64_ucvtf (Type Type Value) Aarch64Inst)  ;; UCVTF: unsigned int to float
(decl aarch64_fpromote (Value) Aarch64Inst)         ;; FCVT: f32 to f64
(decl aarch64_fdemote (Value) Aarch64Inst)          ;; FCVT: f64 to f32

;; Floating-point rounding operations
(decl aarch64_nearest (Type Value) Aarch64Inst)     ;; FRINTN: round to nearest
(decl aarch64_trunc (Type Value) Aarch64Inst)       ;; FRINTZ: round toward zero
(decl aarch64_ceil (Type Value) Aarch64Inst)        ;; FRINTP: round toward +infinity
(decl aarch64_floor (Type Value) Aarch64Inst)       ;; FRINTM: round toward -infinity

;; Multi-register value operations
(decl value_regs_from_values (Value Value) ValueRegs)  ;; Construct ValueRegs from two I64 values
(extern constructor value_regs_from_values value_regs_from_values)

;; Atomic operations
(decl aarch64_atomic_load_acquire (Type Value) Aarch64Inst)   ;; LDAR: atomic load with acquire
(decl aarch64_atomic_store_release (Type Value Value) Aarch64Inst)  ;; STLR: atomic store with release
(decl aarch64_fence (AtomicOrdering) Aarch64Inst)  ;; DMB: memory fence

;; Floating-point arithmetic
(decl aarch64_fadd (Type Value Value) Aarch64Inst)
(decl aarch64_fsub (Type Value Value) Aarch64Inst)
(decl aarch64_fmul (Type Value Value) Aarch64Inst)
(decl aarch64_fdiv (Type Value Value) Aarch64Inst)
(decl aarch64_fmadd (Type Value Value Value) Aarch64Inst)
(decl aarch64_fmsub (Type Value Value Value) Aarch64Inst)
(decl aarch64_fnmadd (Type Value Value Value) Aarch64Inst)
(decl aarch64_fnmsub (Type Value Value Value) Aarch64Inst)
(decl aarch64_fcmp (Value Value) Aarch64Inst)
(decl aarch64_fabs (Type Value) Aarch64Inst)
(decl aarch64_fneg (Type Value) Aarch64Inst)
(decl aarch64_fcopysign_32 (Value Value) Aarch64Inst)
(decl aarch64_fcopysign_64 (Value Value) Aarch64Inst)

;; SIMD/Vector operations
(decl aarch64_vec_add (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_sub (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_mul (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_smin (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_smax (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_umin (VectorSize Value Value) Aarch64Inst)
(decl aarch64_vec_umax (VectorSize Value Value) Aarch64Inst)

;; SIMD/Vector reductions (horizontal operations)
(decl aarch64_addv (VectorSize Value) Aarch64Inst)     ;; Horizontal add across vector
(decl aarch64_sminv (VectorSize Value) Aarch64Inst)    ;; Horizontal signed min
(decl aarch64_smaxv (VectorSize Value) Aarch64Inst)    ;; Horizontal signed max
(decl aarch64_uminv (VectorSize Value) Aarch64Inst)    ;; Horizontal unsigned min
(decl aarch64_umaxv (VectorSize Value) Aarch64Inst)    ;; Horizontal unsigned max

;; SIMD lane manipulation
(decl aarch64_dup (VectorSize Value) Aarch64Inst)             ;; Duplicate scalar to all lanes
(decl aarch64_extract_lane (Value Value) Aarch64Inst)         ;; Extract lane to scalar
(decl aarch64_addp (VectorSize Value Value) Aarch64Inst)      ;; Add adjacent pairs

;; Atomic operations (LSE - Large System Extensions)
(decl aarch64_ldar (Value) Aarch64Inst)                ;; Load-acquire register
(decl aarch64_stlr (Value Value) Aarch64Inst)          ;; Store-release register
(decl aarch64_ldadd (Value Value) Aarch64Inst)         ;; Atomic add with acquire/release
(decl aarch64_ldclr (Value Value) Aarch64Inst)         ;; Atomic clear bits with acquire/release
(decl aarch64_ldeor (Value Value) Aarch64Inst)         ;; Atomic XOR with acquire/release
(decl aarch64_ldset (Value Value) Aarch64Inst)         ;; Atomic OR (set bits) with acquire/release
(decl aarch64_ldsmax (Value Value) Aarch64Inst)        ;; Atomic signed max with acquire/release
(decl aarch64_ldsmin (Value Value) Aarch64Inst)        ;; Atomic signed min with acquire/release
(decl aarch64_ldumax (Value Value) Aarch64Inst)        ;; Atomic unsigned max with acquire/release
(decl aarch64_ldumin (Value Value) Aarch64Inst)        ;; Atomic unsigned min with acquire/release
(decl aarch64_swpal (Value Value) Aarch64Inst)         ;; Atomic swap with acquire/release
(decl aarch64_casal (Value Value Value) Aarch64Inst)   ;; Compare-and-swap with acquire/release

;; LSE Atomic helper implementations
;; These generate atomic instructions with sequentially-consistent ordering

;; aarch64_ldadd: Atomic add - LDADDAL (acquire-release for seq_cst)
(rule (aarch64_ldadd addr val)
      ;; TODO: Select ordering variant based on context
      ;; For now, use sequentially-consistent (acquire-release)
      (Aarch64Inst.ldaddal val addr))

;; aarch64_ldclr: Atomic clear bits - LDCLRAL
(rule (aarch64_ldclr addr val)
      (Aarch64Inst.ldclral val addr))

;; aarch64_ldeor: Atomic XOR - LDEORAL
(rule (aarch64_ldeor addr val)
      (Aarch64Inst.ldeoral val addr))

;; aarch64_ldset: Atomic OR - LDSETAL
(rule (aarch64_ldset addr val)
      (Aarch64Inst.ldsetal val addr))

;; aarch64_ldsmax: Atomic signed maximum - LDSMAXAL
(rule (aarch64_ldsmax addr val)
      (Aarch64Inst.ldsmaxal val addr))

;; aarch64_ldsmin: Atomic signed minimum - LDSMINAL
(rule (aarch64_ldsmin addr val)
      (Aarch64Inst.ldsminal val addr))

;; aarch64_ldumax: Atomic unsigned maximum - LDUMAXAL
(rule (aarch64_ldumax addr val)
      (Aarch64Inst.ldumaxal val addr))

;; aarch64_ldumin: Atomic unsigned minimum - LDUMINAL
(rule (aarch64_ldumin addr val)
      (Aarch64Inst.lduminal val addr))

;; aarch64_swpal: Atomic swap - SWPAL
(rule (aarch64_swpal addr val)
      (Aarch64Inst.swpal val addr))

;; aarch64_casal: Compare-and-swap - CASAL
(rule (aarch64_casal addr expected new_val)
      (Aarch64Inst.casal expected new_val addr))

;; Memory barrier operations
(decl aarch64_dmb (ShareabilityDomain) Aarch64Inst)    ;; Data Memory Barrier
(decl aarch64_dsb (ShareabilityDomain) Aarch64Inst)    ;; Data Synchronization Barrier
(decl aarch64_isb () Aarch64Inst)                      ;; Instruction Synchronization Barrier

;; Shareability domain for memory barrier instructions
(type ShareabilityDomain (enum
  SY      ;; Full system - all agents
  ISH     ;; Inner shareable - processors in same inner shareable domain
  ISHLD   ;; Inner shareable load - acquire barrier for inner shareable
  ISHST   ;; Inner shareable store - release barrier for inner shareable
  NSH     ;; Non-shareable - only this processor
  OSH     ;; Outer shareable - processors in same outer shareable domain
))

;; Vector type enum
(type VectorSize (enum
  V8B   ;; 8 bytes (8x i8)
  V16B  ;; 16 bytes (16x i8)
  V4H   ;; 4 halfwords (4x i16)
  V8H   ;; 8 halfwords (8x i16)
  V2S   ;; 2 singles (2x i32/f32)
  V4S   ;; 4 singles (4x i32/f32)
  V2D   ;; 2 doubles (2x i64/f64)
))

;; Vector shift immediate operations
(type VecShiftImmOp (enum
  Shl   ;; Unsigned shift left
  Ushr  ;; Unsigned shift right
  Sshr  ;; Signed shift right
))

;; SIMD/Vector lowering rules
;; NOTE: Vector arithmetic (add/sub/mul/min/max) now uses type-parameterized iadd/isub/imul/imin/imax
;; NOTE: Vector reductions now use type-parameterized reduce_add/reduce_smin/reduce_smax/reduce_umin/reduce_umax
;; All use multi_lane extractor instead of explicit viadd/visub/vimul/vimin/vimax/vreduce_* opcodes

;; SIMD lane manipulation lowering

;; scalar_to_vector: Duplicate scalar to all vector lanes
;; Pattern: scalar_to_vector(s) => DUP
(rule (lower (scalar_to_vector (value_type $I8X16) x))
      (aarch64_dup VecElemSize_8x16 x))

(rule (lower (scalar_to_vector (value_type $I16X8) x))
      (aarch64_dup VecElemSize_16x8 x))

(rule (lower (scalar_to_vector (value_type $I32X4) x))
      (aarch64_dup VecElemSize_32x4 x))

(rule (lower (scalar_to_vector (value_type $I64X2) x))
      (aarch64_dup VecElemSize_64x2 x))

;; extract_vector: Extract lane from vector to scalar
;; Pattern: extract_vector(vec, lane) => UMOV
(rule (lower (extract_vector vec lane))
      (aarch64_extract_lane vec lane))

;; iadd_pairwise: Add adjacent pairs of elements
;; Pattern: iadd_pairwise(vec) => ADDP
(rule (lower (iadd_pairwise (value_type $I8X16) x))
      (aarch64_addp VecElemSize_8x16 x x))

(rule (lower (iadd_pairwise (value_type $I16X8) x))
      (aarch64_addp VecElemSize_16x8 x x))

(rule (lower (iadd_pairwise (value_type $I32X4) x))
      (aarch64_addp VecElemSize_32x4 x x))

(rule (lower (iadd_pairwise (value_type $I64X2) x))
      (aarch64_addp VecElemSize_64x2 x x))

;; Atomic operations lowering

;; atomic_load: Load with acquire semantics
;; Pattern: atomic_load(addr) => LDAR
(rule (lower (atomic_load addr _flags))
      (aarch64_ldar addr))

;; atomic_store: Store with release semantics
;; Pattern: atomic_store(val, addr) => STLR
(rule (lower (atomic_store val addr _flags))
      (aarch64_stlr val addr))

;; atomic_rmw: Atomic read-modify-write operations
;; These use LSE (Large System Extensions) instructions when available

;; atomic_rmw.add: Atomic add
;; Pattern: atomic_rmw.add(addr, val) => LDADD / LDADDAL
(rule (lower (atomic_rmw_add addr val))
      (aarch64_ldadd addr val))

;; atomic_rmw.sub: Atomic subtract (implemented as add with negated value)
;; Pattern: atomic_rmw.sub(addr, val) => LDADD with -val
(rule (lower (atomic_rmw_sub addr val))
      (let ((neg_val Aarch64Inst (aarch64_neg $I64 val)))
            (aarch64_ldadd addr neg_val)))

;; atomic_rmw.and: Atomic AND (implemented as clear with complement)
;; Pattern: atomic_rmw.and(addr, val) => LDCLR with ~val
(rule (lower (atomic_rmw_and addr val))
      (let ((not_val Aarch64Inst (aarch64_mvn_rr $I64 val)))
            (aarch64_ldclr addr not_val)))

;; atomic_rmw.or: Atomic OR
;; Pattern: atomic_rmw.or(addr, val) => LDSET
(rule (lower (atomic_rmw_or addr val))
      (aarch64_ldset addr val))

;; atomic_rmw.xor: Atomic XOR
;; Pattern: atomic_rmw.xor(addr, val) => LDEOR
(rule (lower (atomic_rmw_xor addr val))
      (aarch64_ldeor addr val))

;; atomic_rmw.xchg: Atomic exchange
;; Pattern: atomic_rmw.xchg(addr, val) => SWPAL
(rule (lower (atomic_rmw_xchg addr val))
      (aarch64_swpal addr val))

;; atomic_rmw.smax: Atomic signed maximum
;; Pattern: atomic_rmw.smax(addr, val) => LDSMAX
(rule (lower (atomic_rmw_smax addr val))
      (aarch64_ldsmax addr val))

;; atomic_rmw.smin: Atomic signed minimum
;; Pattern: atomic_rmw.smin(addr, val) => LDSMIN
(rule (lower (atomic_rmw_smin addr val))
      (aarch64_ldsmin addr val))

;; atomic_rmw.umax: Atomic unsigned maximum
;; Pattern: atomic_rmw.umax(addr, val) => LDUMAX
(rule (lower (atomic_rmw_umax addr val))
      (aarch64_ldumax addr val))

;; atomic_rmw.umin: Atomic unsigned minimum
;; Pattern: atomic_rmw.umin(addr, val) => LDUMIN
(rule (lower (atomic_rmw_umin addr val))
      (aarch64_ldumin addr val))

;; atomic_cas: Compare-and-swap with acquire/release semantics
;; Pattern: atomic_cas(addr, expected, new) => CASAL
(rule (lower (atomic_cas addr expected new_val))
      (aarch64_casal addr expected new_val))

;; Memory barrier lowering

;; fence: Memory fence with sequential consistency
;; Pattern: fence(seq_cst) => DSB SY (full system barrier)
(rule (lower (fence AtomicOrdering.seq_cst))
      (aarch64_dsb ShareabilityDomain.SY))

;; fence: Memory fence with acquire semantics
;; Pattern: fence(acquire) => DMB ISHLD (inner shareable load barrier)
(rule (lower (fence AtomicOrdering.acquire))
      (aarch64_dmb ShareabilityDomain.ISHLD))

;; fence: Memory fence with release semantics
;; Pattern: fence(release) => DMB ISH (inner shareable barrier)
(rule (lower (fence AtomicOrdering.release))
      (aarch64_dmb ShareabilityDomain.ISH))

;; fence: Memory fence with acquire-release semantics
;; Pattern: fence(acq_rel) => DMB ISH (inner shareable barrier)
(rule (lower (fence AtomicOrdering.acq_rel))
      (aarch64_dmb ShareabilityDomain.ISH))

;; System register access lowering

;; SystemReg enum type for system registers
(type SystemReg (enum
  nzcv         ;; Condition flags (Negative, Zero, Carry, Overflow)
  fpcr         ;; Floating-point Control Register
  fpsr         ;; Floating-point Status Register
  tpidr_el0    ;; Thread Pointer/ID Register (User Read/Write)
  tpidrro_el0  ;; Thread Pointer/ID Register (User Read-Only)
))

;; Constructor declarations for system register access

;; MRS - Move from System Register (read system register)
(decl aarch64_mrs (SystemReg) Aarch64Inst)

;; MSR - Move to System Register (write system register)
(decl aarch64_msr (SystemReg Value) Aarch64Inst)

;; Helper functions
(decl negate_i64 (i64) i64)

;; Vector widening constructors

;; SSHLL - Signed shift-left-long (widen and optionally shift)
(decl aarch64_sshll (Value VecElemSize u8 bool) Aarch64Inst)

;; USHLL - Unsigned shift-left-long (widen and optionally shift)
(decl aarch64_ushll (Value VecElemSize u8 bool) Aarch64Inst)

;; Vector narrowing constructors (combined operations)

;; SQXTN + SQXTN2 - Signed narrow combined (narrow x to low, y to high)
(decl aarch64_sqxtn_combined (Value Value VecElemSize) Aarch64Inst)

;; SQXTUN + SQXTUN2 - Signed to unsigned narrow combined
(decl aarch64_sqxtun_combined (Value Value VecElemSize) Aarch64Inst)

;; UQXTN + UQXTN2 - Unsigned narrow combined
(decl aarch64_uqxtn_combined (Value Value VecElemSize) Aarch64Inst)

;; Vector narrowing constructors (combined operations)

;; SQXTN + SQXTN2 - Signed narrow combined (narrow x to low, y to high)
(decl aarch64_sqxtn_combined (Value Value VecElemSize) Aarch64Inst)

;; SQXTUN + SQXTUN2 - Signed to unsigned narrow combined
(decl aarch64_sqxtun_combined (Value Value VecElemSize) Aarch64Inst)

;; UQXTN + UQXTN2 - Unsigned narrow combined
(decl aarch64_uqxtn_combined (Value Value VecElemSize) Aarch64Inst)

;;;; Rules for `swiden_low` (signed widen low half) ;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;; I8X16 -> I16X8: widen low 8 lanes
(rule (lower (has_type $I16X8 (swiden_low x)))
      (aarch64_sshll x (VecElemSize.Size16x8) 0 false))

;; I16X8 -> I32X4: widen low 4 lanes
(rule (lower (has_type $I32X4 (swiden_low x)))
      (aarch64_sshll x (VecElemSize.Size32x4) 0 false))

;; I32X4 -> I64X2: widen low 2 lanes
(rule (lower (has_type $I64X2 (swiden_low x)))
      (aarch64_sshll x (VecElemSize.Size64x2) 0 false))

;;;; Rules for `swiden_high` (signed widen high half) ;;;;;;;;;;;;;;;;;;;;;;;;;;

;; I8X16 -> I16X8: widen high 8 lanes
(rule (lower (has_type $I16X8 (swiden_high x)))
      (aarch64_sshll x (VecElemSize.Size16x8) 0 true))

;; I16X8 -> I32X4: widen high 4 lanes
(rule (lower (has_type $I32X4 (swiden_high x)))
      (aarch64_sshll x (VecElemSize.Size32x4) 0 true))

;; I32X4 -> I64X2: widen high 2 lanes
(rule (lower (has_type $I64X2 (swiden_high x)))
      (aarch64_sshll x (VecElemSize.Size64x2) 0 true))

;;;; Rules for `uwiden_low` (unsigned widen low half) ;;;;;;;;;;;;;;;;;;;;;;;;;;

;; I8X16 -> I16X8: widen low 8 lanes
(rule (lower (has_type $I16X8 (uwiden_low x)))
      (aarch64_ushll x (VecElemSize.Size16x8) 0 false))

;; I16X8 -> I32X4: widen low 4 lanes
(rule (lower (has_type $I32X4 (uwiden_low x)))
      (aarch64_ushll x (VecElemSize.Size32x4) 0 false))

;; I32X4 -> I64X2: widen low 2 lanes
(rule (lower (has_type $I64X2 (uwiden_low x)))
      (aarch64_ushll x (VecElemSize.Size64x2) 0 false))

;;;; Rules for `uwiden_high` (unsigned widen high half) ;;;;;;;;;;;;;;;;;;;;;;;;

;; I8X16 -> I16X8: widen high 8 lanes
(rule (lower (has_type $I16X8 (uwiden_high x)))
      (aarch64_ushll x (VecElemSize.Size16x8) 0 true))

;; I16X8 -> I32X4: widen high 4 lanes
(rule (lower (has_type $I32X4 (uwiden_high x)))
      (aarch64_ushll x (VecElemSize.Size32x4) 0 true))

;; I32X4 -> I64X2: widen high 2 lanes
(rule (lower (has_type $I64X2 (uwiden_high x)))
      (aarch64_ushll x (VecElemSize.Size64x2) 0 true))

;;;; Rules for `snarrow` (signed narrow) ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;; I16X8 + I16X8 -> I8X16: narrow two vectors to one
(rule (lower (has_type $I8X16 (snarrow x y)))
      (aarch64_sqxtn_combined x y (VecElemSize.Size8x16)))

;; I32X4 + I32X4 -> I16X8: narrow two vectors to one
(rule (lower (has_type $I16X8 (snarrow x y)))
      (aarch64_sqxtn_combined x y (VecElemSize.Size16x8)))

;; I64X2 + I64X2 -> I32X4: narrow two vectors to one
(rule (lower (has_type $I32X4 (snarrow x y)))
      (aarch64_sqxtn_combined x y (VecElemSize.Size32x4)))

;;;; Rules for `unarrow` (unsigned narrow from signed) ;;;;;;;;;;;;;;;;;;;;;;;;;;

;; I16X8 + I16X8 -> I8X16: narrow signed to unsigned
(rule (lower (has_type $I8X16 (unarrow x y)))
      (aarch64_sqxtun_combined x y (VecElemSize.Size8x16)))

;; I32X4 + I32X4 -> I16X8: narrow signed to unsigned
(rule (lower (has_type $I16X8 (unarrow x y)))
      (aarch64_sqxtun_combined x y (VecElemSize.Size16x8)))

;; I64X2 + I64X2 -> I32X4: narrow signed to unsigned
(rule (lower (has_type $I32X4 (unarrow x y)))
      (aarch64_sqxtun_combined x y (VecElemSize.Size32x4)))

;;;; Rules for `uunarrow` (unsigned narrow from unsigned) ;;;;;;;;;;;;;;;;;;;;;;;

;; I16X8 + I16X8 -> I8X16: narrow unsigned to unsigned
(rule (lower (has_type $I8X16 (uunarrow x y)))
      (aarch64_uqxtn_combined x y (VecElemSize.Size8x16)))

;; I32X4 + I32X4 -> I16X8: narrow unsigned to unsigned
(rule (lower (has_type $I16X8 (uunarrow x y)))
      (aarch64_uqxtn_combined x y (VecElemSize.Size16x8)))

;; I64X2 + I64X2 -> I32X4: narrow unsigned to unsigned
(rule (lower (has_type $I32X4 (uunarrow x y)))
      (aarch64_uqxtn_combined x y (VecElemSize.Size32x4)))

;;;; Float vector conversions ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;; FCVTL - Float convert to higher precision (F32 -> F64)
(decl aarch64_fcvtl (Value bool) Aarch64Inst)

;; FCVTN - Float convert to lower precision (F64 -> F32)
(decl aarch64_fcvtn_combined (Value Value) Aarch64Inst)

;; F32X4 -> F64X2: promote low 2 lanes
(rule (lower (has_type $F64X2 (fvpromote_low x)))
      (aarch64_fcvtl x false))

;; F64X2 + F64X2 -> F32X4: demote two vectors
(rule (lower (has_type $F32X4 (fvdemote x y)))
      (aarch64_fcvtn_combined x y))

;; Trap operations
;; trap: Unconditional trap with trap code
(decl aarch64_trap (TrapCode) Aarch64Inst)
(rule (lower (trap trap_code))
      (aarch64_trap trap_code))

;; trapz: Trap if value is zero
(decl aarch64_trapz (Value TrapCode) Aarch64Inst)
(rule (lower (trapz val trap_code))
      (aarch64_trapz val trap_code))

;; trapnz: Trap if value is non-zero
(decl aarch64_trapnz (Value TrapCode) Aarch64Inst)
(rule (lower (trapnz val trap_code))
      (aarch64_trapnz val trap_code))

;; nop: No operation (emits nothing)
(rule (lower (nop))
      (invalid_reg))

;; bitcast: Type punning between same-size types
;; Float/Vector <-> Float/Vector: no-op (same register file)
(decl aarch64_bitcast_noop (Value) Aarch64Inst)
(rule 7 (lower (has_type (ty_float_or_vec _) (bitcast _ x @ (value_type (ty_float_or_vec _)))))
      (aarch64_bitcast_noop x))

;; GPR <-> GPR: no-op (same size)
(rule 6 (lower (has_type out_ty (bitcast _ x @ (value_type in_ty))))
      (if (ty_int_ref_scalar_64 out_ty))
      (if (ty_int_ref_scalar_64 in_ty))
      (aarch64_bitcast_noop x))

;; GPR -> Float/Vector: FMOV (GPR to FPR)
(decl aarch64_fmov_from_gpr (Value Type) Aarch64Inst)
(rule 5 (lower (has_type (ty_float_or_vec _) (bitcast _ x @ (value_type in_ty))))
      (if (ty_int_ref_scalar_64 in_ty))
      (aarch64_fmov_from_gpr x in_ty))

;; Float/Vector -> GPR: FMOV (FPR to GPR)
(decl aarch64_fmov_to_gpr (Value Type) Aarch64Inst)
(rule 4 (lower (has_type out_ty (bitcast _ x @ (value_type (fits_in_64 (ty_float_or_vec _))))))
      (if (ty_int_ref_scalar_64 out_ty))
      (aarch64_fmov_to_gpr x out_ty))

;; I128 <-> I128: no-op
(rule 3 (lower (has_type $I128 (bitcast _ x @ (value_type $I128))))
      (aarch64_bitcast_noop x))

;; ABI register accessors
(decl stack_reg () Aarch64Inst)
(rule (lower (get_stack_pointer))
      (stack_reg))

(decl fp_reg () Aarch64Inst)
(rule (lower (get_frame_pointer))
      (fp_reg))

(decl link_reg () Aarch64Inst)
(rule (lower (get_return_address))
      (link_reg))

(decl pinned_reg () Aarch64Inst)
(rule (lower (get_pinned_reg))
      (pinned_reg))

(decl aarch64_set_pinned_reg (Value) Aarch64Inst)
(rule (lower (set_pinned_reg val))
      (aarch64_set_pinned_reg val))

;; Debug operations
(decl aarch64_debugtrap () Aarch64Inst)
(rule (lower (debugtrap))
      (aarch64_debugtrap))

(rule (lower (sequence_point))
      (invalid_reg))

;; stack_addr: Compute address of stack slot
(decl aarch64_stack_addr (StackSlot Offset32) Aarch64Inst)
(rule (lower (stack_addr stack_slot offset))
      (aarch64_stack_addr stack_slot offset))

;; stack_load: Load from stack slot
(decl aarch64_stack_load (Type StackSlot Offset32) Aarch64Inst)
(rule (lower (has_type ty (stack_load stack_slot offset)))
      (aarch64_stack_load ty stack_slot offset))

;; stack_store: Store to stack slot
(decl aarch64_stack_store (Type Value StackSlot Offset32) Aarch64Inst)
(rule (lower (stack_store val stack_slot offset))
      (aarch64_stack_store (value_type val) val stack_slot offset))

;; global_value: Load address of global value
(decl aarch64_global_value (GlobalValue) Aarch64Inst)
(rule (lower (global_value gv))
      (aarch64_global_value gv))

;; symbol_value: Load address of external symbol
(decl aarch64_symbol_value (ExternalName i64) Aarch64Inst)
(rule (lower (symbol_value (symbol_value_data extname dist offset)))
      (aarch64_symbol_value extname offset))

;; func_addr: Load address of function
(decl aarch64_func_addr (ExternalName) Aarch64Inst)
(rule (lower (func_addr (func_ref_data _ extname dist _)))
      (aarch64_func_addr extname))

;; Overflow arithmetic
;; uadd_overflow: Unsigned add with overflow detection
(decl aarch64_uadd_overflow (Type Value Value) ValueRegs)
(rule (lower (has_type (ty_32_or_64 ty) (uadd_overflow a b)))
      (aarch64_uadd_overflow ty a b))

;; usub_overflow: Unsigned subtract with overflow (borrow) detection
(decl aarch64_usub_overflow (Type Value Value) ValueRegs)
(rule (lower (has_type (ty_32_or_64 ty) (usub_overflow a b)))
      (aarch64_usub_overflow ty a b))

;; sadd_overflow: Signed add with overflow detection
(decl aarch64_sadd_overflow (Type Value Value) ValueRegs)
(rule (lower (has_type (ty_32_or_64 ty) (sadd_overflow a b)))
      (aarch64_sadd_overflow ty a b))

;; ssub_overflow: Signed subtract with overflow detection
(decl aarch64_ssub_overflow (Type Value Value) ValueRegs)
(rule (lower (has_type (ty_32_or_64 ty) (ssub_overflow a b)))
      (aarch64_ssub_overflow ty a b))

;; umul_overflow: Unsigned multiply with overflow detection
(decl aarch64_umul_overflow_i16 (Type Value Value) ValueRegs)
(decl aarch64_umul_overflow_i32 (Value Value) ValueRegs)
(decl aarch64_umul_overflow_i64 (Value Value) ValueRegs)

(rule 1 (lower (has_type $I16 (umul_overflow a b)))
      (aarch64_umul_overflow_i16 $I16 a b))

(rule 2 (lower (has_type $I32 (umul_overflow a b)))
      (aarch64_umul_overflow_i32 a b))

(rule 2 (lower (has_type $I64 (umul_overflow a b)))
      (aarch64_umul_overflow_i64 a b))

;; smul_overflow: Signed multiply with overflow detection
(decl aarch64_smul_overflow_i16 (Type Value Value) ValueRegs)
(decl aarch64_smul_overflow_i32 (Value Value) ValueRegs)
(decl aarch64_smul_overflow_i64 (Value Value) ValueRegs)

(rule 1 (lower (has_type $I16 (smul_overflow a b)))
      (aarch64_smul_overflow_i16 $I16 a b))

(rule 2 (lower (has_type $I32 (smul_overflow a b)))
      (aarch64_smul_overflow_i32 a b))

(rule 2 (lower (has_type $I64 (smul_overflow a b)))
      (aarch64_smul_overflow_i64 a b))

;; Tail call operations
;; return_call: Direct tail call (deallocate frame, then branch)
(decl aarch64_return_call (SigRef ExternalName ValueSlice) Aarch64Inst)

;; Near return call optimization (priority 1 - prefer when distance is known near)
(rule 1 (lower (return_call (func_ref_data sig_ref name (RelocDistance.Near) _) args))
      (aarch64_return_call sig_ref name args))

;; Fallback return call (priority 0 - any distance)
(rule (lower (return_call (func_ref_data sig_ref name dist _) args))
      (aarch64_return_call sig_ref name args))

;; return_call_indirect: Indirect tail call
(decl aarch64_return_call_indirect (SigRef Value ValueSlice) Aarch64Inst)
(rule (lower (return_call_indirect sig_ref ptr args))
      (aarch64_return_call_indirect sig_ref ptr args))

;; Vector test operations
;; vall_true: Test if all vector lanes are non-zero
(decl aarch64_vall_true (Value Type) Aarch64Inst)
(rule (lower (vall_true x @ (value_type ty)))
      (aarch64_vall_true x ty))

;; vany_true: Test if any vector lane is non-zero
(decl aarch64_vany_true (Value Type) Aarch64Inst)
(rule (lower (vany_true x @ (value_type ty)))
      (aarch64_vany_true x ty))

;; vhigh_bits: Extract high bit from each lane
(decl aarch64_vhigh_bits (Value Type) Aarch64Inst)
(rule (lower (vhigh_bits vec @ (value_type ty)))
      (aarch64_vhigh_bits vec ty))

;; Call operations
;; call: Direct function call
(decl aarch64_call (SigRef ExternalName ValueSlice) ValueRegs)

;; Near call optimization (priority 1 - prefer when distance is known near)
(rule 1 (lower (call (func_ref_data sig_ref name (RelocDistance.Near) _) args))
      (aarch64_call sig_ref name args))

;; Fallback call (priority 0 - any distance)
(rule (lower (call (func_ref_data sig_ref name dist _) args))
      (aarch64_call sig_ref name args))

;; call_indirect: Indirect function call via pointer
(decl aarch64_call_indirect (SigRef Value ValueSlice) ValueRegs)
(rule (lower (call_indirect sig_ref ptr args))
      (aarch64_call_indirect sig_ref ptr args))

;; Shuffle pattern extractors
;; Extract lane index if shuffle duplicates a single 8-bit lane
(decl shuffle_dup8_from_imm (Immediate) u8)
(extern extractor shuffle_dup8_from_imm shuffle_dup8_from_imm)

;; Extract lane index if shuffle duplicates a single 16-bit lane
(decl shuffle_dup16_from_imm (Immediate) u8)
(extern extractor shuffle_dup16_from_imm shuffle_dup16_from_imm)

;; Extract lane index if shuffle duplicates a single 32-bit lane
(decl shuffle_dup32_from_imm (Immediate) u8)
(extern extractor shuffle_dup32_from_imm shuffle_dup32_from_imm)

;; Extract lane index if shuffle duplicates a single 64-bit lane
(decl shuffle_dup64_from_imm (Immediate) u8)
(extern extractor shuffle_dup64_from_imm shuffle_dup64_from_imm)

;; Extract byte offset for EXT (concatenate + shift) pattern
(decl vec_extract_imm4_from_immediate (Immediate) u8)
(extern extractor vec_extract_imm4_from_immediate vec_extract_imm4_from_immediate)

;; Shuffle operations
;; shuffle: SIMD lane shuffle using 128-bit mask

;; DUP (element) - Duplicate vector element to all lanes
(decl vec_dup_from_fpu (Value VectorSize u8) Aarch64Inst)
(extern constructor vec_dup_from_fpu vec_dup_from_fpu)

;; DUP patterns - single element broadcast
(rule 6 (lower (shuffle a b (shuffle_dup8_from_imm n)))
      (vec_dup_from_fpu a (VectorSize.V16B) n))
(rule 5 (lower (shuffle a b (shuffle_dup16_from_imm n)))
      (vec_dup_from_fpu a (VectorSize.V8H) n))
(rule 4 (lower (shuffle a b (shuffle_dup32_from_imm n)))
      (vec_dup_from_fpu a (VectorSize.V4S) n))
(rule 3 (lower (shuffle a b (shuffle_dup64_from_imm n)))
      (vec_dup_from_fpu a (VectorSize.V2D) n))

;; EXT pattern - byte-wise concatenate and shift
(decl vec_extract (Value Value u8) Aarch64Inst)
(extern constructor vec_extract vec_extract)

(rule 2 (lower (shuffle a b (vec_extract_imm4_from_immediate n)))
      (vec_extract a b n))

;; u128 constant extractor for pattern matching
(decl u128_from_immediate (Immediate) Immediate)
(extern extractor u128_from_immediate u128_from_immediate)

;; UZP1/UZP2 patterns - de-interleave even/odd lanes
(decl vec_uzp1 (Value Value VectorSize) Aarch64Inst)
(extern constructor vec_uzp1 vec_uzp1)
(decl vec_uzp2 (Value Value VectorSize) Aarch64Inst)
(extern constructor vec_uzp2 vec_uzp2)

(rule 1 (lower (shuffle a b (u128_from_immediate 0x1e1c_1a18_1614_1210_0e0c_0a08_0604_0200)))
      (vec_uzp1 a b (VectorSize.V16B)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1f1d_1b19_1715_1311_0f0d_0b09_0705_0301)))
      (vec_uzp2 a b (VectorSize.V16B)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1d1c_1918_1514_1110_0d0c_0908_0504_0100)))
      (vec_uzp1 a b (VectorSize.V8H)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1f1e_1b1a_1716_1312_0f0e_0b0a_0706_0302)))
      (vec_uzp2 a b (VectorSize.V8H)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1b1a1918_13121110_0b0a0908_03020100)))
      (vec_uzp1 a b (VectorSize.V4S)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1f1e1d1c_17161514_0f0e0d0c_07060504)))
      (vec_uzp2 a b (VectorSize.V4S)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1716151413121110_0706050403020100)))
      (vec_uzp1 a b (VectorSize.V2D)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1f1e1d1c1b1a1918_0f0e0d0c0b0a0908)))
      (vec_uzp2 a b (VectorSize.V2D)))

;; ZIP1/ZIP2 patterns - interleave low/high halves
(decl vec_zip1 (Value Value VectorSize) Aarch64Inst)
(extern constructor vec_zip1 vec_zip1)
(decl vec_zip2 (Value Value VectorSize) Aarch64Inst)
(extern constructor vec_zip2 vec_zip2)

(rule 1 (lower (shuffle a b (u128_from_immediate 0x1707_1606_1505_1404_1303_1202_1101_1000)))
      (vec_zip1 a b (VectorSize.V16B)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1f0f_1e0e_1d0d_1c0c_1b0b_1a0a_1909_1808)))
      (vec_zip2 a b (VectorSize.V16B)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1716_0706_1514_0504_1312_0302_1110_0100)))
      (vec_zip1 a b (VectorSize.V8H)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1f1e_0f0e_1d1c_0d0c_1b1a_0b0a_1918_0908)))
      (vec_zip2 a b (VectorSize.V8H)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x17161514_07060504_13121110_03020100)))
      (vec_zip1 a b (VectorSize.V4S)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1f1e1d1c_0f0e0d0c_1b1a1918_0b0a0908)))
      (vec_zip2 a b (VectorSize.V4S)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1716151413121110_0706050403020100)))
      (vec_zip1 a b (VectorSize.V2D)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1f1e1d1c1b1a1918_0f0e0d0c0b0a0908)))
      (vec_zip2 a b (VectorSize.V2D)))

;; TRN1/TRN2 patterns - transpose (interleave alternating lanes)
(decl vec_trn1 (Value Value VectorSize) Aarch64Inst)
(extern constructor vec_trn1 vec_trn1)
(decl vec_trn2 (Value Value VectorSize) Aarch64Inst)
(extern constructor vec_trn2 vec_trn2)

(rule 1 (lower (shuffle a b (u128_from_immediate 0x1e0e_1c0c_1a0a_1808_1606_1404_1202_1000)))
      (vec_trn1 a b (VectorSize.V16B)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1f0f_1d0d_1b0b_1909_1707_1505_1303_1101)))
      (vec_trn2 a b (VectorSize.V16B)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1d1c_0d0c_1918_0908_1514_0504_1110_0100)))
      (vec_trn1 a b (VectorSize.V8H)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1f1e_0f0e_1b1a_0b0a_1716_0706_1312_0302)))
      (vec_trn2 a b (VectorSize.V8H)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1b1a1918_0b0a0908_13121110_03020100)))
      (vec_trn1 a b (VectorSize.V4S)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x1f1e1d1c_0f0e0d0c_17161514_07060504)))
      (vec_trn2 a b (VectorSize.V4S)))

;; REV patterns - byte reversals at different granularities
(decl rev16 (Value VectorSize) Aarch64Inst)
(extern constructor rev16 rev16)
(decl rev32 (Value VectorSize) Aarch64Inst)
(extern constructor rev32 rev32)
(decl rev64 (Value VectorSize) Aarch64Inst)
(extern constructor rev64 rev64)

(rule 1 (lower (shuffle a b (u128_from_immediate 0x0e0f_0c0d_0a0b_0809_0607_0405_0203_0001)))
      (rev16 a (VectorSize.V16B)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x0c0d0e0f_08090a0b_04050607_00010203)))
      (rev32 a (VectorSize.V16B)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x0d0c0f0e_09080b0a_05040706_01000302)))
      (rev32 a (VectorSize.V8H)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x08090a0b0c0d0e0f_0001020304050607)))
      (rev64 a (VectorSize.V16B)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x09080b0a0d0c0f0e_0100030205040706)))
      (rev64 a (VectorSize.V8H)))
(rule 1 (lower (shuffle a b (u128_from_immediate 0x0b0a09080f0e0d0c_0302010007060504)))
      (rev64 a (VectorSize.V4S)))

;; TBL fallback: handles all shuffle patterns via table lookup
(decl aarch64_shuffle_tbl (Value Value Immediate) Aarch64Inst)
(rule 0 (lower (has_type ty (shuffle a b mask)))
      (aarch64_shuffle_tbl a b mask))

;;; FMA fusion helpers

;; Lowers a fused-multiply-add operation handling various forms of the
;; instruction to get maximal coverage of what's available on AArch64.
(decl lower_fmla (VecALUModOp Value Value Value VectorSize) Value)

;; Base case: emit vec_rrr_mod op (FMLA/FMLS vector register form)
(decl vec_rrr_mod (VecALUModOp Value Value Value VectorSize) Value)
(extern constructor vec_rrr_mod vec_rrr_mod)

;; Helper for binary vector operations (VecRRR - 3 registers)
(decl vec_rrr (VecALUOp Value Value VectorSize) Value)
(extern constructor vec_rrr vec_rrr)

;; Helper for unary vector operations (VecMisc - 2 registers)
(decl vec_misc (VecMisc2 Value VectorSize) Value)
(extern constructor vec_misc vec_misc)

;; Condition code converters for zero comparisons
(decl float_cc_cmp_zero_to_vec_misc_op (FloatCC) VecMisc2)
(extern constructor float_cc_cmp_zero_to_vec_misc_op float_cc_cmp_zero_to_vec_misc_op)

(decl float_cc_cmp_zero_to_vec_misc_op_swap (FloatCC) VecMisc2)
(extern constructor float_cc_cmp_zero_to_vec_misc_op_swap float_cc_cmp_zero_to_vec_misc_op_swap)

(decl int_cc_cmp_zero_to_vec_misc_op (IntCC) VecMisc2)
(extern constructor int_cc_cmp_zero_to_vec_misc_op int_cc_cmp_zero_to_vec_misc_op)

(decl int_cc_cmp_zero_to_vec_misc_op_swap (IntCC) VecMisc2)
(extern constructor int_cc_cmp_zero_to_vec_misc_op_swap int_cc_cmp_zero_to_vec_misc_op_swap)

;; Extractors for valid zero comparison conditions
(decl fcmp_zero_cond (FloatCC) FloatCC)
(extern extractor fcmp_zero_cond fcmp_zero_cond)

(decl fcmp_zero_cond_not_eq (FloatCC) FloatCC)
(extern extractor fcmp_zero_cond_not_eq fcmp_zero_cond_not_eq)

(decl icmp_zero_cond (IntCC) IntCC)
(extern extractor icmp_zero_cond icmp_zero_cond)

(decl icmp_zero_cond_not_eq (IntCC) IntCC)
(extern extractor icmp_zero_cond_not_eq icmp_zero_cond_not_eq)

;; Helper for generating float compare equal to zero instruction
(decl fcmeq0 (Reg VectorSize) Reg)
(rule (fcmeq0 rn size)
      (vec_misc (VecMisc2.Fcmeq0) rn size))

;; Helper for generating float compare to zero instructions where 2nd argument is zero
(decl float_cmp_zero (FloatCC Reg VectorSize) Reg)
(rule (float_cmp_zero cond rn size)
      (vec_misc (float_cc_cmp_zero_to_vec_misc_op cond) rn size))

;; Helper for generating float compare to zero instructions in case where 1st argument is zero
(decl float_cmp_zero_swap (FloatCC Reg VectorSize) Reg)
(rule (float_cmp_zero_swap cond rn size)
      (vec_misc (float_cc_cmp_zero_to_vec_misc_op_swap cond) rn size))

;; Helper for generating int compare equal to zero instruction
(decl cmeq0 (Reg VectorSize) Reg)
(rule (cmeq0 rn size)
      (vec_misc (VecMisc2.Cmeq0) rn size))

;; Helper for generating int compare to zero instructions where 2nd argument is zero
(decl int_cmp_zero (IntCC Reg VectorSize) Reg)
(rule (int_cmp_zero cond rn size)
      (vec_misc (int_cc_cmp_zero_to_vec_misc_op cond) rn size))

;; Helper for generating int compare to zero instructions in case where 1st argument is zero
(decl int_cmp_zero_swap (IntCC Reg VectorSize) Reg)
(rule (int_cmp_zero_swap cond rn size)
      (vec_misc (int_cc_cmp_zero_to_vec_misc_op_swap cond) rn size))

;; Helper for vector comparisons using VecALUOp
(decl vec_cmp_vc (Reg Reg VectorSize) Reg)
(rule (vec_cmp_vc rn rm size)
      (let ((dst Reg (vec_rrr (VecALUOp.Fcmeq) rn rn size))
            (tmp Reg (vec_rrr (VecALUOp.Fcmeq) rm rm size))
            (dst Reg (vec_rrr (VecALUOp.And) dst tmp size)))
       dst))

;; Main vector comparison helper - dispatches based on type and condition
(decl vec_cmp (Reg Reg Type Cond) Reg)

;; Floating point Vs / Vc
(rule (vec_cmp rn rm ty (Cond.Vc))
      (if (ty_vector_float ty))
      (vec_cmp_vc rn rm (vector_size ty)))
(rule (vec_cmp rn rm ty (Cond.Vs))
      (if (ty_vector_float ty))
      (let ((tmp Reg (vec_cmp_vc rn rm (vector_size ty))))
       (vec_misc (VecMisc2.Not) tmp (vector_size ty))))

;; Floating-point comparisons
(rule (vec_cmp rn rm ty (Cond.Eq))
      (if (ty_vector_float ty))
      (vec_rrr (VecALUOp.Fcmeq) rn rm (vector_size ty)))
(rule (vec_cmp rn rm ty (Cond.Ne))
      (if (ty_vector_float ty))
      (let ((tmp Reg (vec_rrr (VecALUOp.Fcmeq) rn rm (vector_size ty))))
       (vec_misc (VecMisc2.Not) tmp (vector_size ty))))
(rule (vec_cmp rn rm ty (Cond.Ge))
      (if (ty_vector_float ty))
      (vec_rrr (VecALUOp.Fcmge) rn rm (vector_size ty)))
(rule (vec_cmp rn rm ty (Cond.Gt))
      (if (ty_vector_float ty))
      (vec_rrr (VecALUOp.Fcmgt) rn rm (vector_size ty)))
;; Floating-point swapped-operands
(rule (vec_cmp rn rm ty (Cond.Mi))
      (if (ty_vector_float ty))
      (vec_rrr (VecALUOp.Fcmgt) rm rn (vector_size ty)))
(rule (vec_cmp rn rm ty (Cond.Ls))
      (if (ty_vector_float ty))
      (vec_rrr (VecALUOp.Fcmge) rm rn (vector_size ty)))

;; Integer comparisons
(rule 1 (vec_cmp rn rm ty (Cond.Eq))
      (if (ty_vector_not_float ty))
      (vec_rrr (VecALUOp.Cmeq) rn rm (vector_size ty)))
(rule 1 (vec_cmp rn rm ty (Cond.Ne))
      (if (ty_vector_not_float ty))
      (let ((tmp Reg (vec_rrr (VecALUOp.Cmeq) rn rm (vector_size ty))))
       (vec_misc (VecMisc2.Not) tmp (vector_size ty))))
(rule 1 (vec_cmp rn rm ty (Cond.Ge))
      (if (ty_vector_not_float ty))
      (vec_rrr (VecALUOp.Cmge) rn rm (vector_size ty)))
(rule 1 (vec_cmp rn rm ty (Cond.Gt))
      (if (ty_vector_not_float ty))
      (vec_rrr (VecALUOp.Cmgt) rn rm (vector_size ty)))
(rule (vec_cmp rn rm ty (Cond.Hs))
      (if (ty_vector_not_float ty))
      (vec_rrr (VecALUOp.Cmhs) rn rm (vector_size ty)))
(rule (vec_cmp rn rm ty (Cond.Hi))
      (if (ty_vector_not_float ty))
      (vec_rrr (VecALUOp.Cmhi) rn rm (vector_size ty)))
;; Integer swapped-operands
(rule (vec_cmp rn rm ty (Cond.Le))
      (if (ty_vector_not_float ty))
      (vec_rrr (VecALUOp.Cmge) rm rn (vector_size ty)))
(rule (vec_cmp rn rm ty (Cond.Lt))
      (if (ty_vector_not_float ty))
      (vec_rrr (VecALUOp.Cmgt) rm rn (vector_size ty)))
(rule 1 (vec_cmp rn rm ty (Cond.Ls))
      (if (ty_vector_not_float ty))
      (vec_rrr (VecALUOp.Cmhs) rm rn (vector_size ty)))
(rule (vec_cmp rn rm ty (Cond.Lo))
      (if (ty_vector_not_float ty))
      (vec_rrr (VecALUOp.Cmhi) rm rn (vector_size ty)))

(rule (lower_fmla op x y z size)
      (vec_rrr_mod op z x y size))

;; Special case: if one multiplicand is splat, use element-based FMA with index 0
(rule 1 (lower_fmla op (splat x) y z size)
      (vec_fmla_elem op z y x size 0))
(rule 2 (lower_fmla op x (splat y) z size)
      (vec_fmla_elem op z x y size 0))

;; Special case: if one multiplicand is fneg, peel it and flip operation
(rule 5 (lower_fmla op (fneg x) y z size)
      (lower_fmla (neg_fmla op) x y z size))
(rule 6 (lower_fmla op x (fneg y) z size)
      (lower_fmla (neg_fmla op) x y z size))

;; Shuffle fusion: if one operand is a shuffle broadcasting a single element,
;; use element-indexed FMLA (FMLA Vd.T, Vn.T, Vm.T[idx])
(decl vec_fmla_elem (VecALUModOp Value Value Value VectorSize u8) Value)
(extern constructor vec_fmla_elem vec_fmla_elem)

;; 32-bit float shuffle fusion (V4S)
(rule 3 (lower_fmla op (bitcast _ (shuffle x x (shuffle32_from_imm n n n n))) y z size @ (VectorSize.V4S))
      (if-let true (u64_lt n 4))
      (vec_fmla_elem op z y x size n))
(rule 4 (lower_fmla op x (bitcast _ (shuffle y y (shuffle32_from_imm n n n n))) z size @ (VectorSize.V4S))
      (if-let true (u64_lt n 4))
      (vec_fmla_elem op z x y size n))

;; 64-bit float shuffle fusion (V2D)
(rule 3 (lower_fmla op (bitcast _ (shuffle x x (shuffle64_from_imm n n))) y z size @ (VectorSize.V2D))
      (if-let true (u64_lt n 2))
      (vec_fmla_elem op z y x size n))
(rule 4 (lower_fmla op x (bitcast _ (shuffle y y (shuffle64_from_imm n n))) z size @ (VectorSize.V2D))
      (if-let true (u64_lt n 2))
      (vec_fmla_elem op z x y size n))

;; Helper to negate FMA op (Fmla <-> Fmls)
(decl neg_fmla (VecALUModOp) VecALUModOp)
(rule (neg_fmla (VecALUModOp.Fmla)) (VecALUModOp.Fmls))
(rule (neg_fmla (VecALUModOp.Fmls)) (VecALUModOp.Fmla))

;; ============================================================================
;; Helpers for lower_select
;; ============================================================================

;; Check if type is a scalar float
(decl ty_scalar_float (Type) Type)
(extern extractor ty_scalar_float ty_scalar_float)

;; FPU conditional select (for F32/F64)
(decl fpu_csel (Type IntCC Value Value) ConsumesFlags)
(extern constructor fpu_csel fpu_csel)

;; Vector conditional select (for 128-bit vectors and F128)
(decl vec_csel (IntCC Value Value) ConsumesFlags)
(extern constructor vec_csel vec_csel)

;; Put value into registers (returns ValueRegs)
(decl put_in_regs (Value) ValueRegs)
(extern constructor put_in_regs put_in_regs)

;; Extract register from ValueRegs at index
(decl value_regs_get (ValueRegs u8) Reg)
(extern extractor value_regs_get value_regs_get)

;; Consume flags with two CSELs for I128
(decl consumes_flags_two_csel (IntCC Reg Reg Reg Reg) ConsumesFlags)
(extern constructor consumes_flags_two_csel consumes_flags_two_csel)

;; ============================================================================
;; lower_select: Conditional select based on flags and condition code
;; ============================================================================
;; Lowers conditional select (select based on condition flags).
;; Takes flags, condition code, result type, and two values (true/false).
;; Returns ValueRegs containing the selected value(s).
;;
;; Pattern hierarchy (priority via rule numbers):
;; - Scalars fits_in_64: Use integer CSEL
;; - F32/F64 floats: Use FPU FCSEL
;; - F128: Use vector CSEL (treated as 128-bit vector)
;; - 128-bit vectors: Use vector BSL (bitselect with condition mask)
;; - 64-bit vectors: Reinterpret as F64, use FPU CSEL
;; - I128: Split into two 64-bit CSELs (lo/hi parts)

(decl lower_select (ProducesFlags IntCC Type Value Value) ValueRegs)

;; Scalar floats (F32, F64): Use FPU conditional select
(rule 2 (lower_select flags cond (fits_in_64 ty) rn rm)
      (if (ty_scalar_float ty))
      (with_flags flags (fpu_csel ty cond rn rm)))

;; F128: Treat as 128-bit vector, use vector conditional select
(rule 4 (lower_select flags cond $F128 rn rm)
      (with_flags flags (vec_csel cond rn rm)))

;; 128-bit vectors: Use vector conditional select
(rule 3 (lower_select flags cond ty rn rm)
      (if (ty_vec128 ty))
      (with_flags flags (vec_csel cond rn rm)))

;; 64-bit vectors: Reinterpret as F64 scalar, use FPU CSEL
(rule (lower_select flags cond ty rn rm)
      (if (ty_vec64 ty))
      (with_flags flags (fpu_csel $F64 cond rn rm)))

;; I128: Split into two 64-bit conditional selects (lo and hi)
(rule 4 (lower_select flags cond $I128 rn rm)
      (let ((rn_regs ValueRegs (put_in_regs rn))
            (rm_regs ValueRegs (put_in_regs rm))
            (rn_lo Reg (value_regs_get rn_regs 0))
            (rn_hi Reg (value_regs_get rn_regs 1))
            (rm_lo Reg (value_regs_get rm_regs 0))
            (rm_hi Reg (value_regs_get rm_regs 1)))
       (with_flags flags
        (consumes_flags_two_csel cond rn_lo rn_hi rm_lo rm_hi))))

;; Scalar integers (I8, I16, I32, I64): Use integer CSEL
(rule 1 (lower_select flags cond ty rn rm)
      (if (fits_in_64 ty))
      (with_flags flags (csel cond rn rm)))

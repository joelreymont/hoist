;; aarch64 ISLE lowering rules
;; Minimal bootstrap set for ARM64 instruction selection

;; Integer arithmetic lowering

;; iadd: Add two integers
(rule (lower (iadd ty x y))
      (aarch64_add_rr ty x y))

;; iadd with sign-extended byte operand
;; Pattern: iadd(x, sextb(y)) => ADD Xd, Xn, Wm, SXTB
(rule (lower (iadd ty x (aarch64_sxtb ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtb))

;; iadd with sign-extended halfword operand
;; Pattern: iadd(x, sexth(y)) => ADD Xd, Xn, Wm, SXTH
(rule (lower (iadd ty x (aarch64_sxth ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxth))

;; iadd with sign-extended word operand
;; Pattern: iadd(x, sextw(y)) => ADD Xd, Xn, Wm, SXTW
(rule (lower (iadd ty x (aarch64_sxtw y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtw))

;; iadd with zero-extended byte operand
;; Pattern: iadd(x, uextb(y)) => ADD Xd, Xn, Wm, UXTB
(rule (lower (iadd ty x (aarch64_uxtb ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxtb))

;; iadd with zero-extended halfword operand
;; Pattern: iadd(x, uexth(y)) => ADD Xd, Xn, Wm, UXTH
(rule (lower (iadd ty x (aarch64_uxth ext_ty y)))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxth))

;; iadd with commuted operands (sign-extended byte)
(rule (lower (iadd ty (aarch64_sxtb ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtb))

;; iadd with commuted operands (sign-extended halfword)
(rule (lower (iadd ty (aarch64_sxth ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxth))

;; iadd with commuted operands (sign-extended word)
(rule (lower (iadd ty (aarch64_sxtw y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.sxtw))

;; iadd with commuted operands (zero-extended byte)
(rule (lower (iadd ty (aarch64_uxtb ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxtb))

;; iadd with commuted operands (zero-extended halfword)
(rule (lower (iadd ty (aarch64_uxth ext_ty y) x))
      (if-let $I64 ty)
      (aarch64_add_extended ty x y ExtendOp.uxth))

;; isub: Subtract integers
(rule (lower (isub ty x y))
      (aarch64_sub_rr ty x y))

;; imul: Multiply integers
(rule (lower (imul ty x y))
      (aarch64_mul_rr ty x y))

;; Multiply-add fusion patterns
;; iadd(a, imul(x, y)) => madd(x, y, a)
(rule (lower (iadd ty (imul ty x y) a))
      (aarch64_madd ty x y a))

(rule (lower (iadd ty a (imul ty x y)))
      (aarch64_madd ty x y a))

;; Multiply-subtract fusion patterns
;; isub(a, imul(x, y)) => msub(x, y, a)
(rule (lower (isub ty a (imul ty x y)))
      (aarch64_msub ty x y a))

;; smul_hi: Signed multiply returning upper bits (64×64→128, return high 64)
(rule (lower (smul_hi ty x y))
      (if-let $I64 ty)
      (aarch64_smulh x y))

;; umul_hi: Unsigned multiply returning upper bits (64×64→128, return high 64)
(rule (lower (umul_hi ty x y))
      (if-let $I64 ty)
      (aarch64_umulh x y))

;; sdiv: Signed divide
(rule (lower (sdiv ty x y))
      (aarch64_sdiv ty x y))

;; udiv: Unsigned divide
(rule (lower (udiv ty x y))
      (aarch64_udiv ty x y))

;; srem: Signed remainder (implemented as sdiv + msub)
;; srem(x, y) = x - (x / y) * y
(rule (lower (srem ty x y))
      (let ((quot Aarch64Inst (aarch64_sdiv ty x y)))
            (aarch64_msub ty quot y x)))

;; urem: Unsigned remainder (implemented as udiv + msub)
;; urem(x, y) = x - (x / y) * y
(rule (lower (urem ty x y))
      (let ((quot Aarch64Inst (aarch64_udiv ty x y)))
            (aarch64_msub ty quot y x)))

;; Shift operations lowering

;; ishl: Integer shift left (register form)
(rule (lower (ishl ty x y))
      (aarch64_lsl_rr ty x y))

;; ishl: Integer shift left (immediate form)
(rule (lower (ishl ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_lsl_imm ty x amt))

;; ushr: Unsigned shift right (register form)
(rule (lower (ushr ty x y))
      (aarch64_lsr_rr ty x y))

;; ushr: Unsigned shift right (immediate form)
(rule (lower (ushr ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_lsr_imm ty x amt))

;; sshr: Signed shift right (register form)
(rule (lower (sshr ty x y))
      (aarch64_asr_rr ty x y))

;; sshr: Signed shift right (immediate form)
(rule (lower (sshr ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_asr_imm ty x amt))

;; rotr: Rotate right (register form)
(rule (lower (rotr ty x y))
      (aarch64_ror_rr ty x y))

;; rotr: Rotate right (immediate form)
(rule (lower (rotr ty x (iconst k)))
      (if-let amt (valid_shift_imm k))
      (aarch64_ror_imm ty x amt))

;; rotl: Rotate left - NOT IMPLEMENTED YET
;; Note: Needs register negation for variable amount
;; rotl(x, k) = rotr(x, width - k)

;; Comparison operations lowering

;; icmp: Integer comparison with Equal condition
(rule (lower (icmp IntCC.Equal ty x y))
      (aarch64_cmp_rr ty x y IntCC.Equal))

;; icmp: Integer comparison with NotEqual condition
(rule (lower (icmp IntCC.NotEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.NotEqual))

;; icmp: Integer comparison with SignedLessThan condition
(rule (lower (icmp IntCC.SignedLessThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedLessThan))

;; icmp: Integer comparison with SignedGreaterThanOrEqual condition
(rule (lower (icmp IntCC.SignedGreaterThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedGreaterThanOrEqual))

;; icmp: Integer comparison with SignedGreaterThan condition
(rule (lower (icmp IntCC.SignedGreaterThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedGreaterThan))

;; icmp: Integer comparison with SignedLessThanOrEqual condition
(rule (lower (icmp IntCC.SignedLessThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.SignedLessThanOrEqual))

;; icmp: Integer comparison with UnsignedLessThan condition
(rule (lower (icmp IntCC.UnsignedLessThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedLessThan))

;; icmp: Integer comparison with UnsignedGreaterThanOrEqual condition
(rule (lower (icmp IntCC.UnsignedGreaterThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedGreaterThanOrEqual))

;; icmp: Integer comparison with UnsignedGreaterThan condition
(rule (lower (icmp IntCC.UnsignedGreaterThan ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedGreaterThan))

;; icmp: Integer comparison with UnsignedLessThanOrEqual condition
(rule (lower (icmp IntCC.UnsignedLessThanOrEqual ty x y))
      (aarch64_cmp_rr ty x y IntCC.UnsignedLessThanOrEqual))

;; icmp with immediate form (12-bit unsigned immediate)
(rule (lower (icmp cc ty x (iconst k)))
      (if-let $true (uimm12 k))
      (aarch64_cmp_imm ty x k cc))

;; Bitwise operations lowering

;; band: Bitwise AND (register form)
(rule (lower (band ty x y))
      (aarch64_and_rr ty x y))

;; bor: Bitwise OR (register form)
(rule (lower (bor ty x y))
      (aarch64_orr_rr ty x y))

;; bxor: Bitwise XOR (register form)
(rule (lower (bxor ty x y))
      (aarch64_eor_rr ty x y))

;; bnot: Bitwise NOT (implemented as MVN)
(rule (lower (bnot ty x))
      (aarch64_mvn_rr ty x))

;; ineg: Integer negation (implemented as NEG)
(rule (lower (ineg ty x))
      (aarch64_neg ty x))

;; Conditional select lowering

;; select: Conditional select based on comparison result
;; select(cond, true_val, false_val) -> true_val if cond != 0, else false_val
(rule (lower (select ty (icmp cc cmp_ty x y) true_val false_val))
      (let ((cmp_inst Aarch64Inst (aarch64_cmp_rr cmp_ty x y cc)))
            (aarch64_csel ty true_val false_val cmp_inst cc)))

;; Immediate forms

;; iadd with 12-bit immediate
(rule (lower (iadd ty x (iconst k)))
      (if-let $true (uimm12 k))
      (aarch64_add_imm ty x k))

;; isub with negated 12-bit immediate optimization
;; Lower (isub x iconst) as (iadd x -iconst) when -imm fits in 12-bit
;; This provides more flexibility in immediate selection
(rule (lower (isub ty x (iconst k)))
      (if-let $true (in_neg_uimm12_range k))
      (aarch64_add_imm ty x (negate_i64 k)))

;; isub with 12-bit immediate
(rule (lower (isub ty x (iconst k)))
      (if-let $true (uimm12 k))
      (aarch64_sub_imm ty x k))

;; Load/store lowering

;; load: Load from memory with register+immediate offset
;; Pattern: load(iadd(base, iconst(offset))) => LDR Xt, [Xn, #offset]
(rule (lower (load ty (iadd ptr_ty base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_ldr_imm_offset ty offset))
      (aarch64_ldr_imm ty base scaled_offset))

;; load: Load from memory with commuted register+immediate offset
(rule (lower (load ty (iadd ptr_ty (iconst offset) base) _flags _mem_offset))
      (if-let scaled_offset (valid_ldr_imm_offset ty offset))
      (aarch64_ldr_imm ty base scaled_offset))

;; load: Load from memory with register+register offset
;; Pattern: load(iadd(base, index)) => LDR Xt, [Xn, Xm]
(rule (lower (load ty (iadd ptr_ty base index) _flags _mem_offset))
      (aarch64_ldr_reg ty base index))

;; load: Load from memory with register+extended register offset
;; Pattern: load(iadd(base, sxtw(index))) => LDR Xt, [Xn, Wm, SXTW]
(rule (lower (load ty (iadd ptr_ty base (aarch64_sxtw index)) _flags _mem_offset))
      (aarch64_ldr_ext ty base index ExtendOp.sxtw))

;; load: Load from memory with commuted register+extended register offset
(rule (lower (load ty (iadd ptr_ty (aarch64_sxtw index) base) _flags _mem_offset))
      (aarch64_ldr_ext ty base index ExtendOp.sxtw))

;; load: Load from memory with register+shifted register offset
;; Pattern: load(iadd(base, ishl(index, iconst(shift)))) => LDR Xt, [Xn, Xm, LSL #shift]
(rule (lower (load ty (iadd ptr_ty base (ishl idx_ty index (iconst shift))) _flags _mem_offset))
      (if-let shift_amt (valid_ldr_shift ty shift))
      (aarch64_ldr_shifted ty base index shift_amt))

;; load: Load from memory with commuted register+shifted register offset
(rule (lower (load ty (iadd ptr_ty (ishl idx_ty index (iconst shift)) base) _flags _mem_offset))
      (if-let shift_amt (valid_ldr_shift ty shift))
      (aarch64_ldr_shifted ty base index shift_amt))

;; load: Load from memory (base register only)
(rule (lower (load ty addr _flags _offset))
      (aarch64_ldr ty addr))

;; store: Store to memory with register+immediate offset
;; Pattern: store(val, iadd(base, iconst(offset))) => STR Xt, [Xn, #offset]
(rule (lower (store val (iadd ptr_ty base (iconst offset)) _flags _mem_offset))
      (if-let scaled_offset (valid_str_imm_offset val offset))
      (aarch64_str_imm val base scaled_offset))

;; store: Store to memory with commuted register+immediate offset
(rule (lower (store val (iadd ptr_ty (iconst offset) base) _flags _mem_offset))
      (if-let scaled_offset (valid_str_imm_offset val offset))
      (aarch64_str_imm val base scaled_offset))

;; store: Store to memory with register+register offset
;; Pattern: store(val, iadd(base, index)) => STR Xt, [Xn, Xm]
(rule (lower (store val (iadd ptr_ty base index) _flags _mem_offset))
      (aarch64_str_reg val base index))

;; store: Store to memory with register+extended register offset
;; Pattern: store(val, iadd(base, sxtw(index))) => STR Xt, [Xn, Wm, SXTW]
(rule (lower (store val (iadd ptr_ty base (aarch64_sxtw index)) _flags _mem_offset))
      (aarch64_str_ext val base index ExtendOp.sxtw))

;; store: Store to memory with commuted register+extended register offset
(rule (lower (store val (iadd ptr_ty (aarch64_sxtw index) base) _flags _mem_offset))
      (aarch64_str_ext val base index ExtendOp.sxtw))

;; store: Store to memory with register+shifted register offset
;; Pattern: store(val, iadd(base, ishl(index, iconst(shift)))) => STR Xt, [Xn, Xm, LSL #shift]
(rule (lower (store val (iadd ptr_ty base (ishl idx_ty index (iconst shift))) _flags _mem_offset))
      (if-let shift_amt (valid_str_shift val shift))
      (aarch64_str_shifted val base index shift_amt))

;; store: Store to memory with commuted register+shifted register offset
(rule (lower (store val (iadd ptr_ty (ishl idx_ty index (iconst shift)) base) _flags _mem_offset))
      (if-let shift_amt (valid_str_shift val shift))
      (aarch64_str_shifted val base index shift_amt))

;; store: Store to memory (base register only)
(rule (lower (store val addr _flags _offset))
      (aarch64_str val addr))

;; Control flow lowering

;; jump: Unconditional branch
(rule (lower (jump target))
      (aarch64_b target))

;; brif: Conditional branch
(rule (lower (brif cond target))
      (aarch64_b_cond cond target))

;; return: Return from function
(rule (lower (return))
      (aarch64_ret))

;; Constant materialization

;; iconst: Integer constant (16-bit immediate via MOVZ)
(rule (lower (iconst ty k))
      (if-let $true (uimm16 k))
      (aarch64_movz ty k))

;; iconst: Larger constants (needs multiple instructions)
(rule (lower (iconst ty k))
      (aarch64_iconst ty k))

;; Helper extractors

;; Check if value fits in unsigned 12-bit
(extractor (uimm12 val)
  (if (<= 0 val 4095)
      val))

;; Check if value fits in unsigned 16-bit
(extractor (uimm16 val)
  (if (<= 0 val 65535)
      val))

;; Check if value is a valid shift amount (0-63, suitable for both I32 and I64)
(extractor (valid_shift_imm val)
  (if (<= 0 val 63)
      val))

;; Check if value is in range where negation fits in unsigned 12-bit
;; Returns $true if -4095 <= val <= -1 (i.e., -val fits in 0-4095)
(extractor (in_neg_uimm12_range val)
  (if (<= -4095 val -1)
      $true))

;; Addressing mode extractors

;; Check if offset is valid for load immediate addressing (simplified)
;; For now, accept offsets 0-32760 (max for I64 8-byte aligned access)
(extractor (valid_ldr_imm_offset ty offset)
  (if (<= 0 offset 32760)
      offset))

;; Check if offset is valid for store immediate addressing (simplified)
(extractor (valid_str_imm_offset val offset)
  (if (<= 0 offset 32760)
      offset))

;; Check if shift is valid for load (must be 0-3)
(extractor (valid_ldr_shift ty shift)
  (if (<= 0 shift 3)
      shift))

;; Check if shift is valid for store (must be 0-3)
(extractor (valid_str_shift val shift)
  (if (<= 0 shift 3)
      shift))

;; Type declarations

;; Integer comparison condition codes
(type IntCC (enum
  Equal
  NotEqual
  SignedLessThan
  SignedLessThanOrEqual
  SignedGreaterThan
  SignedGreaterThanOrEqual
  UnsignedLessThan
  UnsignedLessThanOrEqual
  UnsignedGreaterThan
  UnsignedGreaterThanOrEqual
))

;; Extend operations for extended register operands
(type ExtendOp (enum
  uxtb  ;; Zero-extend byte (8-bit)
  uxth  ;; Zero-extend halfword (16-bit)
  uxtw  ;; Zero-extend word (32-bit to 64-bit)
  uxtx  ;; Zero-extend doubleword (no-op for 64-bit)
  sxtb  ;; Sign-extend byte (8-bit)
  sxth  ;; Sign-extend halfword (16-bit)
  sxtw  ;; Sign-extend word (32-bit to 64-bit)
  sxtx  ;; Sign-extend doubleword (no-op for 64-bit)
))

;; Constructor terms for aarch64 instructions

(type Aarch64Inst (enum))

;; Arithmetic
(decl aarch64_add_rr (Type Value Value) Aarch64Inst)
(decl aarch64_add_imm (Type Value i64) Aarch64Inst)
(decl aarch64_add_extended (Type Value Value ExtendOp) Aarch64Inst)
(decl aarch64_sub_rr (Type Value Value) Aarch64Inst)
(decl aarch64_sub_imm (Type Value i64) Aarch64Inst)
(decl aarch64_mul_rr (Type Value Value) Aarch64Inst)
(decl aarch64_madd (Type Value Value Value) Aarch64Inst)
(decl aarch64_msub (Type Value Value Value) Aarch64Inst)
(decl aarch64_smulh (Value Value) Aarch64Inst)
(decl aarch64_umulh (Value Value) Aarch64Inst)
(decl aarch64_sdiv (Type Value Value) Aarch64Inst)
(decl aarch64_udiv (Type Value Value) Aarch64Inst)

;; Shift operations
(decl aarch64_lsl_rr (Type Value Value) Aarch64Inst)
(decl aarch64_lsl_imm (Type Value i64) Aarch64Inst)
(decl aarch64_lsr_rr (Type Value Value) Aarch64Inst)
(decl aarch64_lsr_imm (Type Value i64) Aarch64Inst)
(decl aarch64_asr_rr (Type Value Value) Aarch64Inst)
(decl aarch64_asr_imm (Type Value i64) Aarch64Inst)
(decl aarch64_ror_rr (Type Value Value) Aarch64Inst)
(decl aarch64_ror_imm (Type Value i64) Aarch64Inst)

;; Bitwise operations
(decl aarch64_and_rr (Type Value Value) Aarch64Inst)
(decl aarch64_orr_rr (Type Value Value) Aarch64Inst)
(decl aarch64_eor_rr (Type Value Value) Aarch64Inst)
(decl aarch64_mvn_rr (Type Value) Aarch64Inst)

;; Negation operations
(decl aarch64_neg (Type Value) Aarch64Inst)
(decl aarch64_ngc (Type Value) Aarch64Inst)

;; Sign/zero extend operations
(decl aarch64_sxtb (Type Value) Aarch64Inst)
(decl aarch64_sxth (Type Value) Aarch64Inst)
(decl aarch64_sxtw (Value) Aarch64Inst)
(decl aarch64_uxtb (Type Value) Aarch64Inst)
(decl aarch64_uxth (Type Value) Aarch64Inst)

;; Comparison operations
(decl aarch64_cmp_rr (Type Value Value IntCC) Aarch64Inst)
(decl aarch64_cmp_imm (Type Value i64 IntCC) Aarch64Inst)

;; Conditional select operations
(decl aarch64_csel (Type Value Value Value IntCC) Aarch64Inst)
(decl aarch64_csinc (Type Value Value Value IntCC) Aarch64Inst)
(decl aarch64_csinv (Type Value Value Value IntCC) Aarch64Inst)
(decl aarch64_csneg (Type Value Value Value IntCC) Aarch64Inst)

;; Memory
(decl aarch64_ldr (Type Value) Aarch64Inst)
(decl aarch64_ldr_imm (Type Value i64) Aarch64Inst)
(decl aarch64_ldr_reg (Type Value Value) Aarch64Inst)
(decl aarch64_ldr_ext (Type Value Value ExtendOp) Aarch64Inst)
(decl aarch64_ldr_shifted (Type Value Value i64) Aarch64Inst)
(decl aarch64_str (Value Value) Aarch64Inst)
(decl aarch64_str_imm (Value Value i64) Aarch64Inst)
(decl aarch64_str_reg (Value Value Value) Aarch64Inst)
(decl aarch64_str_ext (Value Value Value ExtendOp) Aarch64Inst)
(decl aarch64_str_shifted (Value Value Value i64) Aarch64Inst)

;; Control flow
(decl aarch64_b (Block) Aarch64Inst)
(decl aarch64_b_cond (Value Block) Aarch64Inst)
(decl aarch64_ret () Aarch64Inst)

;; Constants
(decl aarch64_movz (Type i64) Aarch64Inst)
(decl aarch64_iconst (Type i64) Aarch64Inst)

;; Helper functions
(decl negate_i64 (i64) i64)
